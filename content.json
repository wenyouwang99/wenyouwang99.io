{"pages":[],"posts":[{"title":"","text":"##关于hadoop集群中第二次启动时，DataNode没有启动的解决办法 ##第一次启动时没有问题第二次启动时会有文件残留产生一个新的namenode文件（id），就不认识以前的namenode了导致集群不能正常启动解决办法：在格式化之前，删除datanode里面的信息（默认在/tmp，如果配置该目录，就要去配置的目录下删除（hadoop-2.6.5/data/tmp/）） ##rm -rf /tmp (从节点都要执行) ##hdfs namenode -format (主节点执行) ##主从节点都 jps 然后就成功启动了","link":"/2019/07/31/DataNode无法启动/"},{"title":"Hello World","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. hello hexohello hexo","link":"/2019/07/25/hello-world/"},{"title":"","text":"hdfs文件的读取 (在eclipse中D:\\text\\text.txt01输出 其中text01.txt不必创建，自动生成 )package com.qf.a.b; import java.io.File;import java.io.FileOutputStream;import java.io.IOException;import java.io.OutputStream;import java.net.URI; public static void readFileToLocal(String path) throws IOException { FSDataInputStream fis = null; OutputStream out = null; try{ Configuration conf = new Configuration(); FileSystem fs = FileSystem.get(new URI(&quot;hdfs://192.168.8.10:9000&quot;),conf,path); fis = fs.open(new Path(path)); out = new FileOutputStream(new File(&quot;D:\\\\text\\\\text.txt01&quot;)); IOUtils.copyBytes(fis, System.out, 4096, true); }catch(Exception e){ }finally{ fis.close(); out.close(); } }","link":"/2019/08/06/hdfs文件读取2/"},{"title":"","text":"在windows下安装maven1.先在官网下载maven,并解压在自定义路径我的路径 D:\\java\\maven\\apache-maven-3.6.1 2.修改配置文件D:\\java\\maven\\apache-maven-3.6.1\\conf\\settings D:\\java\\mavenrepository 自定义maven仓库路径，里面是存放jar包的 3.修改环境变量在系统变量中添加MAVEN_HOMED:\\java\\maven\\apache-maven-3.6.1 PathD:\\java\\maven\\apache-maven-3.6.1\\bin 4.测试是否安装成功win+rmvn -v 5.打开eclipse,配置mavenwindow–&gt;preference–&gt;maven–&gt;installations–&gt;add–&gt;D:\\java\\maven\\apache-maven-3.6.1–&gt;apply window–&gt;preference–&gt;maven–&gt;user settings–&gt;update如果更新后仓库路径和 2.修改配置文件 路径一样，则apply后退出即可 6.创建maven工程，配置pom.xml (这个镜像可用)","link":"/2019/08/06/maven安装1/"},{"title":"","text":"hdfs文件的读取 (在eclipse中console输出)package com.qf.a.b; import java.io.IOException; import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.FSDataInputStream;import org.apache.hadoop.fs.FileSystem;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.IOUtils; public class hdfs { public static void main(String[] args) throws IOException { readFileToConsole(“/input/a.txt”); } public static void readFileToConsole(String pat) throws IOException { Configuration conf = new Configuration(); conf.set(&quot;fs.defaultFS&quot;,&quot;hdfs://192.168.8.10:9000&quot;); FileSystem fs = FileSystem.get(conf); //String path = null; //FSDataInputStream fis = fs.open(new Path(path)); FSDataInputStream fis = fs.open(new Path(&quot;/input/a.txt&quot;)); IOUtils.copyBytes(fis, System.out, 4096, true); }}","link":"/2019/08/06/hdfs文件的读取/"},{"title":"","text":"hadoop2.8运行自带的Wordcount①首先在master虚拟机本地某一个目录下创建一个文件夹file，用来存储将要上传到HDFS的文件我是在/opt/hadoop路径下创建的文件夹filecd /opt/hadoopmkdir file ②接着进入file文件夹，然后创建两个文本文件file1.txt和file2.txt，并输入内容cd filevi file1.txt #hello worldvi file2.txt #hello hadoop ③用hadoop dfs命令在HDFS上创建用来接收本地文件的文件夹inputhadoop dfs -mkdir /inputhadoop dfs -ls /hadoop dfs -ls /input ④将刚才在本地创建的两个文件上传到HDFS的input中（此前要关闭所有节点的防火墙，不然会出错）vi file1.txt #hello worldvi file2.txt #hello hadoophadoop dfs -put /opt/hadoop/file/file*.txt /inputhadoop dfs -ls / ⑤现在已经将文件上传到HDFS上了，接下来就要用hadoop自带的Wordcount程序对其进行统计首先进入到Wordcount程序所在目录（找自己的hadoop的安装目录）cd /opt/hadoop/share/hadoop//mapreduce/llhadoop-mapreduce-examples-2.7.2.jarhadoop jar hadoop-mapreduce-examples-2.7.2.jar wordcount /input /output 执行命令运行Wordcount程序，并将结果输出到/output目录下（输出目录自动生成，不可提前创建）hadoop dfs -ls /output （output作为输入结果的文件，不能提前存在，必须是临时生成的） ⑥上面说明job已经成功运行。接下来看输出结果。先查看/output目录下新生成的文件，我们的统计结果在part-r-00000文件中。","link":"/2019/07/31/wordcount/"},{"title":"","text":"6.创建maven工程，配置pom.xml (这个镜像可用)##1. 4.0.0 com.me hadoop 0.0.1-SNAPSHOT &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt; &lt;/properties&gt; /*可用镜像库*/ &lt;repositories&gt; &lt;repository&gt; &lt;id&gt;repo2&lt;/id&gt; &lt;name&gt;Maven Repository Switchboard&lt;/name&gt; &lt;layout&gt;default&lt;/layout&gt; &lt;url &gt;http://repo2.maven.org/maven2&lt;/url&gt; &lt;snapshots&gt; &lt;enabled&gt;false&lt;/enabled&gt; &lt;/snapshots&gt; &lt;/repository&gt; &lt;/repositories&gt; junit junit 3.8.1 test &lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-hdfs&lt;/artifactId&gt; &lt;version&gt;2.6.0&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-client&lt;/artifactId&gt; &lt;version&gt;2.6.0&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-common&lt;/artifactId&gt; &lt;version&gt;2.6.0&lt;/version&gt; &lt;/dependency&gt; &lt;!-- https://mvnrepository.com/artifact/jdk.tools/jdk.tools --&gt; &lt;dependency&gt; &lt;groupId&gt;jdk.tools&lt;/groupId&gt; &lt;artifactId&gt;jdk.tools&lt;/artifactId&gt; &lt;version&gt;1.7&lt;/version&gt; &lt;scope&gt;system&lt;/scope&gt; &lt;systemPath&gt;${JAVA_HOME}/lib/tools.jar&lt;/systemPath&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt; &lt;version&gt;3.1&lt;/version&gt; &lt;configuration&gt; &lt;source&gt;1.7&lt;/source&gt; &lt;target&gt;1.7&lt;/target&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt; &lt;/project&gt;等待安装，安装时间挺长，请等待","link":"/2019/08/06/maven安装2/"},{"title":"","text":"运行自己写的wordcount程序1.将生成的jar文件用xfpt上传到namenode节点，我上传的路径是/opt/hadoop/share/hadoop/mapreduce2.在/input文件中上传数据 (//opt/hadoop/file 对应 /input 为同级目录 /input下 /file1.txt不可存在 才能上传成功)hadoop dfs -put /opt/hadoop/file/file1.txt /input 3.在此路径/opt/hadoop/share/hadoop/mapreduce下执行如下命令yarn jar /opt/hadoop/share/hadoop/mapreduce/wc.jar hadoop.task /input /output3 (我的wordcount程序名叫wc.jar) 4./input目录下如果有3个txt, file1.txt,file2.txt,file3.txt, 3中的/input可指明具体哪个文件yarn jar /opt/hadoop/share/hadoop/mapreduce/wc.jar hadoop.task /input/file1.txt /output3","link":"/2019/07/31/wordcount2/"},{"title":"","text":"在eclipse中向hdfs上传文件public class hdfs { static FileSystem fs = null; static{ Configuration conf = new Configuration(); String path = null; //fs = FileSystem.get(new URI(&quot;hdfs://192.168.8.10:9000&quot;),conf,root); try { fs = FileSystem.get(new URI(&quot;hdfs://192.168.8.10:9000&quot;),conf,path); } catch (IOException e) { // TODO Auto-generated catch block e.printStackTrace(); } catch (InterruptedException e) { // TODO Auto-generated catch block e.printStackTrace(); } catch (URISyntaxException e) { // TODO Auto-generated catch block e.printStackTrace(); } } public static void copyFromLocal() throws IOException { fs.copyFromLocalFile(new Path(&quot;D:\\\\text\\\\123.txt&quot;),new Path(&quot;/input/123&quot;)); System.out.println(&quot;finished...&quot;); } }","link":"/2019/08/06/在eclipse中向hdfs上传文件/"},{"title":"","text":"数据去重 public static class Map extends Mapper&lt;Object, Text, Text, Text&gt;{ private static Text line = new Text(); public void map(Object key,Text value,Context context)throws IOException, InterruptedException{ line = value; context.write(line, new Text(&quot;&quot;)); } } public static class Reduce extends Reducer&lt;Text, Text, Text, Text&gt; { protected void reduce(Text key, Iterable&lt;Text&gt; values,Context context) throws IOException, InterruptedException { context.write(key, new Text(&quot;&quot;)); } }","link":"/2019/08/09/数据去重/"},{"title":"","text":"平均成绩mapreduce 计算平均成绩 public static class Map extends Mapper&lt;LongWritable, Text, Text, IntWritable&gt;{ private static IntWritable data = new IntWritable(); public void map(LongWritable key,Text value,Context context)throws IOException, InterruptedException{ String line = value.toString(); String [] ss = line.split(&quot; &quot;); String strName = ss[0]; String strScore = ss[1]; Text name = new Text(strName); int scoreInt = Integer.parseInt(strScore); context.write(name, new IntWritable(scoreInt)); } } public static class Reduce extends Reducer&lt;Text, IntWritable, Text, IntWritable&gt; { protected void reduce(Text key, Iterable&lt;IntWritable&gt; values,Context context) throws IOException, InterruptedException { int sum = 0; int count = 0; Iterator&lt;IntWritable&gt; iterator = values.iterator(); while(iterator.hasNext()) { sum += iterator.next().get(); count++; } int average = (int)sum/count; context.write(key, new IntWritable(average)); } }","link":"/2019/08/09/平均成绩/"},{"title":"","text":"统计乘用车辆，商用车辆的数量销售和销售额分布/** * 根据汽车所属（个人，商用）来进行划分 * 计算乘用车辆，商用车辆各自的数量，以及各自所占的比重 */ public static class CountMap extends Mapper&lt;LongWritable, Text, Text, LongWritable&gt;{ @Override public void map(LongWritable key, Text value, Mapper&lt;LongWritable, Text, Text, LongWritable&gt;.Context context) throws IOException, InterruptedException { String [] owns = value.toString().trim().split(&quot; &quot;); if(null != owns &amp;&amp; owns.length &gt; 10 &amp;&amp; owns[10] != null) { if(owns[10].equals(&quot;非营运&quot;)) { context.write(new Text(&quot;乘用车辆&quot;), new LongWritable(1)); }else { context.write(new Text(&quot;商用车辆&quot;), new LongWritable(1)); } } }; } /** * 统计乘用车辆和商用车辆的数量以及他们的总量 */ public static class CountReduce extends Reducer&lt;Text, LongWritable, Text, DoubleWritable&gt; { Map&lt;String,Long&gt; maps = new HashMap&lt;String,Long&gt;(); //准备存放非营运的乘用车辆和营运的商用车辆的总和 double all = 0; @Override public void reduce(Text key, Iterable&lt;LongWritable&gt; values,Context context) throws IOException, InterruptedException { Long sum = (long) 0; for (LongWritable val : values) { sum += val.get(); } //求出车辆的总和 all += sum; maps.put(key.toString(),sum); }; protected void cleanup( org.apache.hadoop.mapreduce.Reducer&lt;Text, LongWritable, Text, DoubleWritable&gt;.Context context) throws IOException, InterruptedException { Set&lt;String&gt; keySet = maps.keySet(); //循环set集合中的乘用车辆和商用车辆的数量 for (String str : keySet) { long value = maps.get(str); //用乘用车辆/总量 和 商用车辆/总量，求出各自的比例 double percent = value/all; //输出的key为车辆类型，value的值为该车辆类型的占比 context.write(new Text(str), new DoubleWritable(percent)); } }; }","link":"/2019/08/11/统计乘用车辆，商用车辆的数量销售和销售额分布/"},{"title":"","text":"二次排序（对key,value都排序）import org.apache.hadoop.conf.Configured; import org.apache.hadoop.io.WritableComparable; import java.io.DataInput; import java.io.DataOutput; import java.io.IOException; import org.apache.hadoop.io.*; import org.apache.hadoop.mapreduce.Partitioner; import org.apache.hadoop.mapreduce.Mapper; import org.apache.hadoop.mapreduce.Reducer; import org.apache.hadoop.conf.Configuration; import org.apache.hadoop.fs.FileSystem; import org.apache.hadoop.fs.Path; import org.apache.hadoop.mapreduce.Job; import org.apache.hadoop.mapreduce.lib.input.FileInputFormat; import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat; import org.apache.hadoop.mapreduce.lib.input.TextInputFormat; import org.apache.hadoop.mapreduce.lib.output.TextOutputFormat; import org.apache.hadoop.util.Tool; import org.apache.hadoop.util.ToolRunner; import org.slf4j.Logger; import org.slf4j.LoggerFactory; public class SecondrySort extends Configured implements Tool { static class Fruit implements WritableComparable&lt;Fruit&gt;{ private static final Logger logger = LoggerFactory.getLogger(Fruit.class); private String date; private String name; private Integer sales; public Fruit(){ } public Fruit(String date,String name,Integer sales){ this.date = date; this.name = name; this.sales = sales; } public String getDate(){ return this.date; } public String getName(){ return this.name; } public Integer getSales(){ return this.sales; } @Override public void readFields(DataInput in) throws IOException{ this.date = in.readUTF(); this.name = in.readUTF(); this.sales = in.readInt(); } @Override public void write(DataOutput out) throws IOException{ out.writeUTF(this.date); out.writeUTF(this.name); out.writeInt(sales); } @Override public int compareTo(Fruit other) { int result1 = this.date.compareTo(other.getDate()); if(result1 == 0) { int result2 = this.sales - other.getSales(); if (result2 == 0) { double result3 = this.name.compareTo(other.getName()); if(result3 &gt; 0) return -1; else if(result3 &lt; 0) return 1; else return 0; }else if(result2 &gt;0){ return -1; }else if(result2 &lt; 0){ return 1; } }else if(result1 &gt; 0){ return -1; }else{ return 1; } return 0; } @Override public int hashCode(){ return this.date.hashCode() * 157 + this.sales + this.name.hashCode(); } @Override public boolean equals(Object object){ if (object == null) return false; if (this == object) return true; if (object instanceof Fruit){ Fruit r = (Fruit) object; // if(r.getDate().toString().equals(this.getDate().toString())){ return r.getDate().equals(this.getDate()) &amp;&amp; r.getName().equals(this.getName()) &amp;&amp; this.getSales() == r.getSales(); }else{ return false; } } public String toString() { return this.date + &quot; &quot; + this.name + &quot; &quot; + this.sales; } } static class FruitPartition extends Partitioner&lt;Fruit, NullWritable&gt;{ @Override public int getPartition(Fruit key, NullWritable value,int numPartitions){ return Math.abs(Integer.parseInt(key.getDate()) * 127) % numPartitions; } } public static class GroupingComparator extends WritableComparator{ protected GroupingComparator(){ super(Fruit.class, true); } @Override public int compare(WritableComparable w1, WritableComparable w2){ Fruit f1 = (Fruit) w1; Fruit f2 = (Fruit) w2; if(!f1.getDate().equals(f2.getDate())){ return f1.getDate().compareTo(f2.getDate()); }else{ return f1.getSales().compareTo(f2.getSales()); } } } public static class Map extends Mapper&lt;LongWritable, Text, Fruit, NullWritable&gt; { public void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException { String line = value.toString(); String str[] = line.split(&quot; &quot;); Fruit fruit = new Fruit(str[0],str[1],new Integer(str[2])); //Fruit fruit = new Fruit(); //fruit.set(str[0],str[1],new Integer(str[2])); context.write(fruit, NullWritable.get()); } } public static class Reduce extends Reducer&lt;Fruit, NullWritable, Text, NullWritable&gt; { public void reduce(Fruit key, Iterable&lt;NullWritable&gt; values, Context context) throws IOException, InterruptedException { String str = key.getDate() + &quot; &quot; + key.getName() + &quot; &quot; + key.getSales(); context.write(new Text(str), NullWritable.get()); } } @Override public int run(String[] args) throws Exception { Configuration conf = new Configuration(); // 判断路径是否存在，如果存在，则删除 Path mypath = new Path(args[1]); FileSystem hdfs = mypath.getFileSystem(conf); if (hdfs.isDirectory(mypath)) { hdfs.delete(mypath, true); } Job job = Job.getInstance(conf, &quot;Secondry Sort app&quot;); // 设置主类 job.setJarByClass(SecondrySort.class); // 输入路径 FileInputFormat.setInputPaths(job, new Path(args[0])); // 输出路径 FileOutputFormat.setOutputPath(job, new Path(args[1])); // Mapper job.setMapperClass(Map.class); // Reducer job.setReducerClass(Reduce.class); // 分区函数 job.setPartitionerClass(FruitPartition.class); // 分组函数 job.setGroupingComparatorClass(GroupingComparator.class); // map输出key类型 job.setMapOutputKeyClass(Fruit.class); // map输出value类型 job.setMapOutputValueClass(NullWritable.class); // reduce输出key类型 job.setOutputKeyClass(Text.class); // reduce输出value类型 job.setOutputValueClass(NullWritable.class); // 输入格式 job.setInputFormatClass(TextInputFormat.class); // 输出格式 job.setOutputFormatClass(TextOutputFormat.class); return job.waitForCompletion(true) ? 0 : 1; } public static void main(String[] args) throws Exception{ int exitCode = ToolRunner.run(new SecondrySort(), args); System.exit(exitCode); } } 测试数据： 20180906 Apple 20020180904 Apple 20020180905 Banana 10020180906 Orange 30020180906 Banana 40020180904 Orange 10020180905 Apple 40020180904 Banana 30020180905 Orange 500 运行结果： 20180906 Banana 40020180906 Orange 30020180906 Apple 20020180905 Orange 50020180905 Apple 40020180905 Banana 10020180904 Banana 30020180904 Apple 20020180904 Orange 100 总结： 1、在使用实现WritableComparable接口的方式实现自定义比较器时，必须有一个无参的构造函数。否则会报Unable to initialize any output collector的错误。2、readFields和write方法中处理字段的顺序必须一致，否则会报MapReduce Error: java.io.EOFException at java.io.DataInputStream.readFully(DataInputStream.java:197)的错误。","link":"/2019/08/10/二次排序/"},{"title":"","text":"数据排序 public static class Map extends Mapper&lt;Object, Text, IntWritable, IntWritable&gt;{ private static IntWritable data = new IntWritable(); public void map(Object key,Text value,Context context)throws IOException, InterruptedException{ String line = value.toString(); data.set(Integer.parseInt(line)); context.write(data, new IntWritable(1)); } } public static class Reduce extends Reducer&lt;IntWritable, IntWritable, IntWritable, IntWritable&gt; { private static IntWritable linenum = new IntWritable(1); protected void reduce(IntWritable key, Iterable&lt;IntWritable&gt; values,Context context) throws IOException, InterruptedException { for (IntWritable val : values) { context.write(linenum, key); linenum = new IntWritable(linenum.get()+1); } } }","link":"/2019/08/09/数据排序/"},{"title":"","text":"解决eclipse中无法执行向hadoop上传文件问题main……的问题 Permission denied: user=zhuy, access=WRITE, inode=”/user”:root:supergroup:drwxr-xr-x从这句话我们可知道是权限问题 所以要修改权限在 hdfs-site.xml加入如下代码 （每台都要修改） &lt;property&gt; &lt;name&gt;dfs.permissions&lt;/name&gt; &lt;value&gt;false&lt;/value&gt; &lt;description&gt; If &quot;true&quot;, enable permission checking in HDFS. If &quot;false&quot;, permission checking is turned off, but all other behavior is unchanged. Switching from one parameter value to the other does not change the mode, owner or group of files or directories. &lt;/description&gt; &lt;/property&gt;","link":"/2019/08/06/解决eclipse中无法执行向hadoop上传文件问题/"},{"title":"","text":"title: 分类测试categories: hexo （这个就是文章的分类了）","link":"/2019/08/12/test/"},{"title":"","text":"title: 标签测试tags: Testing （这个就是文章的标签了） Another Tag （这个就是文章的标签了","link":"/2019/08/12/test1/"}],"tags":[],"categories":[]}