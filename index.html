<!DOCTYPE html>
<html>
<head><meta name="generator" content="Hexo 3.9.0">
  <meta charset="utf-8">
  
  <title>Hexo</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta property="og:type" content="website">
<meta property="og:title" content="Hexo">
<meta property="og:url" content="https://wenyouwang99.io/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:locale" content="default">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Hexo">
  
    <link rel="alternate" href="/atom.xml" title="Hexo" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    
  
  <link rel="stylesheet" href="/css/style.css">
  

</head>
</html>
<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    
    <div id="header-inner" class="inner">
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="搜索"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="https://wenyouwang99.io"></form>
      </div>
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">首页</a>
        
          <a class="main-nav-link" href="/archives">归档</a>
        
          <a class="main-nav-link" href="/about">关于</a>
        
      </nav>
      
    </div>
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Hexo</a>
      </h1>
      
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main">
  
    <article id="post-hbase命令2" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2019/08/26/hbase命令2/" class="article-date">
  <time datetime="2019-08-26T08:49:32.709Z" itemprop="datePublished">2019-08-26</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
    <div class="article-entry" itemprop="articleBody">
      
        <!-- Table of Contents -->
        
        <p>1.创建表<br>create ‘table_name’ ,{name=&gt;’pc’},{name=&gt;’ph’}<br>  该表名为：table_name，用来存储用户的行为数据，这个表有两个列族，列族pc用来存储用户pc端的用<br>户行为数据，列族ph用来存储用户的手机端的用户的行为数据。</p>
<p>2.查看所有表<br>list<br>会列出hbase数据库中所有已经创建的表</p>
<p>3.查看建表<br>describe ‘table_name’</p>
<p>4.修改表<br>修改表的模式（schema）之前需要将表先下线，然后执行修改的命令，在上线<br>如下：将table修改为开启集群间复制<br>disable ‘table’<br>alter ‘table’ , {name=&gt;”cf”,replication_scope=&gt;”1”,keep_deleted_cells=&gt;’true’}<br>enable ‘table’</p>
<p>5.put：用来插入一行数据到hbase表<br>put table , rowkey , 列族：列标识符 , 值 </p>
<p>6.get：根据行键获取hbase表的一条记录<br>get table , rowkey<br>get table , rowkey , { 时间戳 }       //根据时间戳获取一行数据在该时刻的数据<br>get table , rowkey , { column=&gt;’pc:v’ , versions=&gt;2 }    //查看版本</p>
<p>7.scan：用来扫描表的数据<br>scan table_name</p>
<p>//获取时间区间内数据<br>scan ‘table’ , { time=&gt; [ 22222 , 33333 ] }</p>
<p>//获取两个版本数据<br>scan ‘table’ , { versions=&gt;2 }</p>
<p>//使用前缀过滤器 过滤 行键 ID，并获取用户的前5行数据<br>scan ‘table’ ,{ filter =&gt; “prefixFilter(‘12345_’)” , columns =&gt; [‘pc’] , limit =&gt; 5 }<br>12345_1    column=pc:v …<br>12345_2    column=pc:v …<br>12345_3    column=pc:v …<br>12345_4    column=pc:v …<br>12345_5    column=pc:v …</p>
<p>//startrow,stoprow   开闭 区间  startrow &lt;=行键&lt; stoprow<br>scan ‘table’ , { startrow=&gt;’12345_1’ , stoprow=&gt;’12345_3’ , column=&gt;[‘pc’] }<br>12345_1    column=pc:v …<br>12345_2    column=pc:v …   //从12345_3开始和往后的用户都不在区间内</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://wenyouwang99.io/2019/08/26/hbase命令2/" data-id="cjzs5vbww000028vgaurc7vdu" class="article-share-link">分享</a>
      
      
      
    </footer>
  </div>
  
</article>
 


  
    <article id="post-hbase命令1" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2019/08/25/hbase命令1/" class="article-date">
  <time datetime="2019-08-25T08:37:49.651Z" itemprop="datePublished">2019-08-25</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
    <div class="article-entry" itemprop="articleBody">
      
        <!-- Table of Contents -->
        
        <p>1.hbase集群增加节点<br>假设新增节点ip为：192.168.1.10，slave2</p>
<p>  1.修改/etc/hosts文件，在所有节点(除新加节点)上运行如下命令以追加新机器名与ip绑定到host文件<br>  然后将hosts文件复制到slave2<br>echo ‘192.168.1.10’ slave2 &gt;&gt; /etc/hosts</p>
<p>  2.修改../hadoop-2.6.5/etc/hadoop/slaves文件，在所有节点上运行如下命令以追加新机器名<br>  到slaves文件<br>echo ‘slave2’ &gt;&gt; ../hadoop-2.6.5/etc/hadoop/slaves(具体路径，这里是略写)</p>
<p>  3.刷新集群节点，在主节点运行<br>../hadoop-2.6.5/etc/hadoop/slaves dfsadmin -refershNodes</p>
<p>  4.启动slave2，在slave2上运行<br>../hadoop-2.6.5/etc/sbin/hadoop-daemon.sh start datanode</p>
<p>  5.运行负载均衡，参数设置为5<br>../hadoop-2.6.5/etc/bin/hdfs balancer -threshold 5</p>
<p>  6.增加hbase分区服务器，在除新加节点外所有节点上运行如下命令以将新节点加入hbase分区服务列表<br>echo ‘slave2’ &gt;&gt; ../hbase-1.2.6/conf/regionservers</p>
<p>  7.启动hbase分区服务器，在slave2上运行<br>../hbase-1.2.6/bin/hbase-daemon.sh start regionserver</p>
<p>1.hbase集群删除节点</p>
<p>  1.下线hbase分区服务器，在slave2上执行，该节点下线过程中，会将数据都转移到集群中其他在线的<br>  节点上<br>../hbase-1.2.6/bin/graceful_stop.sh slave2</p>
<p>  2.hadoop配置数据节点下线，在../hadoop-2.6.5/etc/hadoop/hdfs-site.xml中配置，注意：slaves<br>  文件之前已经存在，只需新建exclude-slaves文件，然后在该文件中添加一行内容slave2，表示将<br>  slave2节点排除<br><property><br><name>dfs.hosts</name><br><value>../hadoop-2.6.5/etc/hadoop/slaves</value><br></property></p>
<property>
<name>dfs.hosts.exclude</name>
<value>../hadoop-2.6.5/etc/hadoop/exclude-slaves</value>
</property>

<p>  3.下线hadoop数据节点，执行刷新命令<br>../hadoop-2.6.5/bin/hadoop dfsadmin -refreshNodes</p>
<p>  4.执行  hadoop dfsadmin -report  或者用浏览器 50070 端口，可以看到执行过程</p>
<p>  5.清理slave2，将slaves与exclude-slaves文件中slave2这行数据删除，然后执行刷新hadoop节点命令<br>  至此slave2已经下线成功了</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://wenyouwang99.io/2019/08/25/hbase命令1/" data-id="cjzqq1kad000404vg80jy2btt" class="article-share-link">分享</a>
      
      
      
    </footer>
  </div>
  
</article>
 


  
    <article id="post-sqoop" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2019/08/23/sqoop/" class="article-date">
  <time datetime="2019-08-23T08:43:55.615Z" itemprop="datePublished">2019-08-23</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
    <div class="article-entry" itemprop="articleBody">
      
        <!-- Table of Contents -->
        
        <p>1.官网：<br><a href="http://sqoop.apache.org/" target="_blank" rel="noopener">http://sqoop.apache.org/</a></p>
<p>2.sqoop,datax:同类产品</p>
<p>3.版本：两个版本完全不兼容，sqoop1用的最多<br>sqoop1:1.4x<br>sqoop2:1.99x</p>
<p>4.安装步骤<br>(1)解压<br>(2)配置环境变量<br>export SQOOP_HOME=/xx/sqoop.xx<br>source /etc/profile<br>(3)添加数据库驱动包<br>cp mysql-connect-…..jar /sqoop-install-path/lib<br>(4)重命名配置文件<br>mv sqoop-evn-template.sh sqoop-env.sh<br>(5)修改配置<br>../bin/configure-sqoop<br>vi configure-sqoop<br>去掉未安装的服务<br>(HCatalog,Accumulo)<br>就是编辑文件，#HCatalog…  (注释掉这两个服务，它们在文件的最后几行，每个服务都只有4行)</p>
<p>接下来，把环境配置一下</p>
<p>vi sqoop-site.xml<br>环境变量加5个<br>EXPORT HADOOP_COMMON_HOME=/opt/hadoop-2.6.0<br>EXPORT HADOOP_MAPRED_HOME=/opt/hadoop-2.6.0<br>hive 安装目录<br>hbase 安装目录<br>ZOOKEEPER(zookeeper的安装目录)</p>
<p>(6)测试<br>sqoop version<br>sqoop list-database -connect jdbc:mysql://node3(可以用安装节点的ip地址):3306/ -username root -password 123456</p>
<p>mysql数据导入到hive的代码<br>//下面能直接运行</p>
<p>sqoop import <br>–connect <br>jdbc:mysql://192.168.8.10:3306/student \                    //student(数据库名)<br>–username <br>root <br>–password <br>123456 <br>–table <br>student <br>–m <br>1 \             //一个map,生成的文件只有2个,一个success,一个part-0-00000<br>–target-dir <br>/sqoop/employee \                 //数据导过来放在hdfs的哪个目录下<br>–fields-terminated-by <br>‘\t’ </p>
<p>sqoop import <br>–connect <br>jdbc:mysql://192.168.8.10:3306/student \                    //student(数据库名)<br>–username <br>root <br>–password <br>123456 <br>–table <br>student <br>–m <br>2 \             //如果为2，就要添加分隔字段,尽量使用int类型,数据分布均匀<br>–split-by <br>id <br>–target-dir <br>/sqoop/employee \                 //数据导过来放在hdfs的哪个目录下<br>–fields-terminated-by <br>‘\t’ </p>
<p>select * from student where id&gt; and id&lt;</p>
<p>hive数据导入到hdfs的代码<br>sqoop import <br>–connect <br>jdbc:mysql://192.168.8.10:3306/student \        //如果报错jdbc拒绝连接，那么ip地址可能错了<br>–username <br>root <br>–password <br>123456 <br>–table \      //这一行可以写：–query ‘select id,age from student where $CONDITINS LIMIT 99’ <br>score <br>–fields-terminated-by <br>“\t” <br>–lines-terminated-by <br>“\n” <br>–hive-import <br>–target-dir <br>/user/hive/warehouse/employee <br>–hive-table <br>employee</p>
<p>可以写成配置文件</p>
<p>import<br>–connect <br>jdbc:mysql://192.168.8.10:3306/student       //如果报错jdbc拒绝连接，那么ip地址可能错了<br>–username<br>root<br>–password<br>123456<br>–table     //这一行可以写：–query ‘select id,age from student where $CONDITINS LIMIT 99’ <br>score<br>–fields-terminated-by<br>“\t”<br>–lines-terminated-by<br>“\n”<br>–hive-import<br>–target-dir<br>/user/hive/warehouse/employee           //写成配置文件时，这个文件必须不能存在<br>–hive-table<br>employee</p>
<p>找一个文件夹<br>vi mysqlToHdfs.conf    //自己命名<br>把上面代码放进去，退出，保存</p>
<p>//执行上面的命令，必须启动yarn,端口为：<br>node1:8088        或者    192.168.8.20(node1的ip地址)</p>
<p>sqoop –options-file sqoop1      //sqoop1:sqoop写的配置文件名<br>sqoop –options-file mysqlToHdfs.conf</p>
<p>mysql导入hbase    //如果报错，zookeeper可能没有启动    zkServer.sh start,start-hbase.sh</p>
<p>import<br>–connect<br>jdbc:mysql://192.168.8.10:3306/student<br>–username<br>root<br>–password<br>123456<br>–table<br>student<br>–hbase-table<br>student<br>–hbase-create-table<br>–hbase-row-key<br>id<br>//–split-by         //这里为-m为1，不用指定<br>//date<br>-m<br>1<br>–column-family<br>info</p>
<p>start-hbase.sh<br>starting…,logging to /usr/local/…/habse-0.98-hadoop2/bin../logs/hbase-root-master-node1.out</p>
<p>tail -f /usr/local/…/habse-0.98-hadoop2/bin../logs/hbase-root-master-node1.log  //看日志<br>出现：Master has completed initialization 就说明成功了<br>也可以去：node1:60010看网页<br>加入此时，上面导入命令中的student表存在了，现在要删除它<br>hbase shell<br>disable ‘student’<br>drop ‘student’<br>现在就可以了，重新执行导入命令</p>
<p>hdfs导入mysql   </p>
<p>export<br>–connect<br>jdbc:mysql://192.168.8.10:3306/student<br>–username<br>root<br>–password<br>123456<br>-m<br>1<br>–columns<br>id,name                          //数据和字段必须匹配<br>–export-dir<br>/user/hive/warehouse/employee<br>–fields-terminated-by<br>‘\t’<br>–table<br>employee  //如果表存在，且其中有数据，则不成功，如果表存在，但进行了数据清空，则成功，                              //或者提前认为在mysql中把表创建好</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://wenyouwang99.io/2019/08/23/sqoop/" data-id="cjzqq1kav000r04vg5m6o0eur" class="article-share-link">分享</a>
      
      
      
    </footer>
  </div>
  
</article>
 


  
    <article id="post-数据仓库" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2019/08/23/数据仓库/" class="article-date">
  <time datetime="2019-08-23T08:43:12.344Z" itemprop="datePublished">2019-08-23</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
    <div class="article-entry" itemprop="articleBody">
      
        <!-- Table of Contents -->
        
        <p>DB–&gt;ETL–&gt;ODS–&gt;Data warehouse–&gt;DataMart</p>
<p>DB：传统关系型数据库<br>1.数据来源<br>  1).传统关系型数据库：mysql，oracl，等<br>  2).文本文件类的日志<br>  3).实时数据来源<br>2.其它来源</p>
<p>ETL：抽取，转换，加载（这是一个过程）<br>工具：sqoop,kettle,datax</p>
<p>ODS：<br>Data warehouse：<br>DataMart：</p>
<p>数据仓库建模方式：<br>(1)分库分表，命名规范，库名以所在数据层开头命名，如：ods_test<br>(2)星型和雪花型建模</p>
<p>范式<br>1NF:<br>1NF简单点就是原子性，列不可在分，没有重复的列，也没有重复的行<br>基本上主要有主键的表都满足第一范式</p>
<p>2NF:<br>2NF首先要满足1NF<br>非主属性必须依赖于键的全部，如果只依赖于主键的一部分，则需要移出，并创建新表</p>
<p>1.事实表：事实表是用来存储主题的主干内容，一些外键指向维度表。事实表一般是没有主键的，<br>基本都是外键。数据的质量完全由业务系统来把握。一般单表字段较多，数据量比较大。<br>2.维度表：事实表中某个方向分支，必须有主键，用于关联事实表。一般数据量较小，变化缓慢。<br>3.宽表：字段和数据量比较巨大，很多维度杂糅在一起。好处：方便查询分析。缺点：没有规范。<br>4.拉链表：记录一个事务从开始，一直到当前状态的所有变化的信息。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://wenyouwang99.io/2019/08/23/数据仓库/" data-id="cjzqq1kb1000z04vgv1c72shl" class="article-share-link">分享</a>
      
      
      
    </footer>
  </div>
  
</article>
 


  
    <article id="post-hive随笔7" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2019/08/22/hive随笔7/" class="article-date">
  <time datetime="2019-08-22T08:23:05.410Z" itemprop="datePublished">2019-08-22</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
    <div class="article-entry" itemprop="articleBody">
      
        <!-- Table of Contents -->
        
        <p>1.<br>cd /opt<br>vi test.sh</p>
<p>   #!/bin/bash<br>   datetime=$(date ‘+%Y%m%d%H’)<br>   source /etc/profile<br>   hive -e “load data local inpath ‘/opt/datame.txt’ into table shujuku.table_name partition(dt=$datetime) “<br>作用：每过一小时就上传一下，把同一时间段的作为一个分区</p>
<p>2.hive调优<br> —建表的注意事项<br>(1)分区，分桶   —一般是按照业务日期进行分区，每天的数据放在一个分区里<br>(2)一般使用外部表，避免数据误删<br>(3)选择适当的文件压缩格式<br>(4)命名要规范<br>(5)数据分层，表分离，但是不要分的太散</p>
<p>—查询调优<br>(1)分区裁剪where过滤，先过滤，后join<br>(2)分区分桶，合并小文件<br>(3)适当的子查询<br>mapjoin(1.2以后自动默认启动mapjoin)<br>select /<em>+mapjoin(b)</em>/ a.xx,b.xx from a left outer join b on a.uid=b.uid<br>左连的时候：大表在左边，小表在右边<br>(4)<br>order by        是全局排序<br>sort by        是单reduce排序<br>distribute by    是分区字段排序<br>cluster by:<br>  可以确保类似的数据分发到同一个reduce task中，并且保证数据有序防止所有的数据分发<br>  到同一个reduce上，导致整体的job时间延迟<br>cluster by的等价语句：<br>  distribute by Word sort by Word ASC</p>
<p>—数据倾斜优化<br>(1)数据倾斜解决<br>看下key的分布<br>原因       ——–一个节点完成80%(举例)的任务，它当然不行了<br>  1)key分布不均匀(实际上还是重复)，比如group by 或者distinct的时候<br>  2)数据重复，join 笛卡尔积 数据膨胀，表现如下：<br>任务进度长时间维持在99%(或100%)，查看任务监控页面，发现只有少量(1个或几个)<br>reduce子任务未完成。因为其处理的数据量和其他reduce差异过大。<br>单一reduce的记录数与平均记录数差异过大，通常可能达到3倍甚至更多。最长时长远大于平均时长<br>解决方案：<br>  1)看下业务上，数据源头能否对数据进行过滤，比如key为null的<br>  2)找到key重复的具体值，进行拆分，hash。异步求和。<br>//案例（解决数据倾斜方法之一）：<br>select city_id,count(*) from test group by city_id ; </p>
<p>select if(city_id=’66666’,hash(rand()),0),city_id from test limit 10 ;     //去掉count看一下输出的样式<br>假入输出为：<br>1234 x1<br>1234 x2<br>6666 0<br>6666 0</p>
<p>select if(city_id=’66666’,hash(rand()),city_id),city_id,count(*) from test group by city_id,if(city_id=’66666’,hash(rand()),0) limit 10 ;<br>假入输出为：<br>1234    x1    1<br>1234    x2    1<br>6666    0    2</p>
<p>select a.city,sum(a.cnt) from (<br>select if(city_id=’66666’,hash(rand()),city_id),city_id,count(*) as cnt from test group by city_id,if(city_id=’66666’,hash(rand()),0)) a group by a.city_id limit 10 ; </p>
<p>—-作业优化<br>调整mapper和reduce的数量<br>太多map导致启动产生过多开销<br>按照输入数据量大小确定reduce数目set mapred.reduce.tasks=3(默认)<br>dfs -count /分区目录/*<br>hive.exec.reducers.max设置阻止资源过度消耗</p>
<p>参数调节<br>set hive.map.aggr=true(hive2默认开启)<br>Map端部分聚合，相当于Combiner<br>hive.groupby.skewindata=true</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://wenyouwang99.io/2019/08/22/hive随笔7/" data-id="cjzqq1kau000p04vg043ucone" class="article-share-link">分享</a>
      
      
      
    </footer>
  </div>
  
</article>
 


  
    <article id="post-hive随笔6" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2019/08/22/hive随笔6/" class="article-date">
  <time datetime="2019-08-22T08:21:41.808Z" itemprop="datePublished">2019-08-22</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
    <div class="article-entry" itemprop="articleBody">
      
        <!-- Table of Contents -->
        
        <p>1.udf函数可以直接应用于select语句，对查询结构做格式化处理后，再输出内容<br>2.编写udf函数的时候需要注意以下几点<br>  (1)自定义udf需要继承org.apache.hadoop.hive.ql.exec.UDF<br>  (2)需要evaluate函数<br>3.步骤<br>  (1)把程序打包放到目标机器上去<br>  (2)进入hive客服端，添加jar包：add jar /usr/local/testdata/hive/hive_UP.jar<br>  (3)创建临时函数：hive&gt;create temporary function f_up as ‘hive_demo.hive_udf’<br>f_up  –》函数名        ‘hive_demo.hive_udf’   –》函数主类名<br>4.查询sql语句<br>select f_up(line) from wc_test;<br>销毁临时函数：hive&gt;drop temporary function f_up;<br>注意：UDF只能实现一进一出的操作，如果需要实现多进一出，则需要实现UDAF</p>
<p>pom.xml依赖<br>  (dependency)       //这里依赖中的所有&lt;&gt;用()代替<br>  (groupId)org.apache.hive(/groupId)<br>  (artifactId)hive-exec(/artifactId)<br>  (version)1.2.1(/version)<br>  (/dependency)</p>
<p>public class hive_udf extends UDF{<br>  public String evaluate (String word){<br>    String upperCase = word.toUpperCase();<br>    return upperCase;<br>  }<br>}</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://wenyouwang99.io/2019/08/22/hive随笔6/" data-id="cjzqq1kat000o04vgni4q26d0" class="article-share-link">分享</a>
      
      
      
    </footer>
  </div>
  
</article>
 


  
    <article id="post-hive随笔5" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2019/08/21/hive随笔5/" class="article-date">
  <time datetime="2019-08-21T08:12:50.837Z" itemprop="datePublished">2019-08-21</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
    <div class="article-entry" itemprop="articleBody">
      
        <!-- Table of Contents -->
        
        <p>1.udf函数可以直接应用于select语句，对查询结构做格式化处理后，再输出内容<br>2.编写udf函数的时候需要注意以下几点<br>  (1)自定义udf需要继承org.apache.hadoop.hive.ql.exec.UDF<br>  (2)需要evaluate函数<br>3.步骤<br>  (1)把程序打包放到目标机器上去<br>  (2)进入hive客服端，添加jar包：add jar /usr/local/testdata/hive/hive_UP.jar<br>  (3)创建临时函数：hive&gt;create temporary function f_up as ‘hive_demo.hive_udf’<br>f_up  –》函数名        ‘hive_demo.hive_udf’   –》函数主类名<br>4.查询sql语句<br>select f_up(line) from wc_test;<br>销毁临时函数：hive&gt;drop temporary function f_up;<br>注意：UDF只能实现一进一出的操作，如果需要实现多进一出，则需要实现UDAF</p>
<p>pom.xml依赖<br><dependency><br><groupid>org.apache.hive</groupid><br><artifactid>hive-exec</artifactid><br><version>1.2.1</version><br></dependency></p>
<p>public class hive_udf extends UDF{<br>  public String evaluate (String word){<br>    String upperCase = word.toUpperCase();<br>    return upperCase;<br>  }<br>}</p>
<p>5.hive的jdbc连接<br>(1)首先开启metastore<br>hive –service metastore &amp;</p>
<p>(2)先开启hiveservice2<br>hive –service hiveserver2 &amp;</p>
<p>添加maven依赖<br><dependency><br><groupid>org.apache.hive</groupid><br><artifactid>hive-jdbc</artifactid><br><version>1.2.1</version><br></dependency></p>
<p>其次在eclipse中编写代码,java中的jdbc</p>
<p>package 包名;</p>
<p>import java.sql.Connection;<br>import java.sql.DriverManager;<br>import java.sql.ResultSet;<br>import java.sql.Statement;</p>
<p>public class 类名{<br>  public static void main(){<br>    Class.forName(“org.apache.hive.jdbc.HiveDriver”);<br>    Connection con = DriverManager.getConnection(“jdbc:hive2://192.168.8.10:10000/数据库名”);<br>    Statement cs = con.createStatement();<br>    //查询sql,query<br>    ResultSet rs = cs.executeQuery(“select * from 数据库名.表名”);<br>    //通常用于DDL操作<br>    //cs.exectte(“create table test”);<br>    while(rs.next()){<br>      String word = rs.getString(1);<br>      String count = rs.getString(2);<br>      System.out.println(word + “,” + count);<br>    }<br>    rs.close();<br>    cs.close();<br>    con.close();<br>  }</p>
<p>}</p>
<p>6.定时功能–       hive -e 写在shell脚本里<br>hive -e “select * from shujuku.tablename limit 5”<br>可以直接在没有启动hive的集群下执行执行sql语句</p>
<p>可以写一个shell脚本：    //shell脚本什么都可以写<br>cd /opt<br>vi test.sh   –&gt;<br>内容如下</p>
<p> #!/bin/bash<br>hive -e “select * from shujuku.tablename limit 5”</p>
<p>保存，退出</p>
<p>启动脚本：<br>sh test.sh</p>
<p>//    #!/bin/bash    这里面还可以写语法</p>
<p> #!/bin/bash<br>date_time=$(date ‘+%Y%m%d’)<br>echo $date_time<br>hive -e “select * from shujuku.tablename where pt=$date_time limit 5”<br>保存，退出</p>
<p>启动脚本：<br>sh test.sh</p>
<p>7.定时功能–       hive -f 写在shell脚本里<br>hive -f /opt/test.hql</p>
<p>hive -f 的shell脚本如下：<br>cd /opt<br>vi test.hql   –&gt;<br>内容如下</p>
<p>//直接写sql<br>select * from shujuku.tablename limit 5</p>
<p>启动脚本：<br>hive -f test.hql</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://wenyouwang99.io/2019/08/21/hive随笔5/" data-id="cjzqq1kas000m04vgnfphqwha" class="article-share-link">分享</a>
      
      
      
    </footer>
  </div>
  
</article>
 


  
    <article id="post-hive随笔4" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2019/08/21/hive随笔4/" class="article-date">
  <time datetime="2019-08-21T08:10:44.187Z" itemprop="datePublished">2019-08-21</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
    <div class="article-entry" itemprop="articleBody">
      
        <!-- Table of Contents -->
        
        <p>1.row_number()      //窗口函数–一般用于分组中求TopN<br>需求：每。。。。。前几名。。。。。<br>//给每个分组数据打上行号<br>作用：把每一组的每一行都打上数字，1，2，3…然后取自己要用的行<br>语法：<br>select * from (select name,data_time,row_number() over(partition by name order cost desc) as rn<br>from window_t) a where rn=1 ;</p>
<p>数据样式：<br>用户    停留日期        停留时间<br>user    date1        date2<br>user1    20190101    10<br>user1    20190102    20<br>user1    20190103    30<br>user2    20190104    40<br>user2    20190105    30<br>user2    20190106    20<br>…</p>
<p>create table test(<br>user      string,<br>date1   string,<br>date2   string<br>)</p>
<p>需求1：user在不同的日期中的最大停留时间<br>需求1：user在不同的日期中的最大停留时间的前两个<br>需求3：每一个user在不同的日期中的最大停留时间的前两个</p>
<p>select *,row_nauber() over(partition by user order by date2 desc) as  rn from test;<br>输出为：<br>user1    20190101    30    1<br>user1    20190102    20    2<br>user1    20190103    10    3<br>user2    20190104    40    1<br>user2    20190104    30    2<br>user2    20190104    20    3</p>
<p>select * from (select *,row_nauber() over(partition by user order by date2 desc) as  rn from test) w<br>where w.rn &lt;= 2 ;<br>输出为：<br>user1    20190101    30    1<br>user1    20190102    20    2<br>user2    20190104    40    1<br>user2    20190104    30    2</p>
<p>2.later view   //行转列<br>(1)later view 用于和split,explode等udft一起使用，它能够将一行数据拆成多行数据，在此基础上可以<br>对拆分后的数据进行聚合。later view 首先为原始表的每行调用udft,udft会把一行拆分成一行或者<br>多行，later view在把结果组合，产生一个支持别名表的虚拟表。<br>(2)explode函数，参数仅接受array和map类型，不支持两个一起用。所以later view可以解决</p>
<p>语法：<br>create table later test(name string,weight array<int>)<br>row format delimited fields terminated by ‘\t’<br>collection items terminated by ‘,’ ;</int></p>
<p>select name new_num from later_test laterral view<br>explode(weight) num as new_num ;</p>
<p>单词统计：<br>select w.word,count(*) from<br>(select explode(split,’ ‘)) word from wc_test) w group by w.word ;</p>
<p>案例：<br>create table test(<br>id        int,<br>num    array<int><br>)</int></p>
<p>数据：<br>1    [100,200]<br>2    [300,400]</p>
<p>select name new_num from later_test laterral view<br>explode(weight) num as new_num ;</p>
<p>输出为：<br>1    100<br>1    200<br>2    300<br>2    400</p>
<p>3.列转行，集合不去重函数:collect_list    //collect_set   –去重      //列转行<br>语法：collect_list(col)<br>hive&gt;<br>select name,age from person;<br>返回值：array<br>说明：将col字段合并成一个数组，不去重<br>举例</p>
<p>create table person(<br>name    string,<br>age    string<br>)</p>
<p>数据<br>name    age<br>aa    10<br>aa    12<br>aa    15</p>
<p>select name,collect_list(age) from person group by name;<br>输出为：<br>aa [“10”,”12”,”15”]</p>
<p>4.向下取整数：floor<br>语法：floor(double a)<br>返回值：bigint<br>说明：返回等于或者小于该double变量的最大的整数<br>举例<br>hive&gt;<br>select floor(301415);<br>3<br>select floor(399995);<br>3<br>select floor(30);<br>30</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://wenyouwang99.io/2019/08/21/hive随笔4/" data-id="cjzqq1kat000n04vglrej7b7f" class="article-share-link">分享</a>
      
      
      
    </footer>
  </div>
  
</article>
 


  
    <article id="post-hive随笔3" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2019/08/19/hive随笔3/" class="article-date">
  <time datetime="2019-08-19T09:30:47.246Z" itemprop="datePublished">2019-08-19</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
    <div class="article-entry" itemprop="articleBody">
      
        <!-- Table of Contents -->
        
        <p>1.hive函数使用</p>
<p>if函数     if( , , )<br>if(条件表达式，如果条件成立返回值，如果条件不成立返回值)<br>select age,if(person_age=’0’,null,person_age) from student;</p>
<p>case when 函数    case when … end<br>case a when b then c [when d then e]* [else] end<br>说明：<br>如果a=b，那么返回c；<br>如果a=d，那么返回e;<br>否则返回f<br>举例：hive&gt;<br>select case 100 when 50 then ‘tom’ when 100 then ‘mary’ else ‘tim’ end from lxw1234;<br>mary</p>
<p>hive&gt;<br>select case 200 when 50 then ‘tom’ when 100 then ‘mary’ else ‘tim’ end from lxw1234;<br>tim</p>
<p>条件判断函数：case<br>case when a then b [when c then d]* [else e] end<br>返回值：T<br>说明：<br>如果a为true，则返回b；<br>如果c为true，则返回d；<br>否则返回e<br>举例：hive&gt;<br>select case when 1=2 then ‘tom’ when 2=2 then ‘mary’ else ‘tim’ end from lxw1234;<br>mary</p>
<p>hive&gt;<br>select case when 1=1 then ‘tom’ when 2=2 then ‘mary’ else ‘tim’ end from lxw1234;<br>tom</p>
<p>select age,case when person_age=’20’ then ‘20’ when person_age=’30’ then ‘30’ else ‘40’;</p>
<p>select age,case when person_age=’20’ then null else person_age end from student;</p>
<p>日期函数    to_date….<br>字符串函数   concat,concat_ws<br>聚合函数     sum,count….<br>null值判断    is null , is not null</p>
<p>2.实现–if函数<br>数据样式：<br>number       dt<br>AS125689  12<br>DA252133  36<br>WSA31233 52<br>处理成：手机号码，qq号码(666666)，微信号码(888888)（第一个字段变成其中一个），平均时长<br>//这种写死的，不对应就会报错，不建议这么写<br>select if(number=’AS125689’,’666666’,’888888’) as num,avg(dt) from data group by number ;<br>//正常应该这么写<br>select if(number=’AS125689’,’666666’,’888888’) as num,avg(dt) from data group by<br>if(number=’AS125689’,’666666’,’888888’);<br>//还可以转化成别的字段返回<br>select if(number=’AS125689’,’666666’,city_id) as num,avg(dt) from data group by<br>if(number=’AS125689’,’666666’,city_id);</p>
<p>3.实现–concat_ws函数   //第一个参数数分隔符<br>select concat_ws(“hello”,’–’,”world”);<br>输出：–helloworld<br>select concat_ws(‘–’,”hello”,”world”);<br>输出：hello–world     //连起来</p>
<p>将数组拼接成字符串<br>create table test(<br>id        int,<br>name  array<string><br>)</string></p>
<p>数据：<br>1    [“zhangsan”,”lisi”]<br>1    [“wangwu”,”zhaoliu”]<br>2    [“aa”,”bb”]</p>
<p>开始拼接语句：<br>select concat_ws(第一个字段是分隔符,第二个字段是数组字段)<br>select concat_ws(‘–’,name) from test;<br>输出为：<br>zhangsan–li<br>wangwu-zhaoliu<br>aa–bb</p>
<p>4.split</p>
<p>create table test(<br>name  string<br>)</p>
<p>数据为：<br>hello world<br>hi tom</p>
<p>select split(name,’ ‘) from test;    //这里参数是空格，第一个参数为字段，第二个为参数符号</p>
<p>[“hello”,”world”]<br>[“hi”,”tom”]</p>
<p>5.explode<br>select explode(split(name,’ ‘)) from test;<br>hello<br>world<br>hi<br>tom</p>
<p>6.单词统计sql<br>select w.word,count(*) from<br>(select explode(split(name,’ ‘)) as word from test) w<br>group by w.word; </p>
<p>输出为：<br>hello    1<br>world    1<br>hi        1<br>tom        1</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://wenyouwang99.io/2019/08/19/hive随笔3/" data-id="cjzqq1kas000l04vg8xv5cnbe" class="article-share-link">分享</a>
      
      
      
    </footer>
  </div>
  
</article>
 


  
    <article id="post-hive随笔2" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2019/08/19/hive随笔2/" class="article-date">
  <time datetime="2019-08-19T09:29:28.401Z" itemprop="datePublished">2019-08-19</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
    <div class="article-entry" itemprop="articleBody">
      
        <!-- Table of Contents -->
        
        <p>1.查看建表语句<br>show create table table_name;</p>
<p>2.查看表的元信息<br>desc table_name;<br>desc extended table_name;<br>desc formatted table_name;</p>
<p>3.重命名表<br>alter table table_name to rename to new_table;</p>
<p>4.创建数据库<br>cretae database database_name;</p>
<p>5.查看数据库<br>show databases;</p>
<p>6.删除数据库<br>drop database tmp;</p>
<p>7.强制删除数据库<br>drop database tmp cascade;</p>
<p>8.修改数据类型<br>alter table lv_test change column colxx string;</p>
<p>9.增加分区<br>alter table test_table add patition (pt=xxx)</p>
<p>10.删除分区<br>alter table test_table drop if exists partition(…);</p>
<p>11.select后可用语法<br>where  –用于过滤，分区裁剪，指定条件<br>join  –用于两表关联,left outer join,join,mapjoin<br>group by  –用于分组聚合<br>order by  –用于全局排列，要尽量避免排序，是针对全局排序的，即对所有的reduce输出是有序的<br>sort by  –sort by : 当有多个reduce时，只能保证单个reduce输出有序，不能保证全局有序<br>    cluster by = distribute by + sort by<br>distinct  –去重</p>
<p>12.mapjoin<br>将小表广播分发到各个计算节点的内存里<br>用于大表关联小表，注意，关联的时候，左表需要是大表<br>select /<em>+mapjoin</em>/ uid,name from bigtable b left outer join small s on biuid=s.uid<br>select /<em>+mapjoin</em>/ * from bigtable b left outer join small s on biuid=s.uid<br>select /<em>+mapjoin</em>/ * from bigtable b left outer join small s on biuid=s.uid where …</p>
<p>13.join<br>左连接会按照左表匹配关联，不管是否能关联上，左边全部输出，没有匹配的右边表，为null。<br>select a from A a left join B b on a.id=b.id where b.id is not null</p>
<p>//先join后过滤<br>select * from city_data d left outer join city_id c on d.city_id=c.city_id where pt=’join’;<br>优化写法：<br>select * from (select * from city_data where pt=’join’) d left outer join city_id c on d.city_id=c.city_id ;<br>//这种写法是 先过滤（导致数据量变小）后join</p>
<p>去重：<br>select distinct city_id from country_info_table limit 100;</p>
<p>排序：<br>select * from city_data distribute by city_id sort by duration;</p>
<p>14.外部表和普通表    //内部表也叫普通表<br>create table city_data_as_S as select * from city_data_503 limit 10;<br>注意：新建表不允许是外部表<br>select 后面表需要是已经存在的表，建表同时会加载数据。<br>会启动mapreduce任务去读取源表数据写入新表</p>
<p>create external table if not exists city_data_like like data_503;</p>
<p>注意：<br>外部表和普通表的区别：<br>1.外部表的路径可以自定义，内部表的路径需要在 hive/warehouse/目录下<br>2.删除表后，普通表数据文件和表信息都删除，外部表仅删除表信息<br>//外部表：删除表，数据不会删除<br>//内部表：删除表，数据会删除</p>
<p>外部表其他功能：可以映射外部数据源。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://wenyouwang99.io/2019/08/19/hive随笔2/" data-id="cjzqq1kar000k04vg0bhirbig" class="article-share-link">分享</a>
      
      
      
    </footer>
  </div>
  
</article>
 


  


  <nav id="page-nav">
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="page-number" href="/page/3/">3</a><span class="space">&hellip;</span><a class="page-number" href="/page/5/">5</a><a class="extend next" rel="next" href="/page/2/">下一页&raquo;</a>
  </nav>
</section>
           
    <aside id="sidebar">
  
    
  <div class="widget-wrap">
     
        <h3 class="follow-title ">Follow me</h3>
     
    <div class="widget follow">
      
              <a class="github" aria-hidden="true" href="https://github.com/giscafer" target="_blank" title="Github"></a>
      
      
            <a class="weibo" aria-hidden="true"  href="http://weibo.com/laohoubin" target="_blank" title="微博"></a>
      
      
              <a class="zhihu" aria-hidden="true"  href="http://www.zhihu.com/people/giscafer" target="_blank" title="知乎"></a>
      
      
            <a class="email" aria-hidden="true"  href="mailto:youemail@outlook.com" target="_blank" title="邮箱"></a>
      
    </div>
  </div>


  
    
  
    

  
    
  
    
  <div class="widget-wrap">
    <h3 class="widget-title recent-posts">最新文章</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2019/08/26/hbase命令2/">(no title)</a>
          </li>
        
          <li>
            <a href="/2019/08/25/hbase命令1/">(no title)</a>
          </li>
        
          <li>
            <a href="/2019/08/23/sqoop/">(no title)</a>
          </li>
        
          <li>
            <a href="/2019/08/23/数据仓库/">(no title)</a>
          </li>
        
          <li>
            <a href="/2019/08/22/hive随笔7/">(no title)</a>
          </li>
        
      </ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title archive">归档</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/08/">August 2019</a><span class="archive-list-count">43</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/07/">July 2019</a><span class="archive-list-count">3</span></li></ul>
    </div>
  </div>


  
    
<div class="widget-wrap">
    <h3 class="widget-title">Links</h3>
    <div class="widget">
        <ul>
            
            <li>
                <a href="http://blog.giscafer.com">giscafer&#39;s blog</a>
            </li>
            
            <li>
                <a href="http://www.gis520.com">GIS520社区</a>
            </li>
            
        </ul>
    </div>
</div>

  
    <!--微信公众号二维码-->

  <div class="widget-wrap">
    <h3 class="follow-title ">WeChat</h3>
    <div class="widget wechat-widget">
        <img src="http://blog.giscafer.com/static/images/qrcode_giscafer.jpg" alt="扫码关注" width="250"/>
    </div>
  </div>


  
</aside>

      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-left">
      &copy; 2014 - 2019 WenYouWang&nbsp;|&nbsp;
      主题 <a href="https://github.com/giscafer/hexo-theme-cafe/" target="_blank">Cafe</a>
    </div>
     <div id="footer-right">
      联系方式&nbsp;|&nbsp;youemail@outlook.com
    </div>
  </div>
</footer>
 <script src="/jquery/jquery.min.js"></script>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">首页</a>
  
    <a href="/archives" class="mobile-nav-link">归档</a>
  
    <a href="/about" class="mobile-nav-link">关于</a>
  
</nav>
    <img class="back-to-top-btn" src="/images/fly-to-top.png"/>
<script>
// Elevator script included on the page, already.
window.onload = function() {
  var elevator = new Elevator({
    selector:'.back-to-top-btn',
    element: document.querySelector('.back-to-top-btn'),
    duration: 1000 // milliseconds
  });
}
</script>
      

  
    <script>
      var cloudTieConfig = {
        url: document.location.href, 
        sourceId: "",
        productKey: "e2fb4051c49842688ce669e634bc983f",
        target: "cloud-tie-wrapper"
      };
    </script>
    <script src="https://img1.ws.126.net/f2e/tie/yun/sdk/loader.js"></script>
    

  







<!-- author:forvoid begin -->
<!-- author:forvoid begin -->

<!-- author:forvoid end -->

<!-- author:forvoid end -->


  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      })
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      })
    </script>
    <script type="text/javascript" src="https://cdn.rawgit.com/mathjax/MathJax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


 <script src="/js/is.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


<script src="/js/script.js"></script>
<script src="/js/elevator.js"></script>
  </div>
</body>
</html>