<!DOCTYPE html>
<html>
<head><meta name="generator" content="Hexo 3.9.0">
  <meta charset="utf-8">
  
  <title>Hexo</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta property="og:type" content="website">
<meta property="og:title" content="Hexo">
<meta property="og:url" content="https://wenyouwang99.io/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:locale" content="default">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Hexo">
  
    <link rel="alternate" href="/atom.xml" title="Hexo" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    
  
  <link rel="stylesheet" href="/css/style.css">
  

</head>
</html>
<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    
    <div id="header-inner" class="inner">
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="搜索"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="https://wenyouwang99.io"></form>
      </div>
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">首页</a>
        
          <a class="main-nav-link" href="/archives">归档</a>
        
          <a class="main-nav-link" href="/about">关于</a>
        
      </nav>
      
    </div>
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Hexo</a>
      </h1>
      
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main">
  
    <article id="post-hive随笔5" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2019/08/21/hive随笔5/" class="article-date">
  <time datetime="2019-08-21T08:12:50.837Z" itemprop="datePublished">2019-08-21</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
    <div class="article-entry" itemprop="articleBody">
      
        <!-- Table of Contents -->
        
        <p>1.udf函数可以直接应用于select语句，对查询结构做格式化处理后，再输出内容<br>2.编写udf函数的时候需要注意以下几点<br>  (1)自定义udf需要继承org.apache.hadoop.hive.ql.exec.UDF<br>  (2)需要evaluate函数<br>3.步骤<br>  (1)把程序打包放到目标机器上去<br>  (2)进入hive客服端，添加jar包：add jar /usr/local/testdata/hive/hive_UP.jar<br>  (3)创建临时函数：hive&gt;create temporary function f_up as ‘hive_demo.hive_udf’<br>f_up  –》函数名        ‘hive_demo.hive_udf’   –》函数主类名<br>4.查询sql语句<br>select f_up(line) from wc_test;<br>销毁临时函数：hive&gt;drop temporary function f_up;<br>注意：UDF只能实现一进一出的操作，如果需要实现多进一出，则需要实现UDAF</p>
<p>pom.xml依赖<br><dependency><br><groupid>org.apache.hive</groupid><br><artifactid>hive-exec</artifactid><br><version>1.2.1</version><br></dependency></p>
<p>public class hive_udf extends UDF{<br>  public String evaluate (String word){<br>    String upperCase = word.toUpperCase();<br>    return upperCase;<br>  }<br>}</p>
<p>5.hive的jdbc连接<br>(1)首先开启metastore<br>hive –service metastore &amp;</p>
<p>(2)先开启hiveservice2<br>hive –service hiveserver2 &amp;</p>
<p>添加maven依赖<br><dependency><br><groupid>org.apache.hive</groupid><br><artifactid>hive-jdbc</artifactid><br><version>1.2.1</version><br></dependency></p>
<p>其次在eclipse中编写代码,java中的jdbc</p>
<p>package 包名;</p>
<p>import java.sql.Connection;<br>import java.sql.DriverManager;<br>import java.sql.ResultSet;<br>import java.sql.Statement;</p>
<p>public class 类名{<br>  public static void main(){<br>    Class.forName(“org.apache.hive.jdbc.HiveDriver”);<br>    Connection con = DriverManager.getConnection(“jdbc:hive2://192.168.8.10:10000/数据库名”);<br>    Statement cs = con.createStatement();<br>    //查询sql,query<br>    ResultSet rs = cs.executeQuery(“select * from 数据库名.表名”);<br>    //通常用于DDL操作<br>    //cs.exectte(“create table test”);<br>    while(rs.next()){<br>      String word = rs.getString(1);<br>      String count = rs.getString(2);<br>      System.out.println(word + “,” + count);<br>    }<br>    rs.close();<br>    cs.close();<br>    con.close();<br>  }</p>
<p>}</p>
<p>6.定时功能–       hive -e 写在shell脚本里<br>hive -e “select * from shujuku.tablename limit 5”<br>可以直接在没有启动hive的集群下执行执行sql语句</p>
<p>可以写一个shell脚本：    //shell脚本什么都可以写<br>cd /opt<br>vi test.sh   –&gt;<br>内容如下</p>
<p> #!/bin/bash<br>hive -e “select * from shujuku.tablename limit 5”</p>
<p>保存，退出</p>
<p>启动脚本：<br>sh test.sh</p>
<p>//    #!/bin/bash    这里面还可以写语法</p>
<p> #!/bin/bash<br>date_time=$(date ‘+%Y%m%d’)<br>echo $date_time<br>hive -e “select * from shujuku.tablename where pt=$date_time limit 5”<br>保存，退出</p>
<p>启动脚本：<br>sh test.sh</p>
<p>7.定时功能–       hive -f 写在shell脚本里<br>hive -f /opt/test.hql</p>
<p>hive -f 的shell脚本如下：<br>cd /opt<br>vi test.hql   –&gt;<br>内容如下</p>
<p>//直接写sql<br>select * from shujuku.tablename limit 5</p>
<p>启动脚本：<br>hive -f test.hql</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://wenyouwang99.io/2019/08/21/hive随笔5/" data-id="cjzkzc10h0013r4vgd6i1kw03" class="article-share-link">分享</a>
      
      
      
    </footer>
  </div>
  
</article>
 


  
    <article id="post-hive随笔4" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2019/08/21/hive随笔4/" class="article-date">
  <time datetime="2019-08-21T08:10:44.187Z" itemprop="datePublished">2019-08-21</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
    <div class="article-entry" itemprop="articleBody">
      
        <!-- Table of Contents -->
        
        <p>1.row_number()      //窗口函数–一般用于分组中求TopN<br>需求：每。。。。。前几名。。。。。<br>//给每个分组数据打上行号<br>作用：把每一组的每一行都打上数字，1，2，3…然后取自己要用的行<br>语法：<br>select * from (select name,data_time,row_number() over(partition by name order cost desc) as rn<br>from window_t) a where rn=1 ;</p>
<p>数据样式：<br>用户    停留日期        停留时间<br>user    date1        date2<br>user1    20190101    10<br>user1    20190102    20<br>user1    20190103    30<br>user2    20190104    40<br>user2    20190105    30<br>user2    20190106    20<br>…</p>
<p>create table test(<br>user      string,<br>date1   string,<br>date2   string<br>)</p>
<p>需求1：user在不同的日期中的最大停留时间<br>需求1：user在不同的日期中的最大停留时间的前两个<br>需求3：每一个user在不同的日期中的最大停留时间的前两个</p>
<p>select *,row_nauber() over(partition by user order by date2 desc) as  rn from test;<br>输出为：<br>user1    20190101    30    1<br>user1    20190102    20    2<br>user1    20190103    10    3<br>user2    20190104    40    1<br>user2    20190104    30    2<br>user2    20190104    20    3</p>
<p>select * from (select *,row_nauber() over(partition by user order by date2 desc) as  rn from test) w<br>where w.rn &lt;= 2 ;<br>输出为：<br>user1    20190101    30    1<br>user1    20190102    20    2<br>user2    20190104    40    1<br>user2    20190104    30    2</p>
<p>2.later view   //行转列<br>(1)later view 用于和split,explode等udft一起使用，它能够将一行数据拆成多行数据，在此基础上可以<br>对拆分后的数据进行聚合。later view 首先为原始表的每行调用udft,udft会把一行拆分成一行或者<br>多行，later view在把结果组合，产生一个支持别名表的虚拟表。<br>(2)explode函数，参数仅接受array和map类型，不支持两个一起用。所以later view可以解决</p>
<p>语法：<br>create table later test(name string,weight array<int>)<br>row format delimited fields terminated by ‘\t’<br>collection items terminated by ‘,’ ;</int></p>
<p>select name new_num from later_test laterral view<br>explode(weight) num as new_num ;</p>
<p>单词统计：<br>select w.word,count(*) from<br>(select explode(split,’ ‘)) word from wc_test) w group by w.word ;</p>
<p>案例：<br>create table test(<br>id        int,<br>num    array<int><br>)</int></p>
<p>数据：<br>1    [100,200]<br>2    [300,400]</p>
<p>select name new_num from later_test laterral view<br>explode(weight) num as new_num ;</p>
<p>输出为：<br>1    100<br>1    200<br>2    300<br>2    400</p>
<p>3.列转行，集合不去重函数:collect_list    //collect_set   –去重      //列转行<br>语法：collect_list(col)<br>hive&gt;<br>select name,age from person;<br>返回值：array<br>说明：将col字段合并成一个数组，不去重<br>举例</p>
<p>create table person(<br>name    string,<br>age    string<br>)</p>
<p>数据<br>name    age<br>aa    10<br>aa    12<br>aa    15</p>
<p>select name,collect_list(age) from person group by name;<br>输出为：<br>aa [“10”,”12”,”15”]</p>
<p>4.向下取整数：floor<br>语法：floor(double a)<br>返回值：bigint<br>说明：返回等于或者小于该double变量的最大的整数<br>举例<br>hive&gt;<br>select floor(301415);<br>3<br>select floor(399995);<br>3<br>select floor(30);<br>30</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://wenyouwang99.io/2019/08/21/hive随笔4/" data-id="cjzkzaqqo000lr4vg7jzhdnki" class="article-share-link">分享</a>
      
      
      
    </footer>
  </div>
  
</article>
 


  
    <article id="post-hive随笔3" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2019/08/19/hive随笔3/" class="article-date">
  <time datetime="2019-08-19T09:30:47.246Z" itemprop="datePublished">2019-08-19</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
    <div class="article-entry" itemprop="articleBody">
      
        <!-- Table of Contents -->
        
        <p>1.hive函数使用</p>
<p>if函数     if( , , )<br>if(条件表达式，如果条件成立返回值，如果条件不成立返回值)<br>select age,if(person_age=’0’,null,person_age) from student;</p>
<p>case when 函数    case when … end<br>case a when b then c [when d then e]* [else] end<br>说明：<br>如果a=b，那么返回c；<br>如果a=d，那么返回e;<br>否则返回f<br>举例：hive&gt;<br>select case 100 when 50 then ‘tom’ when 100 then ‘mary’ else ‘tim’ end from lxw1234;<br>mary</p>
<p>hive&gt;<br>select case 200 when 50 then ‘tom’ when 100 then ‘mary’ else ‘tim’ end from lxw1234;<br>tim</p>
<p>条件判断函数：case<br>case when a then b [when c then d]* [else e] end<br>返回值：T<br>说明：<br>如果a为true，则返回b；<br>如果c为true，则返回d；<br>否则返回e<br>举例：hive&gt;<br>select case when 1=2 then ‘tom’ when 2=2 then ‘mary’ else ‘tim’ end from lxw1234;<br>mary</p>
<p>hive&gt;<br>select case when 1=1 then ‘tom’ when 2=2 then ‘mary’ else ‘tim’ end from lxw1234;<br>tom</p>
<p>select age,case when person_age=’20’ then ‘20’ when person_age=’30’ then ‘30’ else ‘40’;</p>
<p>select age,case when person_age=’20’ then null else person_age end from student;</p>
<p>日期函数    to_date….<br>字符串函数   concat,concat_ws<br>聚合函数     sum,count….<br>null值判断    is null , is not null</p>
<p>2.实现–if函数<br>数据样式：<br>number       dt<br>AS125689  12<br>DA252133  36<br>WSA31233 52<br>处理成：手机号码，qq号码(666666)，微信号码(888888)（第一个字段变成其中一个），平均时长<br>//这种写死的，不对应就会报错，不建议这么写<br>select if(number=’AS125689’,’666666’,’888888’) as num,avg(dt) from data group by number ;<br>//正常应该这么写<br>select if(number=’AS125689’,’666666’,’888888’) as num,avg(dt) from data group by<br>if(number=’AS125689’,’666666’,’888888’);<br>//还可以转化成别的字段返回<br>select if(number=’AS125689’,’666666’,city_id) as num,avg(dt) from data group by<br>if(number=’AS125689’,’666666’,city_id);</p>
<p>3.实现–concat_ws函数   //第一个参数数分隔符<br>select concat_ws(“hello”,’–’,”world”);<br>输出：–helloworld<br>select concat_ws(‘–’,”hello”,”world”);<br>输出：hello–world     //连起来</p>
<p>将数组拼接成字符串<br>create table test(<br>id        int,<br>name  array<string><br>)</string></p>
<p>数据：<br>1    [“zhangsan”,”lisi”]<br>1    [“wangwu”,”zhaoliu”]<br>2    [“aa”,”bb”]</p>
<p>开始拼接语句：<br>select concat_ws(第一个字段是分隔符,第二个字段是数组字段)<br>select concat_ws(‘–’,name) from test;<br>输出为：<br>zhangsan–li<br>wangwu-zhaoliu<br>aa–bb</p>
<p>4.split</p>
<p>create table test(<br>name  string<br>)</p>
<p>数据为：<br>hello world<br>hi tom</p>
<p>select split(name,’ ‘) from test;    //这里参数是空格，第一个参数为字段，第二个为参数符号</p>
<p>[“hello”,”world”]<br>[“hi”,”tom”]</p>
<p>5.explode<br>select explode(split(name,’ ‘)) from test;<br>hello<br>world<br>hi<br>tom</p>
<p>6.单词统计sql<br>select w.word,count(*) from<br>(select explode(split(name,’ ‘)) as word from test) w<br>group by w.word; </p>
<p>输出为：<br>hello    1<br>world    1<br>hi        1<br>tom        1</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://wenyouwang99.io/2019/08/19/hive随笔3/" data-id="cjzkzaqqp000mr4vgc8dg29eu" class="article-share-link">分享</a>
      
      
      
    </footer>
  </div>
  
</article>
 


  
    <article id="post-hive随笔2" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2019/08/19/hive随笔2/" class="article-date">
  <time datetime="2019-08-19T09:29:28.401Z" itemprop="datePublished">2019-08-19</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
    <div class="article-entry" itemprop="articleBody">
      
        <!-- Table of Contents -->
        
        <p>1.查看建表语句<br>show create table table_name;</p>
<p>2.查看表的元信息<br>desc table_name;<br>desc extended table_name;<br>desc formatted table_name;</p>
<p>3.重命名表<br>alter table table_name to rename to new_table;</p>
<p>4.创建数据库<br>cretae database database_name;</p>
<p>5.查看数据库<br>show databases;</p>
<p>6.删除数据库<br>drop database tmp;</p>
<p>7.强制删除数据库<br>drop database tmp cascade;</p>
<p>8.修改数据类型<br>alter table lv_test change column colxx string;</p>
<p>9.增加分区<br>alter table test_table add patition (pt=xxx)</p>
<p>10.删除分区<br>alter table test_table drop if exists partition(…);</p>
<p>11.select后可用语法<br>where  –用于过滤，分区裁剪，指定条件<br>join  –用于两表关联,left outer join,join,mapjoin<br>group by  –用于分组聚合<br>order by  –用于全局排列，要尽量避免排序，是针对全局排序的，即对所有的reduce输出是有序的<br>sort by  –sort by : 当有多个reduce时，只能保证单个reduce输出有序，不能保证全局有序<br>    cluster by = distribute by + sort by<br>distinct  –去重</p>
<p>12.mapjoin<br>将小表广播分发到各个计算节点的内存里<br>用于大表关联小表，注意，关联的时候，左表需要是大表<br>select /<em>+mapjoin</em>/ uid,name from bigtable b left outer join small s on biuid=s.uid<br>select /<em>+mapjoin</em>/ * from bigtable b left outer join small s on biuid=s.uid<br>select /<em>+mapjoin</em>/ * from bigtable b left outer join small s on biuid=s.uid where …</p>
<p>13.join<br>左连接会按照左表匹配关联，不管是否能关联上，左边全部输出，没有匹配的右边表，为null。<br>select a from A a left join B b on a.id=b.id where b.id is not null</p>
<p>//先join后过滤<br>select * from city_data d left outer join city_id c on d.city_id=c.city_id where pt=’join’;<br>优化写法：<br>select * from (select * from city_data where pt=’join’) d left outer join city_id c on d.city_id=c.city_id ;<br>//这种写法是 先过滤（导致数据量变小）后join</p>
<p>去重：<br>select distinct city_id from country_info_table limit 100;</p>
<p>排序：<br>select * from city_data distribute by city_id sort by duration;</p>
<p>14.外部表和普通表    //内部表也叫普通表<br>create table city_data_as_S as select * from city_data_503 limit 10;<br>注意：新建表不允许是外部表<br>select 后面表需要是已经存在的表，建表同时会加载数据。<br>会启动mapreduce任务去读取源表数据写入新表</p>
<p>create external table if not exists city_data_like like data_503;</p>
<p>注意：<br>外部表和普通表的区别：<br>1.外部表的路径可以自定义，内部表的路径需要在 hive/warehouse/目录下<br>2.删除表后，普通表数据文件和表信息都删除，外部表仅删除表信息<br>//外部表：删除表，数据不会删除<br>//内部表：删除表，数据会删除</p>
<p>外部表其他功能：可以映射外部数据源。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://wenyouwang99.io/2019/08/19/hive随笔2/" data-id="cjzkzaqqn000jr4vg11v1qeal" class="article-share-link">分享</a>
      
      
      
    </footer>
  </div>
  
</article>
 


  
    <article id="post-hive随笔1" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2019/08/18/hive随笔1/" class="article-date">
  <time datetime="2019-08-18T08:23:41.156Z" itemprop="datePublished">2019-08-18</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
    <div class="article-entry" itemprop="articleBody">
      
        <!-- Table of Contents -->
        
        <p>1.查看一个有很多内容的1.txt文件，可以先看一点内容<br>head 1.txt</p>
<p>2.hive基本语法-建表<br>create [external] table table_name(<br>a    int,<br>b    string,<br>c    string<br>)<br>partitioned by(非必选，创建分区表 dt string)<br>clustered by (userid) into 3000 buckets    //非必选，分桶子<br>row format delimited fields terminated by ‘\t’  //必选，指定列之间的分隔符<br>stored as rcfile  //非必选，指定文件的读取格式，默认textfile格式<br>location ‘/opt’;   //非必选，指定文件在hdfs上的存储路径，如果已经有文件，会自动加载，<br>默认在hive的warehouse下</p>
<p>注意：还可以这样写<br>create [external] table table_name(<br>a int comment ‘姓名’,<br>b string comment ‘年龄’,<br>c string comment ‘班级’<br>)……</p>
<p>4.mysql登录<br>mysql -uroot -proot</p>
<p>5.查看hive中的表<br>mysql&gt;<br>use hive;<br>show tables;<br>select * from &lt;&gt;;</p>
<p>6.2种建表方式<br>create table table_name1 as select * from table_name2 limit 10;<br>create external table if not exists table_name3 like table_name4;</p>
<p>7.hive中加载数据<br>(1)使用load data命令<br>从hdfs导入数据，路径可以是目录，会将目录下所有文件导入，但是文件格式必须一致<br>load data inpath ‘/test/‘ [overwrite] into table table_name;<br>(2)从本地文件系统导入<br>load data local inpath ‘/test/‘ into table table_name;<br>(3)表对表加载<br>create table if not exists table_name2 as select * from table_name1;<br>insert [overwrite] into table table_name2 select * from table_name1;<br>注意：<br>1.如果建表语句没有指定存储路径，不管是外部表还是内部表，存储路径都是会默认在<br>hive/warehouse/xx.db/表名的目录下<br>加载的数据也会移动到该表的存储目录下。注意是–移动，不是–复制。<br>2.删除外部表。文件不会删除，对应目录也不会删除。</p>
<p>8.<br>hadoop fs -ls /opt/test | wc -l</p>
<p>hadoop fs -ls /opt/test | head -10</p>
<p>9.建表时，集合之间的分隔符<br>collection items terminated by ‘,’ ;<br>create table stud(<br>name      string,<br>chengji   array(int)     //这行用()代替&lt;&gt;<br>)<br>collection items terminated by ‘,’ ;        //集合之间以，分割</p>
<p>张三     [10,20,30]</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://wenyouwang99.io/2019/08/18/hive随笔1/" data-id="cjzkzaqqm000ir4vgyec7r6z7" class="article-share-link">分享</a>
      
      
      
    </footer>
  </div>
  
</article>
 


  
    <article id="post-hive命令13" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2019/08/18/hive命令13/" class="article-date">
  <time datetime="2019-08-18T08:22:52.453Z" itemprop="datePublished">2019-08-18</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
    <div class="article-entry" itemprop="articleBody">
      
        <!-- Table of Contents -->
        
        <p>1.sqoop示例<br>(1)将表导入hdfs<br>sqoop import –connect jdbc:mysql://localhost/test –table test1 –username root –m 1<br>该命令将从mysql数据库test中导出表test1,并将其存放在hdfs的<br>/user/(user)/test1/part-m-00000中      //这行用()来代替&lt;&gt;,&lt;&gt;里代码不显示</p>
<p>(2)将表导入hdfs的特定目录下<br>sqoop import –connect jdbc:mysql://localhost/test –table test1 –username root –m 1<br>–target-dir /hive/tables/test1/<br>在本例中，表test1的内容将存放在hdfs的/hive/tables/test1目录下</p>
<p>(3)将数据库中的表全部导入hdfs<br>sqoop import-all-tables –connect jdbc:mysql://localhost/test –username root<br>该命令将test数据库中的所有表导入到hdfs中。sqoop导入作业在/user/root目录下为每个表<br>都创建了一个目录，如下<br>hadoop fs -ls /user/root</p>
<p>(4)将表导入hive<br>sqoop import –connect jdbc:mysql://localhost/test –table test1 –username root –m 1<br>–hive-import<br>该命令将把表test1导入到hdfs中，同时也会将其元数据添加到hive中。如下验证数据<br>use default;</p>
<p>select count(*) from test1;</p>
<p>(5)将表导入hive并且将表存储为orc表<br>sqoop import –connect jdbc:mysql://localhost/test –table test10 –username root –m 1<br>–hcatalog-database default –hcatalog-table test10_orc –create-hcatalog-table –hcatalog-<br>storage-stanza “stored as orcfile”<br>该命令将在默认数据库中创建一个名为test_orc的新表，并且以orc文件格式存储数据。<br>将hive表数据存储为orc，有理由 矢量化。验证如下<br>hive&gt;<br>describe extended test10_orc;</p>
<p>(6)导入所选择的数据<br>sqoop import –connect jdbc:mysql://localhost/test –table test10 –username root –m 1<br>–where “a&gt;1”<br>通过该命令，可以导入表test1中列a的所有值大于1的数据。<br>该选项提供了一种方法，可以导入任何表的一个子集</p>
<p>(7)导入增量数据<br>incremental  –sqoop使用它来判断哪一行是新的<br>check-column  –提供需要检查确定候选行的列<br>last-value  –执行上一次导入操作的最大值<br>sqoop import –connect jdbc:mysql://localhost/test –table test10 –username root –m 1<br>–incremental  append –check-column id -last-value 1000</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://wenyouwang99.io/2019/08/18/hive命令13/" data-id="cjzkzaqqg000ar4vgk61iktkj" class="article-share-link">分享</a>
      
      
      
    </footer>
  </div>
  
</article>
 


  
    <article id="post-hive命令12" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2019/08/17/hive命令12/" class="article-date">
  <time datetime="2019-08-17T08:18:32.361Z" itemprop="datePublished">2019-08-17</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
    <div class="article-entry" itemprop="articleBody">
      
        <!-- Table of Contents -->
        
        <p>1.ACID<br>(1)原子性(A)<br>(2)一致性(C)<br>(3)隔离性(I)<br>(4)持久性(D)</p>
<p>2.HIVE的配置<br>hive.support.concurrency—true<br>hive.enforce.bucketing——true<br>hive.exec.dynamic.partition.mode—–true<br>hive.txn.manager—org.apache.hadoop.hive.ql.lockmgr.DbTxnManager<br>hive.compactor.initiator.on—在Thrift元存储服务的一个实例上为true<br>hive.compactor.worker.threads—在Thrift元存储服务的一个实例上为10</p>
<p>3.请使用如下建表格式  //这里字段作为举例<br>create table census.personname(<br>persid       int,<br>firstname  string,<br>lastname   string<br>)<br>clustered by (persid) into 1 buckets<br>stored as orc<br>tblproperties(‘transactional’ = ‘true’);</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://wenyouwang99.io/2019/08/17/hive命令12/" data-id="cjzkzaqqd0008r4vgvsw7p28a" class="article-share-link">分享</a>
      
      
      
    </footer>
  </div>
  
</article>
 


  
    <article id="post-hive命令11" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2019/08/17/hive命令11/" class="article-date">
  <time datetime="2019-08-17T08:18:00.806Z" itemprop="datePublished">2019-08-17</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
    <div class="article-entry" itemprop="articleBody">
      
        <!-- Table of Contents -->
        
        <p>1.使用左半连接<br>hive支持表之间的嵌套连接，假设有这样的嵌套连接（如下）<br>select a.key,a.value<br>from a<br>where a.key in (select b.key from B);<br>由于采用分布式处理，该查询在hive中会失败<br>hive处理使用semi join命令<br>select table_fields<br>from table_one<br>left semi join table_two<br>on (table_one.key_one = table_two.key_one);<br>语法解释<br>from table_one left semi join table_two  –罗列检索table_fields而进行半连接操作的2个表<br>on (table_one.key_one = table_two.key_one)  –罗列2个表所需的等值规则</p>
<p>2.案例<br>目标：使用半连接操作，对两个表中的数据进行整合<br>use census;</p>
<p>select<br>    personname.firstname,<br>    personname.lastname<br>from<br>    census.personname<br>left semi join<br>    census.address<br>on<br>    (personname.persid = address.persid);</p>
<p>结果如下<br>Bob    Burger    KA13<br>Charlie    Clown    KA9</p>
<p>3.用单次mapreduce实现连接<br>select table_one.key_one,table_two.key_one,table_three.key_one<br>from table_one join table_two<br>on (table_one.key_one = table_two.key_one)<br>join table_three<br>on (table_three.key_one = table_two.key_one);<br>语法解释<br>(1)select table_one.key_one,table_two.key_one,table_three.key_one<br>    –选关键字<br>(2)from table_one join table_two<br>    –列出为检索table_fields所需连接的第一个表和第二个表<br>(3)on (table_one.key_one = table_two.key_one)<br>    –列出连接第一个表和第二个表的等值规则<br>(4)join table_three<br>    –列出检索table_fields所需连接的第三个表<br>(5)on (table_three.key_one = table_two.key_one)<br>    –列出连接第三个表的等值规则</p>
<p>4.案例，在一次mapreduce中连接3个表<br>use census;</p>
<p>create table census.account (<br>persid       int,<br>bamount  int<br>)<br>clustered by (persid) into 1 buckets<br>stored as orc<br>tblproperties(‘transactional’ = ‘true’);</p>
<p>insert into table census.personname<br>values<br>(1,12),<br>(2,9);</p>
<p>select<br>    personname.firstname,<br>    personname.lastname,<br>    address.postname,<br>    account.bamount<br>from<br>    census.personname<br>join<br>    census.address<br>on<br>    (personname.persid = address.persid)<br>join<br>    census.account<br>on<br>    (personname.persid = address.persid);</p>
<p>结果如下<br>Bob    Burger    KA13    12<br>Charlie    Clown    KA9    9</p>
<p>5.最后使用最大的表<br>hive在实施连接时可以先缓存前几个要连接的表，然后在针对它们映射最后一个表。<br>总是将最大的表放在后面时一种比较好的实践方法，因为这样做会加速处理过程。<br>(1)hive语法1：<br>select table_one.key_one,table_two.key_one,table_three.key_one<br>from table_one join table_two<br>on (table_one.key_one = table_two.key_one)<br>join table_three<br>on (table_three.key_one = table_two.key_one);<br>语法解释<br>(1)table_one,table_two<br>    –在内存中缓存<br>(2)table_three<br>    –直接从硬盘映射</p>
<p>(1)hive语法2：<br>select table_one.key_one,table_two.key_one,table_three.key_one<br>from table_one join table_three<br>on (table_one.key_one = table_three.key_one)<br>join table_two<br>on (table_two.key_one = table_three.key_one);<br>语法解释<br>(1)table_one,table_three<br>    –在内存中缓存<br>(2)table_two<br>    –直接从硬盘映射</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://wenyouwang99.io/2019/08/17/hive命令11/" data-id="cjzkzaqqd0007r4vge3665zj6" class="article-share-link">分享</a>
      
      
      
    </footer>
  </div>
  
</article>
 


  
    <article id="post-hive命令10" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2019/08/17/hive命令10/" class="article-date">
  <time datetime="2019-08-17T08:17:34.774Z" itemprop="datePublished">2019-08-17</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
    <div class="article-entry" itemprop="articleBody">
      
        <!-- Table of Contents -->
        
        <p>1.创建结构相同的表<br>create table table_name like table_name1;<br>语法解释<br>table_name  –要创建表的名字<br>table_name1  –hive中已有表的名字</p>
<p>2.案例<br>目标：使用person表的结构创建一个名为person40的表<br>use census;</p>
<p>create table person40 like person;</p>
<p>select * from person40;</p>
<p>插入数据（能插入就成功）<br>insert into table person40 values (0,’Bob’,’Burger’),(1,’Charlie’,’clown’);<br>select * from person40;   //此时有2行数据</p>
<p>3.连接<br>hive支持表之间的等值连接，使你能够整合来自两个表的数据<br>select table_fields<br>from table_one<br>join table_two<br>on (table_one.key_one = table_two.key_one<br>and table_one.key_two = table_two.key_two);<br>语法解释<br>(1)select table_fields   –用于从两个表中选取一系列字段的关键字<br>(2)from table_one join table_two   –罗列出两个为了检索table_fields而进行连接操作的表<br>(3)on (table_one.key_one = table_two.key_one  —列出连接两个表的等值规则<br>and table_one.key_two = table_two.key_two)</p>
<p>4.案例<br>目标：在表census.personname和表census.address之间创建连接<br>use census;</p>
<p>create table census.personname(<br>persid       int,<br>firstname  string,<br>lastname   string<br>)<br>clustered by (persid) into 1 buckets<br>stored as orc<br>tblproperties(‘transactional’ = ‘true’);</p>
<p>insert into table census.personname<br>values<br>(0,’Albert’,’ape’),<br>(1,’Bob’,’Burger’)<br>(2,’Charlie’,’Clown’),<br>(3,’Danny’,’Drywer’);</p>
<p>create table census.address(<br>persid        int,<br>postname  string<br>)<br>clustered by (persid) into 1 buckets<br>stored as orc<br>tblproperties(‘transactional’ = ‘true’);</p>
<p>insert into table census.address<br>values<br>(1,’KA13’),<br>(2,’KA9’),<br>(10,’SW1’);</p>
<p>现在执行连接操作<br>select<br>    personname.firstname,<br>    personname.lastname,<br>    address.postname<br>from<br>    census.personname<br>join<br>    census.address<br>on<br>    (personname.persid = address.persid);</p>
<p>结果如下<br>Bob    Burger    KA13<br>Charlie    Clown    KA9</p>
<p>5.使用外连接<br>hive支持采用left,right,full outer等连接方式实现表之间的等值连接，这其中无匹配的键<br>select table_fields<br>from table_one<br>[left,right,full outer]<br>join table_two<br>on (table_one.key_one = table_two.key_one<br>and table_one.key_two = table_two.key_two);<br>语法解释<br>(1)left –left 连接产生的结果包含表table_one中匹配where语句的字段值以及表<br>         table_two中匹配和不匹配where语句的字段值<br>(2)right –right 连接产生的结果包含表table_one中匹配where语句的字段值以及表<br>         table_one中匹配和不匹配where语句的字段值<br>(3)full outer –full outer 连接将返回表table_two和table_one中的字段值；当有不匹配<br>                      where语句的行时，其字段值为null<br>(4)on  –列出两个表的等值规则</p>
<p>6.案例<br>左连接方式<br>use census;</p>
<p>select<br>    personname.firstname,<br>    personname.lastname,<br>    address.postname<br>from<br>    census.personname<br>left join<br>    census.address<br>on<br>    (personname.persid = address.persid);</p>
<p>此时结果是4条记录<br>Albert    Ape    null<br>Bob    Burger    KA13<br>Charlie    Clown    KA9<br>Danny    Drywer    null</p>
<p>右连接方式<br>use census;</p>
<p>select<br>    personname.firstname,<br>    personname.lastname,<br>    address.postname<br>from<br>    census.personname<br>right join<br>    census.address<br>on<br>    (personname.persid = address.persid);</p>
<p>此时结果是3条记录<br>Bob    Burger    KA13<br>Charlie    Clown    KA9<br>Danny    Drywer    null</p>
<p>使用完全外连接<br>use census;</p>
<p>select<br>    personname.firstname,<br>    personname.lastname,<br>    address.postname<br>from<br>    census.personname<br>full outer join<br>    census.address<br>on<br>    (personname.persid = address.persid);</p>
<p>此时结果是5条记录<br>Albert    Ape    null<br>Bob    Burger    KA13<br>Charlie    Clown    KA9<br>Danny    Drywer    null<br>null    null    SW1</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://wenyouwang99.io/2019/08/17/hive命令10/" data-id="cjzkzaqq90003r4vgytra98zy" class="article-share-link">分享</a>
      
      
      
    </footer>
  </div>
  
</article>
 


  
    <article id="post-hive命令9" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2019/08/17/hive命令9/" class="article-date">
  <time datetime="2019-08-17T08:17:05.679Z" itemprop="datePublished">2019-08-17</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
    <div class="article-entry" itemprop="articleBody">
      
        <!-- Table of Contents -->
        
        <p>1.将查询到的数据写入文件系统<br>insert [overwrite]<br>directory directoryname<br>select select_fileds from from_statement;<br>语法解释：<br>(1)insert  –将数据向hive装载数据的关键字<br>(2)overwrite  –如果包含，支持用户将数据装载到一个早已建好的表中并且替换原来的数据<br>如果省略，支持用户将数据装载到一个早已建好的表中并且将新数据追加到原来的数据后面<br>(3)directory directoryname  —-directoryname 是hadoop分布式文件系统中已有的目录<br>                                        名称，使用hadoop fs -mkdir directoryname 来创建一个目录<br>(5)select  –可以是针对hive生态系统的任何select命令</p>
<p>2.使用已有表创建输出目录<br>hadoop fs -mkdir ‘exampleoutput’</p>
<p>hive</p>
<p>use census;</p>
<p>insert overwrite<br>directory ‘exampleoutput’<br>row format delimited fields terminated by ‘,’<br>select persid,firstname,lastname<br>from Person;</p>
<p>exits;</p>
<p>测试一下，是否所有的数据都已经装载<br>hadoop fs -cat ‘exampleoutput/000000_0’ </p>
<p>3.直接向表插入值<br>hive支持用一系列静态值直接将数据装载到表中<br>insert into table tablename values (row_values1),(row_values2);<br>语法解释<br>(1)insert  –将数据向hive装载数据的关键字<br>(2)table tablename  —-tablename是hive中已经存在的表的名称<br>           使用create table tablename语句<br>(3)values (row_values1),(row_values2)  –值(row_values1),(row_values2)是相同格式<br>                的单条记录，而不是表的记录</p>
<p>4.案例<br>目标：可以将一条记录直接插入到一个名为 personhub 的表中<br>use census;</p>
<p>insert into table personhub values (0);</p>
<p>测试一下数据是否加载<br>select persid from personhub where persid = 0 ;</p>
<p>5.直接更新表中的数据<br>update tablename set column = value [where expression] ;<br>语法解释<br>set column = value  –set 命令更新该列一个值<br>[where expression]  –where 可用于为不同的查询挑选特定列的值</p>
<p>6.案例<br>目标：更新person20表的数据<br>use census;</p>
<p>create table census.person20(<br>persid       int,<br>firstname  string,<br>lastname  string<br>)<br>clustered by (persid) into 1 buckets<br>stored as orc<br>tblproperties(‘transactional’ = ‘true’);</p>
<p>insert into table person20 values (0,’A’,’B’),(2,’x’,’y’);</p>
<p>测试数据是否已经插入<br>select * from census.person20;       //结果是2行上面插入的结果的记录</p>
<p>现在执行  更新  操作<br>use census;</p>
<p>update census.person20 set lastname = ‘SS’ where persid = 0 ;</p>
<p>select * from census.person20;   //结果是2行记录</p>
<p>7.在表中直接删除数据<br>delete tablename [where experssion] ;</p>
<p>8.案例<br>use census;</p>
<p>create table census.person20(<br>persid       int,<br>firstname  string,<br>lastname  string<br>)<br>clustered by (persid) into 1 buckets<br>stored as orc<br>tblproperties(‘transactional’ = ‘true’);</p>
<p>insert into table person30 values (0,’A’,’B’),(2,’x’,’y’);</p>
<p>测试数据是否已经插入<br>select * from census.person30;       //结果是2行上面插入的结果的记录</p>
<p>删除一条记录<br>use census;</p>
<p>delete from census.person30 where persid = 0 ;</p>
<p>select * from census.person30;   //结果是1条记录</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://wenyouwang99.io/2019/08/17/hive命令9/" data-id="cjzkzaqql000gr4vgkgpoyfd2" class="article-share-link">分享</a>
      
      
      
    </footer>
  </div>
  
</article>
 


  


  <nav id="page-nav">
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="page-number" href="/page/3/">3</a><a class="page-number" href="/page/4/">4</a><a class="extend next" rel="next" href="/page/2/">下一页&raquo;</a>
  </nav>
</section>
           
    <aside id="sidebar">
  
    
  <div class="widget-wrap">
     
        <h3 class="follow-title ">Follow me</h3>
     
    <div class="widget follow">
      
              <a class="github" aria-hidden="true" href="https://github.com/giscafer" target="_blank" title="Github"></a>
      
      
            <a class="weibo" aria-hidden="true"  href="http://weibo.com/laohoubin" target="_blank" title="微博"></a>
      
      
              <a class="zhihu" aria-hidden="true"  href="http://www.zhihu.com/people/giscafer" target="_blank" title="知乎"></a>
      
      
            <a class="email" aria-hidden="true"  href="mailto:youemail@outlook.com" target="_blank" title="邮箱"></a>
      
    </div>
  </div>


  
    
  
    

  
    
  
    
  <div class="widget-wrap">
    <h3 class="widget-title recent-posts">最新文章</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2019/08/21/hive随笔5/">(no title)</a>
          </li>
        
          <li>
            <a href="/2019/08/21/hive随笔4/">(no title)</a>
          </li>
        
          <li>
            <a href="/2019/08/19/hive随笔3/">(no title)</a>
          </li>
        
          <li>
            <a href="/2019/08/19/hive随笔2/">(no title)</a>
          </li>
        
          <li>
            <a href="/2019/08/18/hive随笔1/">(no title)</a>
          </li>
        
      </ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title archive">归档</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/08/">August 2019</a><span class="archive-list-count">37</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/07/">July 2019</a><span class="archive-list-count">3</span></li></ul>
    </div>
  </div>


  
    
<div class="widget-wrap">
    <h3 class="widget-title">Links</h3>
    <div class="widget">
        <ul>
            
            <li>
                <a href="http://blog.giscafer.com">giscafer&#39;s blog</a>
            </li>
            
            <li>
                <a href="http://www.gis520.com">GIS520社区</a>
            </li>
            
        </ul>
    </div>
</div>

  
    <!--微信公众号二维码-->

  <div class="widget-wrap">
    <h3 class="follow-title ">WeChat</h3>
    <div class="widget wechat-widget">
        <img src="http://blog.giscafer.com/static/images/qrcode_giscafer.jpg" alt="扫码关注" width="250"/>
    </div>
  </div>


  
</aside>

      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-left">
      &copy; 2014 - 2019 WenYouWang&nbsp;|&nbsp;
      主题 <a href="https://github.com/giscafer/hexo-theme-cafe/" target="_blank">Cafe</a>
    </div>
     <div id="footer-right">
      联系方式&nbsp;|&nbsp;youemail@outlook.com
    </div>
  </div>
</footer>
 <script src="/jquery/jquery.min.js"></script>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">首页</a>
  
    <a href="/archives" class="mobile-nav-link">归档</a>
  
    <a href="/about" class="mobile-nav-link">关于</a>
  
</nav>
    <img class="back-to-top-btn" src="/images/fly-to-top.png"/>
<script>
// Elevator script included on the page, already.
window.onload = function() {
  var elevator = new Elevator({
    selector:'.back-to-top-btn',
    element: document.querySelector('.back-to-top-btn'),
    duration: 1000 // milliseconds
  });
}
</script>
      

  
    <script>
      var cloudTieConfig = {
        url: document.location.href, 
        sourceId: "",
        productKey: "e2fb4051c49842688ce669e634bc983f",
        target: "cloud-tie-wrapper"
      };
    </script>
    <script src="https://img1.ws.126.net/f2e/tie/yun/sdk/loader.js"></script>
    

  







<!-- author:forvoid begin -->
<!-- author:forvoid begin -->

<!-- author:forvoid end -->

<!-- author:forvoid end -->


  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      })
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      })
    </script>
    <script type="text/javascript" src="https://cdn.rawgit.com/mathjax/MathJax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


 <script src="/js/is.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


<script src="/js/script.js"></script>
<script src="/js/elevator.js"></script>
  </div>
</body>
</html>