<!DOCTYPE html>
<html>
<head><meta name="generator" content="Hexo 3.9.0">
  <meta charset="utf-8">
  
  <title>Hexo</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta property="og:type" content="website">
<meta property="og:title" content="Hexo">
<meta property="og:url" content="https://wenyouwang99.io/page/2/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:locale" content="default">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Hexo">
  
    <link rel="alternate" href="/atom.xml" title="Hexo" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    
  
  <link rel="stylesheet" href="/css/style.css">
  

</head>
</html>
<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    
    <div id="header-inner" class="inner">
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="搜索"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="https://wenyouwang99.io"></form>
      </div>
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">首页</a>
        
          <a class="main-nav-link" href="/archives">归档</a>
        
          <a class="main-nav-link" href="/about">关于</a>
        
      </nav>
      
    </div>
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Hexo</a>
      </h1>
      
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main">
  
    <article id="post-hive命令13" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2019/08/18/hive命令13/" class="article-date">
  <time datetime="2019-08-18T08:22:52.453Z" itemprop="datePublished">2019-08-18</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
    <div class="article-entry" itemprop="articleBody">
      
        <!-- Table of Contents -->
        
        <p>1.sqoop示例<br>(1)将表导入hdfs<br>sqoop import –connect jdbc:mysql://localhost/test –table test1 –username root –m 1<br>该命令将从mysql数据库test中导出表test1,并将其存放在hdfs的<br>/user/(user)/test1/part-m-00000中      //这行用()来代替&lt;&gt;,&lt;&gt;里代码不显示</p>
<p>(2)将表导入hdfs的特定目录下<br>sqoop import –connect jdbc:mysql://localhost/test –table test1 –username root –m 1<br>–target-dir /hive/tables/test1/<br>在本例中，表test1的内容将存放在hdfs的/hive/tables/test1目录下</p>
<p>(3)将数据库中的表全部导入hdfs<br>sqoop import-all-tables –connect jdbc:mysql://localhost/test –username root<br>该命令将test数据库中的所有表导入到hdfs中。sqoop导入作业在/user/root目录下为每个表<br>都创建了一个目录，如下<br>hadoop fs -ls /user/root</p>
<p>(4)将表导入hive<br>sqoop import –connect jdbc:mysql://localhost/test –table test1 –username root –m 1<br>–hive-import<br>该命令将把表test1导入到hdfs中，同时也会将其元数据添加到hive中。如下验证数据<br>use default;</p>
<p>select count(*) from test1;</p>
<p>(5)将表导入hive并且将表存储为orc表<br>sqoop import –connect jdbc:mysql://localhost/test –table test10 –username root –m 1<br>–hcatalog-database default –hcatalog-table test10_orc –create-hcatalog-table –hcatalog-<br>storage-stanza “stored as orcfile”<br>该命令将在默认数据库中创建一个名为test_orc的新表，并且以orc文件格式存储数据。<br>将hive表数据存储为orc，有理由 矢量化。验证如下<br>hive&gt;<br>describe extended test10_orc;</p>
<p>(6)导入所选择的数据<br>sqoop import –connect jdbc:mysql://localhost/test –table test10 –username root –m 1<br>–where “a&gt;1”<br>通过该命令，可以导入表test1中列a的所有值大于1的数据。<br>该选项提供了一种方法，可以导入任何表的一个子集</p>
<p>(7)导入增量数据<br>incremental  –sqoop使用它来判断哪一行是新的<br>check-column  –提供需要检查确定候选行的列<br>last-value  –执行上一次导入操作的最大值<br>sqoop import –connect jdbc:mysql://localhost/test –table test10 –username root –m 1<br>–incremental  append –check-column id -last-value 1000</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://wenyouwang99.io/2019/08/18/hive命令13/" data-id="cjznvd93u0008rgvg1srg480h" class="article-share-link">分享</a>
      
      
      
    </footer>
  </div>
  
</article>
 


  
    <article id="post-hive命令12" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2019/08/17/hive命令12/" class="article-date">
  <time datetime="2019-08-17T08:18:32.361Z" itemprop="datePublished">2019-08-17</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
    <div class="article-entry" itemprop="articleBody">
      
        <!-- Table of Contents -->
        
        <p>1.ACID<br>(1)原子性(A)<br>(2)一致性(C)<br>(3)隔离性(I)<br>(4)持久性(D)</p>
<p>2.HIVE的配置<br>hive.support.concurrency—true<br>hive.enforce.bucketing——true<br>hive.exec.dynamic.partition.mode—–true<br>hive.txn.manager—org.apache.hadoop.hive.ql.lockmgr.DbTxnManager<br>hive.compactor.initiator.on—在Thrift元存储服务的一个实例上为true<br>hive.compactor.worker.threads—在Thrift元存储服务的一个实例上为10</p>
<p>3.请使用如下建表格式  //这里字段作为举例<br>create table census.personname(<br>persid       int,<br>firstname  string,<br>lastname   string<br>)<br>clustered by (persid) into 1 buckets<br>stored as orc<br>tblproperties(‘transactional’ = ‘true’);</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://wenyouwang99.io/2019/08/17/hive命令12/" data-id="cjznvd93v0009rgvgymebvmtz" class="article-share-link">分享</a>
      
      
      
    </footer>
  </div>
  
</article>
 


  
    <article id="post-hive命令11" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2019/08/17/hive命令11/" class="article-date">
  <time datetime="2019-08-17T08:18:00.806Z" itemprop="datePublished">2019-08-17</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
    <div class="article-entry" itemprop="articleBody">
      
        <!-- Table of Contents -->
        
        <p>1.使用左半连接<br>hive支持表之间的嵌套连接，假设有这样的嵌套连接（如下）<br>select a.key,a.value<br>from a<br>where a.key in (select b.key from B);<br>由于采用分布式处理，该查询在hive中会失败<br>hive处理使用semi join命令<br>select table_fields<br>from table_one<br>left semi join table_two<br>on (table_one.key_one = table_two.key_one);<br>语法解释<br>from table_one left semi join table_two  –罗列检索table_fields而进行半连接操作的2个表<br>on (table_one.key_one = table_two.key_one)  –罗列2个表所需的等值规则</p>
<p>2.案例<br>目标：使用半连接操作，对两个表中的数据进行整合<br>use census;</p>
<p>select<br>    personname.firstname,<br>    personname.lastname<br>from<br>    census.personname<br>left semi join<br>    census.address<br>on<br>    (personname.persid = address.persid);</p>
<p>结果如下<br>Bob    Burger    KA13<br>Charlie    Clown    KA9</p>
<p>3.用单次mapreduce实现连接<br>select table_one.key_one,table_two.key_one,table_three.key_one<br>from table_one join table_two<br>on (table_one.key_one = table_two.key_one)<br>join table_three<br>on (table_three.key_one = table_two.key_one);<br>语法解释<br>(1)select table_one.key_one,table_two.key_one,table_three.key_one<br>    –选关键字<br>(2)from table_one join table_two<br>    –列出为检索table_fields所需连接的第一个表和第二个表<br>(3)on (table_one.key_one = table_two.key_one)<br>    –列出连接第一个表和第二个表的等值规则<br>(4)join table_three<br>    –列出检索table_fields所需连接的第三个表<br>(5)on (table_three.key_one = table_two.key_one)<br>    –列出连接第三个表的等值规则</p>
<p>4.案例，在一次mapreduce中连接3个表<br>use census;</p>
<p>create table census.account (<br>persid       int,<br>bamount  int<br>)<br>clustered by (persid) into 1 buckets<br>stored as orc<br>tblproperties(‘transactional’ = ‘true’);</p>
<p>insert into table census.personname<br>values<br>(1,12),<br>(2,9);</p>
<p>select<br>    personname.firstname,<br>    personname.lastname,<br>    address.postname,<br>    account.bamount<br>from<br>    census.personname<br>join<br>    census.address<br>on<br>    (personname.persid = address.persid)<br>join<br>    census.account<br>on<br>    (personname.persid = address.persid);</p>
<p>结果如下<br>Bob    Burger    KA13    12<br>Charlie    Clown    KA9    9</p>
<p>5.最后使用最大的表<br>hive在实施连接时可以先缓存前几个要连接的表，然后在针对它们映射最后一个表。<br>总是将最大的表放在后面时一种比较好的实践方法，因为这样做会加速处理过程。<br>(1)hive语法1：<br>select table_one.key_one,table_two.key_one,table_three.key_one<br>from table_one join table_two<br>on (table_one.key_one = table_two.key_one)<br>join table_three<br>on (table_three.key_one = table_two.key_one);<br>语法解释<br>(1)table_one,table_two<br>    –在内存中缓存<br>(2)table_three<br>    –直接从硬盘映射</p>
<p>(1)hive语法2：<br>select table_one.key_one,table_two.key_one,table_three.key_one<br>from table_one join table_three<br>on (table_one.key_one = table_three.key_one)<br>join table_two<br>on (table_two.key_one = table_three.key_one);<br>语法解释<br>(1)table_one,table_three<br>    –在内存中缓存<br>(2)table_two<br>    –直接从硬盘映射</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://wenyouwang99.io/2019/08/17/hive命令11/" data-id="cjznvd93t0007rgvgcqtv88j9" class="article-share-link">分享</a>
      
      
      
    </footer>
  </div>
  
</article>
 


  
    <article id="post-hive命令10" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2019/08/17/hive命令10/" class="article-date">
  <time datetime="2019-08-17T08:17:34.774Z" itemprop="datePublished">2019-08-17</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
    <div class="article-entry" itemprop="articleBody">
      
        <!-- Table of Contents -->
        
        <p>1.创建结构相同的表<br>create table table_name like table_name1;<br>语法解释<br>table_name  –要创建表的名字<br>table_name1  –hive中已有表的名字</p>
<p>2.案例<br>目标：使用person表的结构创建一个名为person40的表<br>use census;</p>
<p>create table person40 like person;</p>
<p>select * from person40;</p>
<p>插入数据（能插入就成功）<br>insert into table person40 values (0,’Bob’,’Burger’),(1,’Charlie’,’clown’);<br>select * from person40;   //此时有2行数据</p>
<p>3.连接<br>hive支持表之间的等值连接，使你能够整合来自两个表的数据<br>select table_fields<br>from table_one<br>join table_two<br>on (table_one.key_one = table_two.key_one<br>and table_one.key_two = table_two.key_two);<br>语法解释<br>(1)select table_fields   –用于从两个表中选取一系列字段的关键字<br>(2)from table_one join table_two   –罗列出两个为了检索table_fields而进行连接操作的表<br>(3)on (table_one.key_one = table_two.key_one  —列出连接两个表的等值规则<br>and table_one.key_two = table_two.key_two)</p>
<p>4.案例<br>目标：在表census.personname和表census.address之间创建连接<br>use census;</p>
<p>create table census.personname(<br>persid       int,<br>firstname  string,<br>lastname   string<br>)<br>clustered by (persid) into 1 buckets<br>stored as orc<br>tblproperties(‘transactional’ = ‘true’);</p>
<p>insert into table census.personname<br>values<br>(0,’Albert’,’ape’),<br>(1,’Bob’,’Burger’)<br>(2,’Charlie’,’Clown’),<br>(3,’Danny’,’Drywer’);</p>
<p>create table census.address(<br>persid        int,<br>postname  string<br>)<br>clustered by (persid) into 1 buckets<br>stored as orc<br>tblproperties(‘transactional’ = ‘true’);</p>
<p>insert into table census.address<br>values<br>(1,’KA13’),<br>(2,’KA9’),<br>(10,’SW1’);</p>
<p>现在执行连接操作<br>select<br>    personname.firstname,<br>    personname.lastname,<br>    address.postname<br>from<br>    census.personname<br>join<br>    census.address<br>on<br>    (personname.persid = address.persid);</p>
<p>结果如下<br>Bob    Burger    KA13<br>Charlie    Clown    KA9</p>
<p>5.使用外连接<br>hive支持采用left,right,full outer等连接方式实现表之间的等值连接，这其中无匹配的键<br>select table_fields<br>from table_one<br>[left,right,full outer]<br>join table_two<br>on (table_one.key_one = table_two.key_one<br>and table_one.key_two = table_two.key_two);<br>语法解释<br>(1)left –left 连接产生的结果包含表table_one中匹配where语句的字段值以及表<br>         table_two中匹配和不匹配where语句的字段值<br>(2)right –right 连接产生的结果包含表table_one中匹配where语句的字段值以及表<br>         table_one中匹配和不匹配where语句的字段值<br>(3)full outer –full outer 连接将返回表table_two和table_one中的字段值；当有不匹配<br>                      where语句的行时，其字段值为null<br>(4)on  –列出两个表的等值规则</p>
<p>6.案例<br>左连接方式<br>use census;</p>
<p>select<br>    personname.firstname,<br>    personname.lastname,<br>    address.postname<br>from<br>    census.personname<br>left join<br>    census.address<br>on<br>    (personname.persid = address.persid);</p>
<p>此时结果是4条记录<br>Albert    Ape    null<br>Bob    Burger    KA13<br>Charlie    Clown    KA9<br>Danny    Drywer    null</p>
<p>右连接方式<br>use census;</p>
<p>select<br>    personname.firstname,<br>    personname.lastname,<br>    address.postname<br>from<br>    census.personname<br>right join<br>    census.address<br>on<br>    (personname.persid = address.persid);</p>
<p>此时结果是3条记录<br>Bob    Burger    KA13<br>Charlie    Clown    KA9<br>Danny    Drywer    null</p>
<p>使用完全外连接<br>use census;</p>
<p>select<br>    personname.firstname,<br>    personname.lastname,<br>    address.postname<br>from<br>    census.personname<br>full outer join<br>    census.address<br>on<br>    (personname.persid = address.persid);</p>
<p>此时结果是5条记录<br>Albert    Ape    null<br>Bob    Burger    KA13<br>Charlie    Clown    KA9<br>Danny    Drywer    null<br>null    null    SW1</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://wenyouwang99.io/2019/08/17/hive命令10/" data-id="cjznvd93r0005rgvgxsikcsos" class="article-share-link">分享</a>
      
      
      
    </footer>
  </div>
  
</article>
 


  
    <article id="post-hive命令9" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2019/08/17/hive命令9/" class="article-date">
  <time datetime="2019-08-17T08:17:05.679Z" itemprop="datePublished">2019-08-17</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
    <div class="article-entry" itemprop="articleBody">
      
        <!-- Table of Contents -->
        
        <p>1.将查询到的数据写入文件系统<br>insert [overwrite]<br>directory directoryname<br>select select_fileds from from_statement;<br>语法解释：<br>(1)insert  –将数据向hive装载数据的关键字<br>(2)overwrite  –如果包含，支持用户将数据装载到一个早已建好的表中并且替换原来的数据<br>如果省略，支持用户将数据装载到一个早已建好的表中并且将新数据追加到原来的数据后面<br>(3)directory directoryname  —-directoryname 是hadoop分布式文件系统中已有的目录<br>                                        名称，使用hadoop fs -mkdir directoryname 来创建一个目录<br>(5)select  –可以是针对hive生态系统的任何select命令</p>
<p>2.使用已有表创建输出目录<br>hadoop fs -mkdir ‘exampleoutput’</p>
<p>hive</p>
<p>use census;</p>
<p>insert overwrite<br>directory ‘exampleoutput’<br>row format delimited fields terminated by ‘,’<br>select persid,firstname,lastname<br>from Person;</p>
<p>exits;</p>
<p>测试一下，是否所有的数据都已经装载<br>hadoop fs -cat ‘exampleoutput/000000_0’ </p>
<p>3.直接向表插入值<br>hive支持用一系列静态值直接将数据装载到表中<br>insert into table tablename values (row_values1),(row_values2);<br>语法解释<br>(1)insert  –将数据向hive装载数据的关键字<br>(2)table tablename  —-tablename是hive中已经存在的表的名称<br>           使用create table tablename语句<br>(3)values (row_values1),(row_values2)  –值(row_values1),(row_values2)是相同格式<br>                的单条记录，而不是表的记录</p>
<p>4.案例<br>目标：可以将一条记录直接插入到一个名为 personhub 的表中<br>use census;</p>
<p>insert into table personhub values (0);</p>
<p>测试一下数据是否加载<br>select persid from personhub where persid = 0 ;</p>
<p>5.直接更新表中的数据<br>update tablename set column = value [where expression] ;<br>语法解释<br>set column = value  –set 命令更新该列一个值<br>[where expression]  –where 可用于为不同的查询挑选特定列的值</p>
<p>6.案例<br>目标：更新person20表的数据<br>use census;</p>
<p>create table census.person20(<br>persid       int,<br>firstname  string,<br>lastname  string<br>)<br>clustered by (persid) into 1 buckets<br>stored as orc<br>tblproperties(‘transactional’ = ‘true’);</p>
<p>insert into table person20 values (0,’A’,’B’),(2,’x’,’y’);</p>
<p>测试数据是否已经插入<br>select * from census.person20;       //结果是2行上面插入的结果的记录</p>
<p>现在执行  更新  操作<br>use census;</p>
<p>update census.person20 set lastname = ‘SS’ where persid = 0 ;</p>
<p>select * from census.person20;   //结果是2行记录</p>
<p>7.在表中直接删除数据<br>delete tablename [where experssion] ;</p>
<p>8.案例<br>use census;</p>
<p>create table census.person20(<br>persid       int,<br>firstname  string,<br>lastname  string<br>)<br>clustered by (persid) into 1 buckets<br>stored as orc<br>tblproperties(‘transactional’ = ‘true’);</p>
<p>insert into table person30 values (0,’A’,’B’),(2,’x’,’y’);</p>
<p>测试数据是否已经插入<br>select * from census.person30;       //结果是2行上面插入的结果的记录</p>
<p>删除一条记录<br>use census;</p>
<p>delete from census.person30 where persid = 0 ;</p>
<p>select * from census.person30;   //结果是1条记录</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://wenyouwang99.io/2019/08/17/hive命令9/" data-id="cjznvd943000hrgvg9oc9zisc" class="article-share-link">分享</a>
      
      
      
    </footer>
  </div>
  
</article>
 


  
    <article id="post-hive命令8" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2019/08/16/hive命令8/" class="article-date">
  <time datetime="2019-08-16T10:26:17.214Z" itemprop="datePublished">2019-08-16</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
    <div class="article-entry" itemprop="articleBody">
      
        <!-- Table of Contents -->
        
        <p>1.将数据装载到表中<br>load data [local] inpath ‘filepath’ [overwrite] into table table_name;<br>(1)load data –向hive装载数据的关键字<br>(2)local  –如果包含该关键字，则支持用户从其本地文件装载数据<br>          如果省略该关键字，则从hadoop配置变量fs.default.name中设定的路径加载文件<br>(3)inpath ‘filrpath’ –如果使用local：file://user/hive/example (自己写的路径，随便写)<br>如果省略local：hdfs://namenode:9000/user/hive/example(这里举例区别不同)<br>(4)overwrite  –如果包含，支持用户将数据装载到一个早已建好的表中并且替换原来的数据<br>如果省略，支持用户将数据装载到一个早已建好的表中并且将新数据追加到原来的数据后面<br>(5)into table table_name  –tablename是hive中已经存在的表的名称<br>                使用create table tablename语句</p>
<p>2.hive的建表语法<br>create [external] table table_name(<br>a    int,<br>b    string,<br>c    string<br>)<br>partitioned by(非必选，创建分区表 dt string)<br>clustered by (userid) into 3000 buckets    //非必选，分桶子<br>row format delimited fields terminated by ‘\t’  //必选，指定列之间的分隔符<br>stored as rcfile  //非必选，指定文件的读取格式，默认textfile格式<br>location ‘/opt’;   //非必选，指定文件在hdfs上的存储路径，如果已经有文件，会自动加载，<br>默认在hive的warehouse下   </p>
<p>注意：还可以这样写<br>create [external] table table_name(<br>a int comment ‘姓名’,<br>b string comment ‘年龄’,<br>c string comment ‘班级’<br>)……</p>
<p>3.案例<br>创建一个新数据库<br>create database census;<br>使用该数据库<br>use census;<br>创建一个新表<br>create table person(<br>persid        int,<br>lastname    string,<br>firstname   string<br>)<br>row format delimited terminated by ‘,’;<br>将数据从csv(逗号文件，以逗号分隔，excel也能打开)文件装载到该新表<br>load data local inpath ‘file:///root/hive/example/person001’ overwrite into table person;<br>查看表中是否有数据<br>select persid,lastname,firstname from person limit 10;  //limit有助于观赏效果</p>
<p>4.使用查询装载数据<br>insert [overwrite]<br>table tablename [if not exists]<br>select select_fileds from from_statement;<br>语法解释：<br>(1)insert  –将数据向hive装载数据的关键字<br>(2)overwrite  –如果包含，支持用户将数据装载到一个早已建好的表中并且替换原来的数据<br>如果省略，支持用户将数据装载到一个早已建好的表中并且将新数据追加到原来的数据后面<br>(3)table tablename  —-tablename是hive中已经存在的表的名称<br>           使用create table tablename语句<br>(4)if not exists  –如果在命令中包含if not exists，那么hive命令将在当前数据库中创建一个          表，如果省略，当该表不存在时将执行失败<br>(5)select  –可以是针对hive生态系统的任何select命令</p>
<p>5.使用已有表创建新表，案例<br>使用已有数据库<br>use census;<br>创建新表<br>create table personhub(<br>persid  int<br>);<br>将数据插入新表，覆盖表中已有数据<br>insert overwrite<br>table tablename<br>select Distinct personID from Person;<br>//SELECT DISTINCT 表示查询结果中，去掉了重复的行；Distinct表示去掉重复的行<br>检查数据是否已在表中(测试数据假设是80条)<br>select persid from personhub limit 10;</p>
<p>现在再次上传数据并测试去除overwrite参数后的执行情况<br>use census;<br>insert overwrite table personhub select distinct persid from Person;<br>查看是否所有数据都已经加载，而且没有删除之前的数据<br>select persid from personhub;  //此时数据是80*2=160条</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://wenyouwang99.io/2019/08/16/hive命令8/" data-id="cjznvd942000grgvgb270eehw" class="article-share-link">分享</a>
      
      
      
    </footer>
  </div>
  
</article>
 


  
    <article id="post-hive命令7" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2019/08/16/hive命令7/" class="article-date">
  <time datetime="2019-08-16T10:25:35.546Z" itemprop="datePublished">2019-08-16</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
    <div class="article-entry" itemprop="articleBody">
      
        <!-- Table of Contents -->
        
        <p>1.重命名分区<br>alter table ids partition (datestamp=’2016-05-31’) rename to<br>partition (datestamp=’31-05-2016’);</p>
<p>show partitions ids;</p>
<p>datestamp=2016-05-30 (修改前)<br>datestamp=31-05-2016 (修改后)</p>
<p>2.修改列–添加列<br>在schema-on-read架构上，有一个关键需求，就是要能够修改模式或表的元数据。<br>用户可以在表上定义各种类型的元数据，而且修改这些元数据时，不需要担心修改<br>底层数据（只对外部表）。<br>alter table retail.transactions add columns (loyalty_card boolean);<br>新列将被添加到当前列之后、分区列之前。分区列的值来自于分区定义，并不是<br>存放在数据文件本身之中，也不在create table命令中列的列表之中。<br>虽然实际上分区列并没有嵌入到数据本身之中，但是当你执行select * 语句时，分区列<br>总是会出现在列的列表的最后。</p>
<p>3.删除表<br>drop table 删除 hive 表时，表的元数据也会被删除<br>然而，hive 中的删除：我们只需要删除 受控表 中的数据<br>如果配置/etc/hadoop/conf/core-site.xml中的fs.trash.interval参数<br>那么在用drop table 删除 hive 表时,该表的数据就会被移动到/user/$USER/.trash文件夹下<br>drop table table_name;<br>如果你还想从trash中删除它，那么可以加入关键字purge<br>drop table table_name purge;</p>
<p>4.删除分区,只有当表是受控表时，hive才能删除真实的分区数据，下面2种删除方式(一样)<br>alter table transactions drop partition (store=’oakdrive’);<br>dropped the partition store=oakdrive;<br>本例中，数据仍然存放在hdfs上（假设你使用了一个外部表），但是针对该事务处理表的<br>查询无法再读取该分区。因此查询结果集中没有含有store=oakdrive的行，因为这个表<br>中已经没有这个分区了</p>
<p>5.保护表/分区<br>alter table enable no_drop防止用户删除hive中的表<br>案例：如何在hive更改一个表，以避免它被删除<br>alter table transactions enable no_drop;<br>drop table transactions ;     //这个命令会报错</p>
<p>案例：使某个表处于离线状态，以防止该表的数据被用户查询到，同时这并不会对另一个<br>访问同一底层数据的表造成影响<br>alter table transactions enable offline;<br>select * from transactions ;   //这个命令会报错</p>
<p>案例：你可以运行2个分区层命令，按照以下方式指定分区名称<br>alter table table_name partition partition _spec enable offline;</p>
<p>6.其他create table 命令选项<br>(1)使用 create table as select (ctas) 命令，利用其结果集和查询输出模式来创建一个内部表<br>create table retail.transactions _top100<br>as<br>select * from retail.transactions where custid&lt;101;</p>
<p>你可以使用这一特性来抽取某个表的子集，并且以另一种格式将该子集存放在一个新表中。<br>下面给出例子，实现：它为目标表指定了一种新格式<br>create table retail.transactions _top100 stored as orcfile<br>as<br>select * from retail.transactions where custid&lt;101;<br>注意：在ctas命令中，hive对目标表的格式有一些限制。新的目标表不能是外部表、分区表或<br>分桶表。</p>
<p>(2)create table like 命令<br>如果向复制某个已有表的模式而不复制它的数据，可以使用这个命令<br>create table transactions_test like transactions;</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://wenyouwang99.io/2019/08/16/hive命令7/" data-id="cjznvd940000drgvg54d88yrr" class="article-share-link">分享</a>
      
      
      
    </footer>
  </div>
  
</article>
 


  
    <article id="post-hive命令6" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2019/08/16/hive命令6/" class="article-date">
  <time datetime="2019-08-16T10:24:45.414Z" itemprop="datePublished">2019-08-16</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
    <div class="article-entry" itemprop="articleBody">
      
        <!-- Table of Contents -->
        
        <p>1.更改表分区<br>(1)为已有表添加分区，首先为外部表创建一个目录，并且在hdfs上创建2个分区<br>alter table add partition</p>
<p>hadoop fs -mkdir /user/demo/ids<br>hadoop fs -mkdir /user/demo/ids/2016-05-31<br>hadoop fs -mkdir /user/demo/ids/2016-05-30</p>
<p>将数据复制到这些目录下</p>
<p>hadoop fs -put /tmp/2016-05-31.txt /user/demo/ids/2016-05-31/<br>hadoop fs -put /tmp/2016-05-30.txt /user/demo/ids/2016-05-30/</p>
<p>创建外部表并且为其添加分区</p>
<p>create external table ids(a int) partitioned by (datestamp string) location’/user/demo/ids’;</p>
<p>为表添加分区<br>alter table ids add partition (datestamp=’2016-05-30’) location ‘/user/demo/ids/2016-05-30/‘;</p>
<p>hive&gt;select * from ids;</p>
<p>11    2016-05-30<br>12    2016-05-30<br>13    2016-05-30<br>14    2016-05-30</p>
<p>alter table ids add partition (datestamp=’2016-05-31’) location ‘/user/demo/ids/2016-05-31/‘;</p>
<p>hive&gt;select * from ids;</p>
<p>11    2016-05-30<br>12    2016-05-30<br>13    2016-05-30<br>14    2016-05-30<br>1    2016-05-31<br>2    2016-05-31<br>3    2016-05-31<br>4    2016-05-31</p>
<p>(2)为内部表添加分区<br>创建内部表<br>msck pepair table</p>
<p>create table ids_internal (a int) partitioned by (datestamp string);</p>
<p>为2个不同的分区插入几行数据</p>
<p>insert into ids_internal partition (datestamp = ‘2016-05-30’) value (1);<br>insert into ids_internal partition (datestamp = ‘2016-05-3’) value (11);</p>
<p>show partitions ids_internal;</p>
<p>datestamp=2016-05-30<br>datestamp=2016-05-31</p>
<p>我们将在该表的目录下创建一个新的子目录并且为其添加一个文件</p>
<p>hadoop fs -mkdir /apps/hive/warehouse/ids_internal/datestamp=2016-05-21 </p>
<p>hadoop fs -put /tmp/2016-05-21.txt /apps/hive/warehouse/ids_internal/datestamp<br>=2016-05-21 </p>
<p>现在运行msck pepair table命令来为该表添加新分区</p>
<p>msck pepair table ids_internal;</p>
<p>show partitions ids_internal;</p>
<p>datestamp=2016-05-21<br>datestamp=2016-05-30<br>datestamp=2016-05-31</p>
<p>msck pepair 命令为ids_internal表检查/apps/hive/warehouse/ids_internal下的子目录，<br>而且因为它找到了一个名为datestamp=2016-05-21的新目录，所以它会将该子目录作为<br>一个新分区添加到ids_internal表。当你添加了很多新的分区目录，并且想要一次性全部更新<br>它们的表定义时，这种方法尤其有用。注意：这种方式仅对内部表有效。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://wenyouwang99.io/2019/08/16/hive命令6/" data-id="cjznvd941000frgvgbmkvlh1a" class="article-share-link">分享</a>
      
      
      
    </footer>
  </div>
  
</article>
 


  
    <article id="post-hive命令5" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2019/08/15/hive命令5/" class="article-date">
  <time datetime="2019-08-15T09:23:13.126Z" itemprop="datePublished">2019-08-15</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
    <div class="article-entry" itemprop="articleBody">
      
        <!-- Table of Contents -->
        
        <h2 id="hive命令5"><a href="#hive命令5" class="headerlink" title="hive命令5"></a>hive命令5</h2><p>1.分桶<br>hive中的分桶是另一种将数据切分为更小片段的方式，<br>然而，高效的分区要求采用分区键，并不会导致出现大量的非常小的分区<br>因此，对于你的分区键有很多的值，但是分区键的每个值都没有多少行时，那么分区<br>并不是最佳选择，此时分桶很适合这种情形</p>
<p>2.分桶可以让你为每个表的分桶列定义桶的最大数目，hive中的一个分区就是一个目录，<br>分区键的值存放在实际的分区目录名中，而分区键是表中的一个虚拟列。<br>然而，在分桶中，每个桶都是一个保存实际数据的文件，这些数据基于一种散列算法进行<br>分割。分桶并不会为当前表添加一个虚拟列。</p>
<p>3.分桶的优势，最主要的一点就是能够提升多种查询的性能，如果分桶所用的键是非倾斜的，<br>那么对你的数据将会均衡分布，这一点可以用于实现高效的数据抽样。</p>
<p>4.分桶代码案例<br>创建customers表，并且将创建的custid列作为一个分桶列，将其分割成11个桶。<br>create external table customers(<br>custid     int,<br>fname    string,<br>lname    string<br>)<br>clustered by (custid) into 11 buckets<br>location ‘/opt/customers’;</p>
<p>现在，当你向该表插入数据时，Hive将custid用于散列函数，将数据分发到11个桶当中。<br>对于有些数据类型来说，就意味着哪写含有相同custid值的行将被存放在相同的桶中。</p>
<p>注意：要设置<br>hive.enforce.bucketing=true<br>没有这个参数，就需要为表定义与桶的数量相同的映射器。</p>
<p>5.分桶的注意事项<br>(1)选择唯一值的个数较多的桶键，这样会减小出现倾斜的可能性<br>(2)采用质数作为桶的编号<br>(3)如果桶键中的数据时倾斜的，为倾斜的值单独创建桶，这可以通过列表分桶来实现<br>(4)分桶对于通常连接在一起的事实表来说非常有用<br>(5)需要连接在一起的表，其桶的数目必须相同，或者一个表的桶数是另一个表的桶数的因子<br>(6)要仔细选择桶的数目，一个cpu核只会对一个桶进行写入操作，因此对于一个大型集群而言，如果桶的数目很小，则集群的利用严重不足<br>(7)一旦表建好，桶的数目就不能改变了<br>(8)仔细选择进行分桶的列，因为散列函数会引发倾斜，字符串散列更有这种倾向，因为通常使用的字符串子集很小。例如，如果桶键含有abc789,abc567,abc123三个值，但是散列算法在计算候选桶时仅仅用到了前3个字符(abc),那么最后这3个值都会在同一个桶中<br>(9)桶文件的大小至少是1GB<br>(10)通过设置hive.enforce.bucketing=true实现强制分桶<br>(11)对于Map端连接，分桶表要比非分桶表的速度更快。在Map端连接中，映射器处理左侧表的某个桶时，知道右侧表中相匹配的行在其对应的桶中，因此只需要检索该桶即可，这只是右侧表中存储的全部数据中的一小部分<br>(12)分桶也允许你按照一列或多列对每个桶中的数据进行排序，这样就可以把Map端连接转换成排序-合并连接，使速度更快</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://wenyouwang99.io/2019/08/15/hive命令5/" data-id="cjznvd941000ergvgsqfijxm3" class="article-share-link">分享</a>
      
      
      
    </footer>
  </div>
  
</article>
 


  
    <article id="post-hive命令4" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2019/08/15/hive命令4/" class="article-date">
  <time datetime="2019-08-15T09:22:36.897Z" itemprop="datePublished">2019-08-15</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
    <div class="article-entry" itemprop="articleBody">
      
        <!-- Table of Contents -->
        
        <h2 id="hive命令4"><a href="#hive命令4" class="headerlink" title="hive命令4"></a>hive命令4</h2><p>1.分区注意事项<br>(1)挑选一列作为分区键，其唯一值的个数应在较低值到中间值之间<br>(2)避免分区小于1GB（越大越好）<br>(3)当分区数量较多时，调整HiveServer2和Hive Metastore的内存<br>(4)当使用多列作为分区键时，对于每一个分区键列的组合都要创建一个子目录的<br>嵌套树。应该避免深入嵌套，因为这回导致太多的分区，进而使创建的文件非常小<br>(5)当使用hive流处理插入数据时，如果多个会话向相同的分区写入数据，那么就会导致锁闭<br>(6)可以修改某一分区表的模式，但是，一旦结构发生变化，就无法在已有分区中修改数据了<br>(7)如果要将数据并行插入到多个分区，应该将<br>hive.optimize.sort.dynamic.partition设置为true</p>
<p>2.对日期列进行高效分区<br>如：日期中按照”YYYY-MM-DD”格式比按照 年月日各自的值进行多深度分区在某些时候<br>更加高效。<br>使用单个字符串的优势：它允许使用更多的SQL运算符，例如：LIKE,IN,BETWEEN，<br>但是如果采用嵌套分区，就不太容易使用这些运算符了</p>
<p>3.案例<br>假设有表A，通过DataStamp字符串按照”YYYY-MM-DD”格式进行分区，SQL查询如下</p>
<p>查询选择特定日期：<br>select * from table A where DataStamp IN (‘2015-01-01’,’2016-02-02’,’2017-03-03’);</p>
<p>查询一年中所有的日期<br>select * from table A where DataStamp LIKE ‘2018-%’;</p>
<p>查询一年中特定月的所有日期<br>select * from table A where DataStamp LIKE ‘2019-02-%’;</p>
<p>查询所有以5开始/结束的日子<br>select * from table A where DataStamp LIKE ‘%-%-%5’;</p>
<p>查询2015年1月1日到2015年3月1日的所有日子<br>select * from table A where DataStamp BETWEEN ‘2015-01-01’ AND ‘2015-03-01’;</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://wenyouwang99.io/2019/08/15/hive命令4/" data-id="cjznvd93z000crgvg351zfa3l" class="article-share-link">分享</a>
      
      
      
    </footer>
  </div>
  
</article>
 


  


  <nav id="page-nav">
    <a class="extend prev" rel="prev" href="/">&laquo; 上一页</a><a class="page-number" href="/">1</a><span class="page-number current">2</span><a class="page-number" href="/page/3/">3</a><a class="page-number" href="/page/4/">4</a><a class="page-number" href="/page/5/">5</a><a class="extend next" rel="next" href="/page/3/">下一页&raquo;</a>
  </nav>
</section>
           
    <aside id="sidebar">
  
    

  
    
  
    

  
    
  
    
  <div class="widget-wrap">
    <h3 class="widget-title recent-posts">最新文章</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2019/08/25/hbase命令1/">(no title)</a>
          </li>
        
          <li>
            <a href="/2019/08/23/sqoop/">(no title)</a>
          </li>
        
          <li>
            <a href="/2019/08/23/数据仓库/">(no title)</a>
          </li>
        
          <li>
            <a href="/2019/08/22/hive随笔7/">(no title)</a>
          </li>
        
          <li>
            <a href="/2019/08/22/hive随笔6/">(no title)</a>
          </li>
        
      </ul>
    </div>
  </div>

  
    

  
    
  
    <!--微信公众号二维码-->


  
</aside>

      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-left">
      &copy; 2014 - 2019 WenYouWang&nbsp;|&nbsp;
      主题 <a href="https://github.com/giscafer/hexo-theme-cafe/" target="_blank">Cafe</a>
    </div>
     <div id="footer-right">
      联系方式&nbsp;|&nbsp;youemail@outlook.com
    </div>
  </div>
</footer>
 <script src="/jquery/jquery.min.js"></script>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">首页</a>
  
    <a href="/archives" class="mobile-nav-link">归档</a>
  
    <a href="/about" class="mobile-nav-link">关于</a>
  
</nav>
    <img class="back-to-top-btn" src="/images/fly-to-top.png"/>
<script>
// Elevator script included on the page, already.
window.onload = function() {
  var elevator = new Elevator({
    selector:'.back-to-top-btn',
    element: document.querySelector('.back-to-top-btn'),
    duration: 1000 // milliseconds
  });
}
</script>
      

  
    <script>
      var cloudTieConfig = {
        url: document.location.href, 
        sourceId: "",
        productKey: "e2fb4051c49842688ce669e634bc983f",
        target: "cloud-tie-wrapper"
      };
    </script>
    <script src="https://img1.ws.126.net/f2e/tie/yun/sdk/loader.js"></script>
    

  







<!-- author:forvoid begin -->
<!-- author:forvoid begin -->

<!-- author:forvoid end -->

<!-- author:forvoid end -->


  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      })
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      })
    </script>
    <script type="text/javascript" src="https://cdn.rawgit.com/mathjax/MathJax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


 <script src="/js/is.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


<script src="/js/script.js"></script>
<script src="/js/elevator.js"></script>
  </div>
</body>
</html>