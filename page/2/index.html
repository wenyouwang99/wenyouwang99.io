<!DOCTYPE html>
<html>
<head><meta name="generator" content="Hexo 3.9.0">
  <meta charset="utf-8">
  
  <title>Hexo</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta property="og:type" content="website">
<meta property="og:title" content="Hexo">
<meta property="og:url" content="https://wenyouwang99.io/page/2/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:locale" content="default">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Hexo">
  
    <link rel="alternate" href="/atom.xml" title="Hexo" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    
  
  <link rel="stylesheet" href="/css/style.css">
  

</head>
</html>
<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    
    <div id="header-inner" class="inner">
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="搜索"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="https://wenyouwang99.io"></form>
      </div>
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">首页</a>
        
          <a class="main-nav-link" href="/archives">归档</a>
        
          <a class="main-nav-link" href="/about">关于</a>
        
      </nav>
      
    </div>
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Hexo</a>
      </h1>
      
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main">
  
    <article id="post-hive命令6" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2019/08/16/hive命令6/" class="article-date">
  <time datetime="2019-08-16T10:24:45.414Z" itemprop="datePublished">2019-08-16</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
    <div class="article-entry" itemprop="articleBody">
      
        <!-- Table of Contents -->
        
        <p>1.更改表分区<br>(1)为已有表添加分区，首先为外部表创建一个目录，并且在hdfs上创建2个分区<br>alter table add partition</p>
<p>hadoop fs -mkdir /user/demo/ids<br>hadoop fs -mkdir /user/demo/ids/2016-05-31<br>hadoop fs -mkdir /user/demo/ids/2016-05-30</p>
<p>将数据复制到这些目录下</p>
<p>hadoop fs -put /tmp/2016-05-31.txt /user/demo/ids/2016-05-31/<br>hadoop fs -put /tmp/2016-05-30.txt /user/demo/ids/2016-05-30/</p>
<p>创建外部表并且为其添加分区</p>
<p>create external table ids(a int) partitioned by (datestamp string) location’/user/demo/ids’;</p>
<p>为表添加分区<br>alter table ids add partition (datestamp=’2016-05-30’) location ‘/user/demo/ids/2016-05-30/‘;</p>
<p>hive&gt;select * from ids;</p>
<p>11    2016-05-30<br>12    2016-05-30<br>13    2016-05-30<br>14    2016-05-30</p>
<p>alter table ids add partition (datestamp=’2016-05-31’) location ‘/user/demo/ids/2016-05-31/‘;</p>
<p>hive&gt;select * from ids;</p>
<p>11    2016-05-30<br>12    2016-05-30<br>13    2016-05-30<br>14    2016-05-30<br>1    2016-05-31<br>2    2016-05-31<br>3    2016-05-31<br>4    2016-05-31</p>
<p>(2)为内部表添加分区<br>创建内部表<br>msck pepair table</p>
<p>create table ids_internal (a int) partitioned by (datestamp string);</p>
<p>为2个不同的分区插入几行数据</p>
<p>insert into ids_internal partition (datestamp = ‘2016-05-30’) value (1);<br>insert into ids_internal partition (datestamp = ‘2016-05-3’) value (11);</p>
<p>show partitions ids_internal;</p>
<p>datestamp=2016-05-30<br>datestamp=2016-05-31</p>
<p>我们将在该表的目录下创建一个新的子目录并且为其添加一个文件</p>
<p>hadoop fs -mkdir /apps/hive/warehouse/ids_internal/datestamp=2016-05-21 </p>
<p>hadoop fs -put /tmp/2016-05-21.txt /apps/hive/warehouse/ids_internal/datestamp<br>=2016-05-21 </p>
<p>现在运行msck pepair table命令来为该表添加新分区</p>
<p>msck pepair table ids_internal;</p>
<p>show partitions ids_internal;</p>
<p>datestamp=2016-05-21<br>datestamp=2016-05-30<br>datestamp=2016-05-31</p>
<p>msck pepair 命令为ids_internal表检查/apps/hive/warehouse/ids_internal下的子目录，<br>而且因为它找到了一个名为datestamp=2016-05-21的新目录，所以它会将该子目录作为<br>一个新分区添加到ids_internal表。当你添加了很多新的分区目录，并且想要一次性全部更新<br>它们的表定义时，这种方法尤其有用。注意：这种方式仅对内部表有效。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://wenyouwang99.io/2019/08/16/hive命令6/" data-id="cjzgpja8r000e4gvgz9ih7d03" class="article-share-link">分享</a>
      
      
      
    </footer>
  </div>
  
</article>
 


  
    <article id="post-hive命令5" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2019/08/15/hive命令5/" class="article-date">
  <time datetime="2019-08-15T09:23:13.126Z" itemprop="datePublished">2019-08-15</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
    <div class="article-entry" itemprop="articleBody">
      
        <!-- Table of Contents -->
        
        <h2 id="hive命令5"><a href="#hive命令5" class="headerlink" title="hive命令5"></a>hive命令5</h2><p>1.分桶<br>hive中的分桶是另一种将数据切分为更小片段的方式，<br>然而，高效的分区要求采用分区键，并不会导致出现大量的非常小的分区<br>因此，对于你的分区键有很多的值，但是分区键的每个值都没有多少行时，那么分区<br>并不是最佳选择，此时分桶很适合这种情形</p>
<p>2.分桶可以让你为每个表的分桶列定义桶的最大数目，hive中的一个分区就是一个目录，<br>分区键的值存放在实际的分区目录名中，而分区键是表中的一个虚拟列。<br>然而，在分桶中，每个桶都是一个保存实际数据的文件，这些数据基于一种散列算法进行<br>分割。分桶并不会为当前表添加一个虚拟列。</p>
<p>3.分桶的优势，最主要的一点就是能够提升多种查询的性能，如果分桶所用的键是非倾斜的，<br>那么对你的数据将会均衡分布，这一点可以用于实现高效的数据抽样。</p>
<p>4.分桶代码案例<br>创建customers表，并且将创建的custid列作为一个分桶列，将其分割成11个桶。<br>create external table customers(<br>custid     int,<br>fname    string,<br>lname    string<br>)<br>clustered by (custid) into 11 buckets<br>location ‘/opt/customers’;</p>
<p>现在，当你向该表插入数据时，Hive将custid用于散列函数，将数据分发到11个桶当中。<br>对于有些数据类型来说，就意味着哪写含有相同custid值的行将被存放在相同的桶中。</p>
<p>注意：要设置<br>hive.enforce.bucketing=true<br>没有这个参数，就需要为表定义与桶的数量相同的映射器。</p>
<p>5.分桶的注意事项<br>(1)选择唯一值的个数较多的桶键，这样会减小出现倾斜的可能性<br>(2)采用质数作为桶的编号<br>(3)如果桶键中的数据时倾斜的，为倾斜的值单独创建桶，这可以通过列表分桶来实现<br>(4)分桶对于通常连接在一起的事实表来说非常有用<br>(5)需要连接在一起的表，其桶的数目必须相同，或者一个表的桶数是另一个表的桶数的因子<br>(6)要仔细选择桶的数目，一个cpu核只会对一个桶进行写入操作，因此对于一个大型集群而言，如果桶的数目很小，则集群的利用严重不足<br>(7)一旦表建好，桶的数目就不能改变了<br>(8)仔细选择进行分桶的列，因为散列函数会引发倾斜，字符串散列更有这种倾向，因为通常使用的字符串子集很小。例如，如果桶键含有abc789,abc567,abc123三个值，但是散列算法在计算候选桶时仅仅用到了前3个字符(abc),那么最后这3个值都会在同一个桶中<br>(9)桶文件的大小至少是1GB<br>(10)通过设置hive.enforce.bucketing=true实现强制分桶<br>(11)对于Map端连接，分桶表要比非分桶表的速度更快。在Map端连接中，映射器处理左侧表的某个桶时，知道右侧表中相匹配的行在其对应的桶中，因此只需要检索该桶即可，这只是右侧表中存储的全部数据中的一小部分<br>(12)分桶也允许你按照一列或多列对每个桶中的数据进行排序，这样就可以把Map端连接转换成排序-合并连接，使速度更快</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://wenyouwang99.io/2019/08/15/hive命令5/" data-id="cjzgpja8p000c4gvg15sjob52" class="article-share-link">分享</a>
      
      
      
    </footer>
  </div>
  
</article>
 


  
    <article id="post-hive命令4" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2019/08/15/hive命令4/" class="article-date">
  <time datetime="2019-08-15T09:22:36.897Z" itemprop="datePublished">2019-08-15</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
    <div class="article-entry" itemprop="articleBody">
      
        <!-- Table of Contents -->
        
        <h2 id="hive命令4"><a href="#hive命令4" class="headerlink" title="hive命令4"></a>hive命令4</h2><p>1.分区注意事项<br>(1)挑选一列作为分区键，其唯一值的个数应在较低值到中间值之间<br>(2)避免分区小于1GB（越大越好）<br>(3)当分区数量较多时，调整HiveServer2和Hive Metastore的内存<br>(4)当使用多列作为分区键时，对于每一个分区键列的组合都要创建一个子目录的<br>嵌套树。应该避免深入嵌套，因为这回导致太多的分区，进而使创建的文件非常小<br>(5)当使用hive流处理插入数据时，如果多个会话向相同的分区写入数据，那么就会导致锁闭<br>(6)可以修改某一分区表的模式，但是，一旦结构发生变化，就无法在已有分区中修改数据了<br>(7)如果要将数据并行插入到多个分区，应该将<br>hive.optimize.sort.dynamic.partition设置为true</p>
<p>2.对日期列进行高效分区<br>如：日期中按照”YYYY-MM-DD”格式比按照 年月日各自的值进行多深度分区在某些时候<br>更加高效。<br>使用单个字符串的优势：它允许使用更多的SQL运算符，例如：LIKE,IN,BETWEEN，<br>但是如果采用嵌套分区，就不太容易使用这些运算符了</p>
<p>3.案例<br>假设有表A，通过DataStamp字符串按照”YYYY-MM-DD”格式进行分区，SQL查询如下</p>
<p>查询选择特定日期：<br>select * from table A where DataStamp IN (‘2015-01-01’,’2016-02-02’,’2017-03-03’);</p>
<p>查询一年中所有的日期<br>select * from table A where DataStamp LIKE ‘2018-%’;</p>
<p>查询一年中特定月的所有日期<br>select * from table A where DataStamp LIKE ‘2019-02-%’;</p>
<p>查询所有以5开始/结束的日子<br>select * from table A where DataStamp LIKE ‘%-%-%5’;</p>
<p>查询2015年1月1日到2015年3月1日的所有日子<br>select * from table A where DataStamp BETWEEN ‘2015-01-01’ AND ‘2015-03-01’;</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://wenyouwang99.io/2019/08/15/hive命令4/" data-id="cjzgpja8q000d4gvgp8v077ba" class="article-share-link">分享</a>
      
      
      
    </footer>
  </div>
  
</article>
 


  
    <article id="post-hive命令3" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2019/08/15/hive命令3/" class="article-date">
  <time datetime="2019-08-15T09:21:51.256Z" itemprop="datePublished">2019-08-15</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
    <div class="article-entry" itemprop="articleBody">
      
        <!-- Table of Contents -->
        
        <h2 id="hive命令3"><a href="#hive命令3" class="headerlink" title="hive命令3"></a>hive命令3</h2><p>1.重命名表   –stud:表名<br>create external table stud (state string) location ‘/opt/stud’;</p>
<p>alter table stud rename to stud_old; </p>
<p>2.修改表的存储属性<br>(1)alter table<br>(2)删除该表，然后重新创建表，修改create table中的存储属性</p>
<p>3.orc文件格式<br>将states表转换成orc文件格式并且查看其属性<br>create table states_orc stored as orc tblproperties(“orc.compress”=”snappy”)<br>as select * from states;</p>
<p>4.hadoop中处理小文件将会大量耗费大量的namenode元数据记录，所以通常合并小文件<br>hdfs的namenode进程维护了hdfs上所有文件的元数据<br>对于以rcfile或orcfile格式存储的hive表，可以做下面的操作，将多个数据文件合并成一个<br>alter table states concatenate;</p>
<p>5.避免小文件的方式之一（推荐），在数据进入hadoop之前，将它们合并成集群数据块<br>数倍大小的文件，通常为好几GB或者更大。在数据进入hadoop之后，有很多的方式整合，<br>但hadoop处理小文件并不在行，因此这样合并速度会比较慢。</p>
<p>6.hive表分区<br>hive中的分区表有一个或多个分区键，基于这些键，数据被分割成若干逻辑块并存放在单独<br>的路径中。每个分区键都为表的存储添加了一个目录层结构。</p>
<p>7.hive表分区代码案例<br>create external table retail.transactions(<br>transdate  date,<br>transid       int,<br>custid        int,<br>fname       string     //注意：最后一个属性不要打逗号<br>)<br>partitioned by (store string);<br>本例中的表基于一个名为store(存放商店的名称)的字符串列进行分区。<br>注意：在分区中用到的列实际上在create table 结构中并不存在。</p>
<p>8.当你查询一个分区表时，分区的值会作为该分区中所有行在这一列的值，例<br>select * from retail.transactions 将返回store列的值，即使数据库中兵没有<br>存储这样的数据。</p>
<p>9.创建分区表需要你预先为底层分区创建目录结构。对于内部表的情形，当你使用insert<br>命令将数据插入新分区时，分区目录是自动创建的。</p>
<p>10.注意：不能在实际的表定义中加入分区键列，否则报错<br>failed:error in semantic analysis:column repeated in patitioning columns错误</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://wenyouwang99.io/2019/08/15/hive命令3/" data-id="cjzgpja8o000b4gvgd255f630" class="article-share-link">分享</a>
      
      
      
    </footer>
  </div>
  
</article>
 


  
    <article id="post-MapReduce_Join" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2019/08/14/MapReduce_Join/" class="article-date">
  <time datetime="2019-08-14T12:01:19.908Z" itemprop="datePublished">2019-08-14</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
    <div class="article-entry" itemprop="articleBody">
      
        <!-- Table of Contents -->
        
        <p>## </p>
<p>package hadoop;<br>//需求：城市区域id,城市id,城市名<br>/**</p>
<ul>
<li>数据样式：</li>
<li>用户手机号码，时间戳，城市id，城市区域id，所在区域停留时长，开始进入区域的时间，离开区域的时间，日期</li>
<li>数据进入hdfs的方式：put</li>
<li></li>
<li>/<br>import java.io.IOException;<br>import java.util.Iterator;<br>import java.util.Vector;</li>
</ul>
<p>import org.apache.hadoop.conf.Configuration;<br>import org.apache.hadoop.fs.Path;<br>import org.apache.hadoop.io.LongWritable;<br>import org.apache.hadoop.io.Text;<br>import org.apache.hadoop.mapred.FileSplit;<br>import org.apache.hadoop.mapreduce.InputSplit;<br>import org.apache.hadoop.mapreduce.Job;<br>import org.apache.hadoop.mapreduce.Mapper;<br>import org.apache.hadoop.mapreduce.Reducer;<br>import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;<br>import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</p>
<p>public class MapReduce_Join {</p>
<pre><code>public static class MyMapper extends Mapper&lt;LongWritable, Text, Text, Text&gt;{

    protected void map(LongWritable key, Text value, org.apache.hadoop.mapreduce.Mapper&lt;LongWritable, Text, Text, Text&gt;.Context context) throws java.io.IOException ,InterruptedException 
    {
        //获取切片信息
        InputSplit inputSplit = (FileSplit)context.getInputSplit();
        //获取路径信息
        String path = ((FileSplit) inputSplit).getPath().toString();
        //====打标签===区分两个输入路径的数据
        //判断是否是data的数据


        if(path.contains(&quot;data&quot;)) {
            String line = value.toString();
            String [] split = line.split(&quot;\t&quot;);
            if(split.length==8 &amp;&amp; split[2] != null) {
                String JoinKey = split[2];
                //将value打上标记
                String joinValue = &quot;data&quot;+split[0] + split[4];
                context.write(new Text(JoinKey), new Text(joinValue));
                //数据此时会变成
                //83522     data.txt,手机号码,停留时间    
            }
        }    
        //给city_ID的数据打标记
        if(path.contains(&quot;city_id&quot;)) {
            String line = value.toString();
            String [] split = line.split(&quot;,&quot;);
            if(split.length==2) {
                String JoinKey = split[0];
                //将value打上标记
                String joinValue = &quot;city_id&quot; + &quot;,&quot; +split[1];    
                context.write(new Text(JoinKey), new Text(joinValue));
                //数据此时会变成
                //83522   city_id,合肥市    
            }
        }
    };    
}

public static class MyReduce extends Reducer&lt;Text, Text, Text, Text&gt;{

    @Override
    protected void reduce(Text k2, Iterable&lt;Text&gt; v2s,
            Reducer&lt;Text, Text, Text, Text&gt;.Context context) throws IOException, InterruptedException {

        //01  {data.txt,手机号码,301  city_id,合肥市}

        //创建2个集合,用ArrayList也可以，都行
        Vector&lt;String&gt; vecA = new Vector&lt;String&gt;();
        Vector&lt;String&gt; vecB = new Vector&lt;String&gt;();
        //判断两个输入数据的value,分别加入上面两个集合
        for (Text value : v2s) {
            String line = value.toString();
            //判断开头标记，然后放入集合
            if(line.startsWith(&quot;data&quot;)) {
                //把数据中的data截取掉，让输出的数据中没有data
                vecA.add(line.substring(4)); 
            }
            if(line.startsWith(&quot;city_id&quot;)) {
                //把数据中的data截取掉，让输出的数据中没有city_id
                vecB.add(line.substring(7)); 
            }

        }

        //将两个集合 拼接，笛卡尔积
        for (String s1 : vecA) {
            for (String s2 : vecB) {

                context.write(new Text(s1+k2.toString()), new Text(s2));

            }
        }



    }
}


public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException {
    Configuration conf = new Configuration();
    Job job = Job.getInstance(conf, MapReduce_Join.class.getSimpleName());

    //输出以逗号为间隔,加上这个配置，前面substring里的值都加x
    //conf.set(&quot;mapred.textoutputformat.separator&quot;,&quot;,&quot;);

    job.setJarByClass(MapReduce_Join.class);

    //输入两个路径  //在传参数时，两个文件夹路径之间用空格
    FileInputFormat.addInputPath(job, new Path(args[0]));    
    FileInputFormat.addInputPath(job, new Path(args[1]));

    job.setMapperClass(MyMapper.class);    
    job.setMapOutputKeyClass(Text.class);
    job.setMapOutputValueClass(Text.class);
    job.setReducerClass(MyReduce.class);
    job.setOutputKeyClass(Text.class);
    job.setOutputValueClass(Text.class);    
    FileOutputFormat.setOutputPath(job, new Path(args[2]));    
    job.waitForCompletion(true);    
}</code></pre><p>}</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://wenyouwang99.io/2019/08/14/MapReduce_Join/" data-id="cjzgpja8d00014gvgolfywx7y" class="article-share-link">分享</a>
      
      
      
    </footer>
  </div>
  
</article>
 


  
    <article id="post-求每个用户平均停留时长" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2019/08/14/求每个用户平均停留时长/" class="article-date">
  <time datetime="2019-08-14T09:04:11.915Z" itemprop="datePublished">2019-08-14</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
    <div class="article-entry" itemprop="articleBody">
      
        <!-- Table of Contents -->
        
        <h2 id="求每个用户平均停留时长"><a href="#求每个用户平均停留时长" class="headerlink" title="求每个用户平均停留时长"></a>求每个用户平均停留时长</h2><p>package hadoop;<br>//需求：求每个用户平均停留时长<br>/**</p>
<ul>
<li>数据样式：</li>
<li>用户手机号码，时间戳，城市id，城市区域id，所在区域停留时长，开始进入区域的时间，离开区域的时间，日期</li>
<li>数据进入hdfs的方式：put</li>
<li></li>
<li>/<br>import java.io.IOException;</li>
</ul>
<p>import org.apache.hadoop.conf.Configuration;<br>import org.apache.hadoop.fs.Path;<br>import org.apache.hadoop.io.LongWritable;<br>import org.apache.hadoop.io.Text;<br>import org.apache.hadoop.mapreduce.Job;<br>import org.apache.hadoop.mapreduce.Mapper;<br>import org.apache.hadoop.mapreduce.Reducer;<br>import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;<br>import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</p>
<p>public class test {</p>
<pre><code>public static class MyMapper extends Mapper&lt;LongWritable, Text, Text, Text&gt;{

    protected void map(LongWritable key, Text value, org.apache.hadoop.mapreduce.Mapper&lt;LongWritable, Text, Text, Text&gt;.Context context) throws java.io.IOException ,InterruptedException 
    {
        String line = value.toString();
        String[] split = line.split(&quot;\t&quot;);
        //  \N 是所取数据位置上出现的错误数据
        if(split.length == 3 &amp;&amp; split[4] != null ) {
            if(split[4].equals(&quot;\\N&quot;)) {
                split[4]=&quot;0&quot;;    
            }
            context.write(new Text(split[0]), new Text(split[4]));

        }

    };    
}

public static class MyReduce extends Reducer&lt;Text, Text, Text, LongWritable&gt;{

    @Override
    protected void reduce(Text k2, Iterable&lt;Text&gt; v2s,
            Reducer&lt;Text, Text, Text, LongWritable&gt;.Context context) throws IOException, InterruptedException {
        long sum = 0l;
        //定义次数
        long count = 0l;
        for (Text time : v2s) {
            count++;
            sum += Long.parseLong(time.toString());
        }
        context.write(k2, new LongWritable(sum/count));        
    }
}


public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException {
    Configuration conf = new Configuration();
    Job job = Job.getInstance(conf, test.class.getSimpleName());
    job.setJarByClass(test.class);    
    FileInputFormat.addInputPath(job, new Path(args[0]));    
    job.setMapperClass(MyMapper.class);    
    job.setMapOutputKeyClass(Text.class);
    job.setMapOutputValueClass(LongWritable.class);
    job.setReducerClass(MyReduce.class);
    job.setOutputKeyClass(Text.class);
    job.setOutputValueClass(LongWritable.class);    
    FileOutputFormat.setOutputPath(job, new Path(args[1]));    
    job.waitForCompletion(true);    
}</code></pre><p>}</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://wenyouwang99.io/2019/08/14/求每个用户平均停留时长/" data-id="cjzgpja92000w4gvg1wa9grs2" class="article-share-link">分享</a>
      
      
      
    </footer>
  </div>
  
</article>
 


  
    <article id="post-hadoop中combine优化" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2019/08/14/hadoop中combine优化/" class="article-date">
  <time datetime="2019-08-14T03:31:19.531Z" itemprop="datePublished">2019-08-14</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
    <div class="article-entry" itemprop="articleBody">
      
        <!-- Table of Contents -->
        
        <h2 id="hadoop中combine优化"><a href="#hadoop中combine优化" class="headerlink" title="hadoop中combine优化"></a>hadoop中combine优化</h2><p>1.写法<br>  (1)把reduce函数的代码块复制放在map和reduce代码块中间，把复制的代码块类名<br>  改为MapReduce_Combine(名字自己随便写)</p>
<p>  (2)在main中加入<br>  job.setCombinerClass(MapReduce_Combine.class);</p>
<p>2.优点<br>  把数据在map阶段结束后，直接执行combine函数，进行聚合，减少在map和reduce节点<br>  之间的数据传输量，以提高io性能</p>
<p>3.缺点<br>  增加了reduce的进程</p>
<p>4.杀死进程<br>  如果进程卡住了，或者时间太长而不想继续执行了，可以杀死进程<br>  加入进程编号为：  job_135525622353<br>  hadoop job -kill job_135525622353</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://wenyouwang99.io/2019/08/14/hadoop中combine优化/" data-id="cjzgpja8g00034gvg55qebhmr" class="article-share-link">分享</a>
      
      
      
    </footer>
  </div>
  
</article>
 


  
    <article id="post-hive命令2" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2019/08/13/hive命令2/" class="article-date">
  <time datetime="2019-08-13T09:04:52.056Z" itemprop="datePublished">2019-08-13</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
    <div class="article-entry" itemprop="articleBody">
      
        <!-- Table of Contents -->
        
        <h2 id="hive命令2"><a href="#hive命令2" class="headerlink" title="hive命令2"></a>hive命令2</h2><p>1.创建表–stud:表名    retail–数据库名<br>create external table stud(<br>fname string,<br>lname string,<br>address struct (houseno:string, street:string, city:string),  //这一行用()代替&lt;&gt;,&lt;&gt;代码不显示<br>active boolean,<br>create date,<br>location ‘/opt/stud’);</p>
<p>2.可以在数据库中直接创建一个表<br>在表名前加上数据库名<br>create external table retail.stud(）</p>
<p>3.列出表<br>show tables in retail</p>
<p>4.如果数据库中有很多表，可以用通配符搜索特定的表</p>
<p>5.外部表，其在被删除时，数据保留，在create table 中使用<br>external关键字来创建外部表<br>create external table retail.stud(）</p>
<p>6.内部表,其在被删除时，数据不保留<br>create table retail.stud(）</p>
<p>7.查看表内容<br>select * from stud;</p>
<p>8.查看表的模式，其中在table type中指明表是否是内部表（外部表）<br>describe formatted stud;</p>
<p>9.表的属性<br>(1)last_modified_user<br>(2)last_modified_time<br>(3)immutable<br>(4)orc.compress<br>(5)skip.header.line.count</p>
<p>(1)(2)属性可控，由hive自动增加，hive通过它们<br>将上次修改的用户和时间信息存放在metastore中</p>
<p>(3)当immutable属性设置为true,此时一个表里已经有一些数据了，则无法<br>在向其插入新行，强行插入，会报错<br>insert into text1 values (‘bacon’);<br>failed: SemanticException……..immutable table is not allowed text1</p>
<p>(4)orc.compress属性用于指定基于orc的存储所采用的算法</p>
<p>(5)skip.header.line.count<br>使用该属性，可以跳过底层数据文件的标题行<br>例子，如果一个文件中有2个标题行，那么跳过标题行写法如下<br>create external table stud (states string) location ‘/opt/stud’<br>tblproperties(“skip.header.line.count” = “2”);<br>此时创建的新表，再次查询时就会跳过2个标题行来显示文件内容</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://wenyouwang99.io/2019/08/13/hive命令2/" data-id="cjzgpja8n000a4gvgq16q767i" class="article-share-link">分享</a>
      
      
      
    </footer>
  </div>
  
</article>
 


  
    <article id="post-求平均以及数据清洗 " class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2019/08/13/求平均以及数据清洗 /" class="article-date">
  <time datetime="2019-08-13T09:03:52.805Z" itemprop="datePublished">2019-08-13</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
    <div class="article-entry" itemprop="articleBody">
      
        <!-- Table of Contents -->
        
        <h2 id="求平均数以及数据清洗"><a href="#求平均数以及数据清洗" class="headerlink" title="求平均数以及数据清洗"></a>求平均数以及数据清洗</h2><p>package hadoop;<br>//每个人平均花多少钱<br>//并且对数据进行清洗<br>/**</p>
<ul>
<li>数据样式：姓名，日期，金额，三个字段</li>
<li>数据：</li>
<li>张三，20198-1，30</li>
<li>张三，20198-2，30</li>
<li>李四，20198-1，30</li>
<li>李四，20198-2，          —日期位置是空格   –在空格位置填0，不会影响数据</li>
<li>李四，20198-3，，      —日期位置是逗号</li>
<li>李四，20198-4， ，     —日期位置是空格</li>
<li>王五，20198-1</li>
<li>王五，20198-1，30</li>
<li>/</li>
</ul>
<p>  package hadoop;<br>//每个人平均花多少钱<br>//并且对数据进行清洗<br>/**</p>
<ul>
<li>数据样式：姓名，日期，金额，三个字段</li>
<li>数据：</li>
<li>张三，20198-1，30</li>
<li>张三，20198-2，30</li>
<li>李四，20198-1，30</li>
<li>李四，20198-2，          —日期位置是空格   –在空格位置填0，不会影响数据</li>
<li>李四，20198-3，，      —日期位置是逗号</li>
<li>李四，20198-4， ，     —日期位置是空格</li>
<li>王五，20198-1</li>
<li>王五，20198-1，30</li>
<li>/<br>import java.io.IOException;</li>
</ul>
<p>import org.apache.hadoop.conf.Configuration;<br>import org.apache.hadoop.fs.Path;<br>import org.apache.hadoop.io.LongWritable;<br>import org.apache.hadoop.io.Text;<br>import org.apache.hadoop.mapreduce.Job;<br>import org.apache.hadoop.mapreduce.Mapper;<br>import org.apache.hadoop.mapreduce.Reducer;<br>import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;<br>import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</p>
<p>public class MapReduce_avgClean {</p>
<pre><code>public static class MyMapper extends Mapper&lt;LongWritable, Text, Text, LongWritable&gt;{

    protected void map(LongWritable key, Text value, org.apache.hadoop.mapreduce.Mapper&lt;LongWritable, Text, Text, LongWritable&gt;.Context context) throws java.io.IOException ,InterruptedException 
    {
        //数据样式：姓名，日期，金额，三个字段
        String line = value.toString();
        String[] split = line.split(&quot;,&quot;);
        //对数据进行过滤，将数组长度不为3的过滤出去
        //首先将字段不满足数量的过滤掉
        if(split.length == 3) {
            //将关键字段不满足需求的进行清洗，回填
            if(split[2].equals(null) || split[2].equals(&quot;&quot;) || split[2].equals(&quot; &quot;)) {
                split[2] = &quot;0&quot;;
            }

            String name = split[0];
            Long money = Long.parseLong(split[2]);
            context.write(new Text(name), new LongWritable(money));

        }

    };    
}

public static class MyReduce extends Reducer&lt;Text, LongWritable, Text, LongWritable&gt;{

    @Override
    protected void reduce(Text k2, Iterable&lt;LongWritable&gt; v2s,
            Reducer&lt;Text, LongWritable, Text, LongWritable&gt;.Context context) throws IOException, InterruptedException {
        //在reduce算出消费总和，以及消费次数，因为在同一个reduce中是同一个人消费
        //所以直接算出reduce的数据条数
        long sum = 0l;
        //定义次数
        long count = 0l;
        for (LongWritable value : v2s) {
            count++;
            sum += value.get();
        }
        context.write(k2, new LongWritable(sum/count));        
    }
}


public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException {
    Configuration conf = new Configuration();
    Job job = Job.getInstance(conf, MapReduce_sumClean.class.getSimpleName());
    job.setJarByClass(MapReduce_avgClean.class);    
    FileInputFormat.addInputPath(job, new Path(args[0]));    
    job.setMapperClass(MyMapper.class);    
    job.setMapOutputKeyClass(Text.class);
    job.setMapOutputValueClass(LongWritable.class);
    job.setReducerClass(MyReduce.class);
    job.setOutputKeyClass(Text.class);
    job.setOutputValueClass(LongWritable.class);    
    FileOutputFormat.setOutputPath(job, new Path(args[1]));    
    job.waitForCompletion(true);    
}</code></pre><p>}</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://wenyouwang99.io/2019/08/13/求平均以及数据清洗 /" data-id="cjzgpja92000v4gvgaactyg06" class="article-share-link">分享</a>
      
      
      
    </footer>
  </div>
  
</article>
 


  
    <article id="post-求和以及数据清洗 " class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2019/08/13/求和以及数据清洗 /" class="article-date">
  <time datetime="2019-08-13T08:46:30.399Z" itemprop="datePublished">2019-08-13</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
    <div class="article-entry" itemprop="articleBody">
      
        <!-- Table of Contents -->
        
        <h2 id="求和以及数据清洗"><a href="#求和以及数据清洗" class="headerlink" title="求和以及数据清洗"></a>求和以及数据清洗</h2><p>package hadoop;<br>//每个人总共花多少钱<br>//并且对数据进行清洗<br>/**</p>
<ul>
<li>数据样式：姓名，日期，金额，三个字段</li>
<li>数据：</li>
<li>张三，20198-1，30</li>
<li>张三，20198-2，30</li>
<li>李四，20198-1，30</li>
<li>李四，20198-2，          —日期位置是空格   –在空格位置填0，不会影响数据</li>
<li>李四，20198-3，，      —日期位置是逗号</li>
<li>李四，20198-4， ，     —日期位置是空格</li>
<li>王五，20198-1</li>
<li>王五，20198-1，30</li>
<li>/<br>import java.io.IOException;</li>
</ul>
<p>import org.apache.hadoop.conf.Configuration;<br>import org.apache.hadoop.fs.Path;<br>import org.apache.hadoop.io.LongWritable;<br>import org.apache.hadoop.io.Text;<br>import org.apache.hadoop.mapreduce.Job;<br>import org.apache.hadoop.mapreduce.Mapper;<br>import org.apache.hadoop.mapreduce.Reducer;<br>import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;<br>import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</p>
<p>public class MapReduce_sumClean {</p>
<pre><code>public static class MyMapper extends Mapper&lt;LongWritable, Text, Text, LongWritable&gt;{

    protected void map(LongWritable key, Text value, org.apache.hadoop.mapreduce.Mapper&lt;LongWritable, Text, Text, LongWritable&gt;.Context context) throws java.io.IOException ,InterruptedException 
    {
        //数据样式：姓名，日期，金额，三个字段
        String line = value.toString();
        String[] split = line.split(&quot;,&quot;);
        //对数据进行过滤，将数组长度不为3的过滤出去
        //首先将字段不满足数量的过滤掉
        if(split.length == 3) {
            //将关键字段不满足需求的进行清洗，回填
            if(split[2].equals(null) || split[2].equals(&quot;&quot;) || split[2].equals(&quot; &quot;)) {
                split[2] = &quot;0&quot;;
            }

            String name = split[0];
            Long money = Long.parseLong(split[2]);
            context.write(new Text(name), new LongWritable(money));

        }

    };    
}

public static class MyReduce extends Reducer&lt;Text, LongWritable, Text, LongWritable&gt;{

    @Override
    protected void reduce(Text k2, Iterable&lt;LongWritable&gt; v2s,
            Reducer&lt;Text, LongWritable, Text, LongWritable&gt;.Context context) throws IOException, InterruptedException {
        long sum = 0l;
        for (LongWritable value : v2s) {
            sum += value.get();
        }
        context.write(k2, new LongWritable(sum));        
    }
}


public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException {
    Configuration conf = new Configuration();
    Job job = Job.getInstance(conf, MapReduce_sumClean.class.getSimpleName());
    job.setJarByClass(MapReduce_sumClean.class);    
    FileInputFormat.addInputPath(job, new Path(args[0]));    
    job.setMapperClass(MyMapper.class);    
    job.setMapOutputKeyClass(Text.class);
    job.setMapOutputValueClass(LongWritable.class);
    job.setReducerClass(MyReduce.class);
    job.setOutputKeyClass(Text.class);
    job.setOutputValueClass(LongWritable.class);    
    FileOutputFormat.setOutputPath(job, new Path(args[1]));    
    job.waitForCompletion(true);    
}</code></pre><p>}</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://wenyouwang99.io/2019/08/13/求和以及数据清洗 /" data-id="cjzgpja91000u4gvgchezzare" class="article-share-link">分享</a>
      
      
      
    </footer>
  </div>
  
</article>
 


  


  <nav id="page-nav">
    <a class="extend prev" rel="prev" href="/">&laquo; 上一页</a><a class="page-number" href="/">1</a><span class="page-number current">2</span><a class="page-number" href="/page/3/">3</a><a class="page-number" href="/page/4/">4</a><a class="extend next" rel="next" href="/page/3/">下一页&raquo;</a>
  </nav>
</section>
           
    <aside id="sidebar">
  
    

  
    
  
    

  
    
  
    
  <div class="widget-wrap">
    <h3 class="widget-title recent-posts">最新文章</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2019/08/19/hive随笔3/">(no title)</a>
          </li>
        
          <li>
            <a href="/2019/08/19/hive随笔2/">(no title)</a>
          </li>
        
          <li>
            <a href="/2019/08/18/hive随笔1/">(no title)</a>
          </li>
        
          <li>
            <a href="/2019/08/18/hive命令13/">(no title)</a>
          </li>
        
          <li>
            <a href="/2019/08/17/hive命令12/">(no title)</a>
          </li>
        
      </ul>
    </div>
  </div>

  
    

  
    
  
    <!--微信公众号二维码-->


  
</aside>

      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-left">
      &copy; 2014 - 2019 WenYouWang&nbsp;|&nbsp;
      主题 <a href="https://github.com/giscafer/hexo-theme-cafe/" target="_blank">Cafe</a>
    </div>
     <div id="footer-right">
      联系方式&nbsp;|&nbsp;youemail@outlook.com
    </div>
  </div>
</footer>
 <script src="/jquery/jquery.min.js"></script>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">首页</a>
  
    <a href="/archives" class="mobile-nav-link">归档</a>
  
    <a href="/about" class="mobile-nav-link">关于</a>
  
</nav>
    <img class="back-to-top-btn" src="/images/fly-to-top.png"/>
<script>
// Elevator script included on the page, already.
window.onload = function() {
  var elevator = new Elevator({
    selector:'.back-to-top-btn',
    element: document.querySelector('.back-to-top-btn'),
    duration: 1000 // milliseconds
  });
}
</script>
      

  
    <script>
      var cloudTieConfig = {
        url: document.location.href, 
        sourceId: "",
        productKey: "e2fb4051c49842688ce669e634bc983f",
        target: "cloud-tie-wrapper"
      };
    </script>
    <script src="https://img1.ws.126.net/f2e/tie/yun/sdk/loader.js"></script>
    

  







<!-- author:forvoid begin -->
<!-- author:forvoid begin -->

<!-- author:forvoid end -->

<!-- author:forvoid end -->


  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      })
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      })
    </script>
    <script type="text/javascript" src="https://cdn.rawgit.com/mathjax/MathJax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


 <script src="/js/is.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


<script src="/js/script.js"></script>
<script src="/js/elevator.js"></script>
  </div>
</body>
</html>