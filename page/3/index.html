<!DOCTYPE html>
<html>
<head><meta name="generator" content="Hexo 3.9.0">
  <meta charset="utf-8">
  
  <title>Hexo</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta property="og:type" content="website">
<meta property="og:title" content="Hexo">
<meta property="og:url" content="https://wenyouwang99.io/page/3/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:locale" content="default">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Hexo">
  
    <link rel="alternate" href="/atom.xml" title="Hexo" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    
  
  <link rel="stylesheet" href="/css/style.css">
  

</head>
</html>
<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    
    <div id="header-inner" class="inner">
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="搜索"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="https://wenyouwang99.io"></form>
      </div>
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">首页</a>
        
          <a class="main-nav-link" href="/archives">归档</a>
        
          <a class="main-nav-link" href="/about">关于</a>
        
      </nav>
      
    </div>
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Hexo</a>
      </h1>
      
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main">
  
    <article id="post-hadoop中combine优化" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2019/08/14/hadoop中combine优化/" class="article-date">
  <time datetime="2019-08-14T03:31:19.531Z" itemprop="datePublished">2019-08-14</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
    <div class="article-entry" itemprop="articleBody">
      
        <!-- Table of Contents -->
        
        <h2 id="hadoop中combine优化"><a href="#hadoop中combine优化" class="headerlink" title="hadoop中combine优化"></a>hadoop中combine优化</h2><p>1.写法<br>  (1)把reduce函数的代码块复制放在map和reduce代码块中间，把复制的代码块类名<br>  改为MapReduce_Combine(名字自己随便写)</p>
<p>  (2)在main中加入<br>  job.setCombinerClass(MapReduce_Combine.class);</p>
<p>2.优点<br>  把数据在map阶段结束后，直接执行combine函数，进行聚合，减少在map和reduce节点<br>  之间的数据传输量，以提高io性能</p>
<p>3.缺点<br>  增加了reduce的进程</p>
<p>4.杀死进程<br>  如果进程卡住了，或者时间太长而不想继续执行了，可以杀死进程<br>  加入进程编号为：  job_135525622353<br>  hadoop job -kill job_135525622353</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://wenyouwang99.io/2019/08/14/hadoop中combine优化/" data-id="cjzmf6plq000270vgiagpl91r" class="article-share-link">分享</a>
      
      
      
    </footer>
  </div>
  
</article>
 


  
    <article id="post-hive命令2" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2019/08/13/hive命令2/" class="article-date">
  <time datetime="2019-08-13T09:04:52.056Z" itemprop="datePublished">2019-08-13</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
    <div class="article-entry" itemprop="articleBody">
      
        <!-- Table of Contents -->
        
        <h2 id="hive命令2"><a href="#hive命令2" class="headerlink" title="hive命令2"></a>hive命令2</h2><p>1.创建表–stud:表名    retail–数据库名<br>create external table stud(<br>fname string,<br>lname string,<br>address struct (houseno:string, street:string, city:string),  //这一行用()代替&lt;&gt;,&lt;&gt;代码不显示<br>active boolean,<br>create date,<br>location ‘/opt/stud’);</p>
<p>2.可以在数据库中直接创建一个表<br>在表名前加上数据库名<br>create external table retail.stud(）</p>
<p>3.列出表<br>show tables in retail</p>
<p>4.如果数据库中有很多表，可以用通配符搜索特定的表</p>
<p>5.外部表，其在被删除时，数据保留，在create table 中使用<br>external关键字来创建外部表<br>create external table retail.stud(）</p>
<p>6.内部表,其在被删除时，数据不保留<br>create table retail.stud(）</p>
<p>7.查看表内容<br>select * from stud;</p>
<p>8.查看表的模式，其中在table type中指明表是否是内部表（外部表）<br>describe formatted stud;</p>
<p>9.表的属性<br>(1)last_modified_user<br>(2)last_modified_time<br>(3)immutable<br>(4)orc.compress<br>(5)skip.header.line.count</p>
<p>(1)(2)属性可控，由hive自动增加，hive通过它们<br>将上次修改的用户和时间信息存放在metastore中</p>
<p>(3)当immutable属性设置为true,此时一个表里已经有一些数据了，则无法<br>在向其插入新行，强行插入，会报错<br>insert into text1 values (‘bacon’);<br>failed: SemanticException……..immutable table is not allowed text1</p>
<p>(4)orc.compress属性用于指定基于orc的存储所采用的算法</p>
<p>(5)skip.header.line.count<br>使用该属性，可以跳过底层数据文件的标题行<br>例子，如果一个文件中有2个标题行，那么跳过标题行写法如下<br>create external table stud (states string) location ‘/opt/stud’<br>tblproperties(“skip.header.line.count” = “2”);<br>此时创建的新表，再次查询时就会跳过2个标题行来显示文件内容</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://wenyouwang99.io/2019/08/13/hive命令2/" data-id="cjzmf6ply000a70vg55nx7q0z" class="article-share-link">分享</a>
      
      
      
    </footer>
  </div>
  
</article>
 


  
    <article id="post-求平均以及数据清洗 " class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2019/08/13/求平均以及数据清洗 /" class="article-date">
  <time datetime="2019-08-13T09:03:52.805Z" itemprop="datePublished">2019-08-13</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
    <div class="article-entry" itemprop="articleBody">
      
        <!-- Table of Contents -->
        
        <h2 id="求平均数以及数据清洗"><a href="#求平均数以及数据清洗" class="headerlink" title="求平均数以及数据清洗"></a>求平均数以及数据清洗</h2><p>package hadoop;<br>//每个人平均花多少钱<br>//并且对数据进行清洗<br>/**</p>
<ul>
<li>数据样式：姓名，日期，金额，三个字段</li>
<li>数据：</li>
<li>张三，20198-1，30</li>
<li>张三，20198-2，30</li>
<li>李四，20198-1，30</li>
<li>李四，20198-2，          —日期位置是空格   –在空格位置填0，不会影响数据</li>
<li>李四，20198-3，，      —日期位置是逗号</li>
<li>李四，20198-4， ，     —日期位置是空格</li>
<li>王五，20198-1</li>
<li>王五，20198-1，30</li>
<li>/</li>
</ul>
<p>  package hadoop;<br>//每个人平均花多少钱<br>//并且对数据进行清洗<br>/**</p>
<ul>
<li>数据样式：姓名，日期，金额，三个字段</li>
<li>数据：</li>
<li>张三，20198-1，30</li>
<li>张三，20198-2，30</li>
<li>李四，20198-1，30</li>
<li>李四，20198-2，          —日期位置是空格   –在空格位置填0，不会影响数据</li>
<li>李四，20198-3，，      —日期位置是逗号</li>
<li>李四，20198-4， ，     —日期位置是空格</li>
<li>王五，20198-1</li>
<li>王五，20198-1，30</li>
<li>/<br>import java.io.IOException;</li>
</ul>
<p>import org.apache.hadoop.conf.Configuration;<br>import org.apache.hadoop.fs.Path;<br>import org.apache.hadoop.io.LongWritable;<br>import org.apache.hadoop.io.Text;<br>import org.apache.hadoop.mapreduce.Job;<br>import org.apache.hadoop.mapreduce.Mapper;<br>import org.apache.hadoop.mapreduce.Reducer;<br>import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;<br>import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</p>
<p>public class MapReduce_avgClean {</p>
<pre><code>public static class MyMapper extends Mapper&lt;LongWritable, Text, Text, LongWritable&gt;{

    protected void map(LongWritable key, Text value, org.apache.hadoop.mapreduce.Mapper&lt;LongWritable, Text, Text, LongWritable&gt;.Context context) throws java.io.IOException ,InterruptedException 
    {
        //数据样式：姓名，日期，金额，三个字段
        String line = value.toString();
        String[] split = line.split(&quot;,&quot;);
        //对数据进行过滤，将数组长度不为3的过滤出去
        //首先将字段不满足数量的过滤掉
        if(split.length == 3) {
            //将关键字段不满足需求的进行清洗，回填
            if(split[2].equals(null) || split[2].equals(&quot;&quot;) || split[2].equals(&quot; &quot;)) {
                split[2] = &quot;0&quot;;
            }

            String name = split[0];
            Long money = Long.parseLong(split[2]);
            context.write(new Text(name), new LongWritable(money));

        }

    };    
}

public static class MyReduce extends Reducer&lt;Text, LongWritable, Text, LongWritable&gt;{

    @Override
    protected void reduce(Text k2, Iterable&lt;LongWritable&gt; v2s,
            Reducer&lt;Text, LongWritable, Text, LongWritable&gt;.Context context) throws IOException, InterruptedException {
        //在reduce算出消费总和，以及消费次数，因为在同一个reduce中是同一个人消费
        //所以直接算出reduce的数据条数
        long sum = 0l;
        //定义次数
        long count = 0l;
        for (LongWritable value : v2s) {
            count++;
            sum += value.get();
        }
        context.write(k2, new LongWritable(sum/count));        
    }
}


public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException {
    Configuration conf = new Configuration();
    Job job = Job.getInstance(conf, MapReduce_sumClean.class.getSimpleName());
    job.setJarByClass(MapReduce_avgClean.class);    
    FileInputFormat.addInputPath(job, new Path(args[0]));    
    job.setMapperClass(MyMapper.class);    
    job.setMapOutputKeyClass(Text.class);
    job.setMapOutputValueClass(LongWritable.class);
    job.setReducerClass(MyReduce.class);
    job.setOutputKeyClass(Text.class);
    job.setOutputValueClass(LongWritable.class);    
    FileOutputFormat.setOutputPath(job, new Path(args[1]));    
    job.waitForCompletion(true);    
}</code></pre><p>}</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://wenyouwang99.io/2019/08/13/求平均以及数据清洗 /" data-id="cjzmf6pmh001170vg0567pw5y" class="article-share-link">分享</a>
      
      
      
    </footer>
  </div>
  
</article>
 


  
    <article id="post-求和以及数据清洗 " class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2019/08/13/求和以及数据清洗 /" class="article-date">
  <time datetime="2019-08-13T08:46:30.399Z" itemprop="datePublished">2019-08-13</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
    <div class="article-entry" itemprop="articleBody">
      
        <!-- Table of Contents -->
        
        <h2 id="求和以及数据清洗"><a href="#求和以及数据清洗" class="headerlink" title="求和以及数据清洗"></a>求和以及数据清洗</h2><p>package hadoop;<br>//每个人总共花多少钱<br>//并且对数据进行清洗<br>/**</p>
<ul>
<li>数据样式：姓名，日期，金额，三个字段</li>
<li>数据：</li>
<li>张三，20198-1，30</li>
<li>张三，20198-2，30</li>
<li>李四，20198-1，30</li>
<li>李四，20198-2，          —日期位置是空格   –在空格位置填0，不会影响数据</li>
<li>李四，20198-3，，      —日期位置是逗号</li>
<li>李四，20198-4， ，     —日期位置是空格</li>
<li>王五，20198-1</li>
<li>王五，20198-1，30</li>
<li>/<br>import java.io.IOException;</li>
</ul>
<p>import org.apache.hadoop.conf.Configuration;<br>import org.apache.hadoop.fs.Path;<br>import org.apache.hadoop.io.LongWritable;<br>import org.apache.hadoop.io.Text;<br>import org.apache.hadoop.mapreduce.Job;<br>import org.apache.hadoop.mapreduce.Mapper;<br>import org.apache.hadoop.mapreduce.Reducer;<br>import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;<br>import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</p>
<p>public class MapReduce_sumClean {</p>
<pre><code>public static class MyMapper extends Mapper&lt;LongWritable, Text, Text, LongWritable&gt;{

    protected void map(LongWritable key, Text value, org.apache.hadoop.mapreduce.Mapper&lt;LongWritable, Text, Text, LongWritable&gt;.Context context) throws java.io.IOException ,InterruptedException 
    {
        //数据样式：姓名，日期，金额，三个字段
        String line = value.toString();
        String[] split = line.split(&quot;,&quot;);
        //对数据进行过滤，将数组长度不为3的过滤出去
        //首先将字段不满足数量的过滤掉
        if(split.length == 3) {
            //将关键字段不满足需求的进行清洗，回填
            if(split[2].equals(null) || split[2].equals(&quot;&quot;) || split[2].equals(&quot; &quot;)) {
                split[2] = &quot;0&quot;;
            }

            String name = split[0];
            Long money = Long.parseLong(split[2]);
            context.write(new Text(name), new LongWritable(money));

        }

    };    
}

public static class MyReduce extends Reducer&lt;Text, LongWritable, Text, LongWritable&gt;{

    @Override
    protected void reduce(Text k2, Iterable&lt;LongWritable&gt; v2s,
            Reducer&lt;Text, LongWritable, Text, LongWritable&gt;.Context context) throws IOException, InterruptedException {
        long sum = 0l;
        for (LongWritable value : v2s) {
            sum += value.get();
        }
        context.write(k2, new LongWritable(sum));        
    }
}


public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException {
    Configuration conf = new Configuration();
    Job job = Job.getInstance(conf, MapReduce_sumClean.class.getSimpleName());
    job.setJarByClass(MapReduce_sumClean.class);    
    FileInputFormat.addInputPath(job, new Path(args[0]));    
    job.setMapperClass(MyMapper.class);    
    job.setMapOutputKeyClass(Text.class);
    job.setMapOutputValueClass(LongWritable.class);
    job.setReducerClass(MyReduce.class);
    job.setOutputKeyClass(Text.class);
    job.setOutputValueClass(LongWritable.class);    
    FileOutputFormat.setOutputPath(job, new Path(args[1]));    
    job.waitForCompletion(true);    
}</code></pre><p>}</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://wenyouwang99.io/2019/08/13/求和以及数据清洗 /" data-id="cjzmf6pmg000z70vg8g1xj0sk" class="article-share-link">分享</a>
      
      
      
    </footer>
  </div>
  
</article>
 


  
    <article id="post-求和" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2019/08/13/求和/" class="article-date">
  <time datetime="2019-08-13T08:45:34.747Z" itemprop="datePublished">2019-08-13</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
    <div class="article-entry" itemprop="articleBody">
      
        <!-- Table of Contents -->
        
        <h2 id="求和"><a href="#求和" class="headerlink" title="求和"></a>求和</h2><p>package hadoop;<br>//每个人花多少钱<br>import java.io.IOException;</p>
<p>import org.apache.hadoop.conf.Configuration;<br>import org.apache.hadoop.fs.Path;<br>import org.apache.hadoop.io.LongWritable;<br>import org.apache.hadoop.io.Text;<br>import org.apache.hadoop.mapreduce.Job;<br>import org.apache.hadoop.mapreduce.Mapper;<br>import org.apache.hadoop.mapreduce.Reducer;<br>import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;<br>import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</p>
<p>public class MapReduce_Sum {</p>
<pre><code>public static class MyMapper extends Mapper&lt;LongWritable, Text, Text, LongWritable&gt;{

    protected void map(LongWritable key, Text value, org.apache.hadoop.mapreduce.Mapper&lt;LongWritable, Text, Text, LongWritable&gt;.Context context) throws java.io.IOException ,InterruptedException 
    {
        //数据样式：姓名，日期，金额，三个字段
        String line = value.toString();
        String[] split = line.split(&quot;,&quot;);
        //对数据进行过滤，将数组长度为3的过滤出来
        if(split.length==3) {
        for (String string : split) {
            String name = split[0];
            Long money = Long.parseLong(split[2]);
            context.write(new Text(name), new LongWritable(money));
        }
        }
    };    
}

public static class MyReduce extends Reducer&lt;Text, LongWritable, Text, LongWritable&gt;{

    @Override
    protected void reduce(Text k2, Iterable&lt;LongWritable&gt; v2s,
            Reducer&lt;Text, LongWritable, Text, LongWritable&gt;.Context context) throws IOException, InterruptedException {
        long sum = 0l;
        for (LongWritable value : v2s) {
            sum += value.get();
        }
        context.write(k2, new LongWritable(sum));        
    }
}


public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException {
    Configuration conf = new Configuration();
    Job job = Job.getInstance(conf, MapReduce_Sum.class.getSimpleName());
    job.setJarByClass(MapReduce_Sum.class);    
    FileInputFormat.addInputPath(job, new Path(args[0]));    
    job.setMapperClass(MyMapper.class);    
    job.setMapOutputKeyClass(Text.class);
    job.setMapOutputValueClass(LongWritable.class);
    job.setReducerClass(MyReduce.class);
    job.setOutputKeyClass(Text.class);
    job.setOutputValueClass(LongWritable.class);    
    FileOutputFormat.setOutputPath(job, new Path(args[1]));    
    job.waitForCompletion(true);    
}</code></pre><p>}</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://wenyouwang99.io/2019/08/13/求和/" data-id="cjzmf6pmh001070vg9574hlgf" class="article-share-link">分享</a>
      
      
      
    </footer>
  </div>
  
</article>
 


  
    <article id="post-单词计数" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2019/08/13/单词计数/" class="article-date">
  <time datetime="2019-08-13T08:43:48.369Z" itemprop="datePublished">2019-08-13</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
    <div class="article-entry" itemprop="articleBody">
      
        <!-- Table of Contents -->
        
        <h2 id="单词计数-（自己写）"><a href="#单词计数-（自己写）" class="headerlink" title="单词计数 （自己写）"></a>单词计数 （自己写）</h2><p>package hadoop;<br>//单词计数<br>import java.io.IOException;</p>
<p>import org.apache.hadoop.conf.Configuration;<br>import org.apache.hadoop.fs.Path;<br>import org.apache.hadoop.io.LongWritable;<br>import org.apache.hadoop.io.Text;<br>import org.apache.hadoop.mapreduce.Job;<br>import org.apache.hadoop.mapreduce.Mapper;<br>import org.apache.hadoop.mapreduce.Reducer;<br>import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;<br>import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</p>
<p>public class MapReduceWC {</p>
<pre><code>public static class MyMapper extends Mapper&lt;LongWritable, Text, Text, LongWritable&gt;{

    protected void map(LongWritable key, Text value, org.apache.hadoop.mapreduce.Mapper&lt;LongWritable, Text, Text, LongWritable&gt;.Context context) throws java.io.IOException ,InterruptedException 
    {
        String line = value.toString();
        String[] split = line.split(&quot;,&quot;);
        for (String word : split) {
            context.write(new Text(word), new LongWritable(1L));
        }
    };    
}

public static class MyReduce extends Reducer&lt;Text, LongWritable, Text, LongWritable&gt;{

    @Override
    protected void reduce(Text k2, Iterable&lt;LongWritable&gt; v2s,
            Reducer&lt;Text, LongWritable, Text, LongWritable&gt;.Context context) throws IOException, InterruptedException {
        long count = 0l;
        for (LongWritable value : v2s) {
            count += value.get();
        }
        context.write(k2, new LongWritable(count));        
    }
}


public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException {
    Configuration conf = new Configuration();
    Job job = Job.getInstance(conf, MapReduceWC.class.getSimpleName());
    job.setJarByClass(MapReduceWC.class);    
    FileInputFormat.addInputPath(job, new Path(args[0]));    
    job.setMapperClass(MyMapper.class);    
    job.setMapOutputKeyClass(Text.class);
    job.setMapOutputValueClass(LongWritable.class);
    job.setReducerClass(MyReduce.class);
    job.setOutputKeyClass(Text.class);
    job.setOutputValueClass(LongWritable.class);    
    FileOutputFormat.setOutputPath(job, new Path(args[1]));    
    job.waitForCompletion(true);    
}</code></pre><p>}</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://wenyouwang99.io/2019/08/13/单词计数/" data-id="cjzmf6pmd000u70vgg1wwskjl" class="article-share-link">分享</a>
      
      
      
    </footer>
  </div>
  
</article>
 


  
    <article id="post-wordcount 完整案例" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2019/08/12/wordcount 完整案例/" class="article-date">
  <time datetime="2019-08-12T13:16:40.407Z" itemprop="datePublished">2019-08-12</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
    <div class="article-entry" itemprop="articleBody">
      
        <!-- Table of Contents -->
        
        <h2 id="wordcount-完整案例"><a href="#wordcount-完整案例" class="headerlink" title="wordcount 完整案例"></a>wordcount 完整案例</h2><p>package hadoop;</p>
<p>import java.io.IOException;</p>
<p>import org.apache.hadoop.conf.Configuration;<br>import org.apache.hadoop.fs.Path;<br>import org.apache.hadoop.io.LongWritable;<br>import org.apache.hadoop.io.Text;<br>import org.apache.hadoop.mapreduce.Job;<br>import org.apache.hadoop.mapreduce.Mapper;<br>import org.apache.hadoop.mapreduce.Reducer;<br>import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;<br>import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</p>
<p>public class MapReduceWC {</p>
<pre><code>public static class MyMapper extends Mapper&lt;LongWritable, Text, Text, LongWritable&gt;{

    protected void map(LongWritable key, Text value, org.apache.hadoop.mapreduce.Mapper&lt;LongWritable, Text, Text, LongWritable&gt;.Context context) throws java.io.IOException ,InterruptedException 
    {
        String line = value.toString();
        String[] split = line.split(&quot;,&quot;);
        for (String word : split) {
            context.write(new Text(word), new LongWritable(1L));
        }
    };    
}

public static class MyReduce extends Reducer&lt;Text, LongWritable, Text, LongWritable&gt;{

    @Override
    protected void reduce(Text k2, Iterable&lt;LongWritable&gt; v2s,
            Reducer&lt;Text, LongWritable, Text, LongWritable&gt;.Context context) throws IOException, InterruptedException {
        long count = 0l;
        for (LongWritable value : v2s) {
            count += value.get();
        }
        context.write(k2, new LongWritable(count));        
    }
}


public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException {
    Configuration conf = new Configuration();
    Job job = Job.getInstance(conf, MapReduceWC.class.getSimpleName());
    job.setJarByClass(MapReduceWC.class);    
    FileInputFormat.addInputPath(job, new Path(args[0]));    
    job.setMapperClass(MyMapper.class);    
    job.setMapOutputKeyClass(Text.class);
    job.setMapOutputValueClass(LongWritable.class);
    job.setReducerClass(MyReduce.class);
    job.setOutputKeyClass(Text.class);
    job.setOutputValueClass(LongWritable.class);    
    FileOutputFormat.setOutputPath(job, new Path(args[1]));    
    job.waitForCompletion(true);    
}</code></pre><p>}</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://wenyouwang99.io/2019/08/12/wordcount 完整案例/" data-id="cjzmf6pmb000r70vgeawyy0zc" class="article-share-link">分享</a>
      
      
      
    </footer>
  </div>
  
</article>
 


  
    <article id="post-hive命令1" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2019/08/12/hive命令1/" class="article-date">
  <time datetime="2019-08-12T12:29:38.492Z" itemprop="datePublished">2019-08-12</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
    <div class="article-entry" itemprop="articleBody">
      
        <!-- Table of Contents -->
        
        <h2 id="hive命令"><a href="#hive命令" class="headerlink" title="hive命令"></a>hive命令</h2><p>1.登录到hive<br>cd $HIVE_HOME<br>bin/hive<br>或者环境变量没问题的话，直接hive&gt;</p>
<p>2.<br>show databases;</p>
<p>3.<br>show tables;</p>
<p>4.查看表列的定义<br>describe &lt;表名&gt;;</p>
<p>5.查看表具体的某一列<br>describe &lt;列名&gt;;</p>
<p>6.查看10条列的信息<br>select * from &lt;列名&gt; limit 10;</p>
<p>7.创建数据库(默认创建语法),其位置在hive.metastore.warehouse.dir<br>中定义的默认顶层目录下创建一个&lt;库名&gt;.db的目录<br>create database &lt;库名&gt;;</p>
<p>8.创建数据库(完整创建语法),with dbproperties可以将<br>任何自定义属性指派给数据库<br>create database if not exists &lt;库名&gt;<br>comment ‘stores all &lt;库名&gt; basket dara’<br>location ‘/opt/hive/&lt;库名&gt;.db’<br>with dbproperties (‘purpose’ = ‘texting’);</p>
<p>9.查看with dbproperties的属性<br>describe database extended &lt;库名&gt;;</p>
<p>10.更改数据库<br>alter database &lt;库名&gt;<br>set dbproperties (‘department’ = ‘sales’);</p>
<p>11.删除数据库,cascade的使用可选,其作用是<br>允许你删除数据库时将已有表一起删除<br>该命令将删除该库所有内部表和外部表<br>drop database &lt;库名&gt; cascade;</p>
<p>12.列出所有名字是s开头的数据库<br>show databases like ‘s*’;</p>
<p>13.基本数据类型<br>数值型–存放正负数字和浮点数<br>日期/时间型–存放时间值<br>字符型–将字符和数字存放在字符串中<br>布尔型–true或false<br>二进制型–二进制数的可变长数组</p>
<p>14.复杂数据类型<br>数组<br>map<br>结构体<br>联合体</p>
<p>15.数组<br>有序，下标从0开始，和java不同的是，不能在hive数组中定义最大元素值<br>例如，声明一个items数组来保存字符串值<br>items array&lt;”bread”,”butter”,”happy”&gt;<br>字符串的复合集有一个预定义的排序，因此可以从0访问<br>items[0] returns “bread”<br>items[2] returns “happy”</p>
<p>16.map<br>无序的键/值对集合，map的键可以用基本类型，map的值可以用基本和复杂数据类，map数据类型的元素需要使用键来访问，map数据类型的元素不能用下标访问<br>例如，声明一个包含商品项和数量的basket集合<br>basket map&lt;’string’,’int’&gt;<br>basket map&lt;”eggs”,’12’&gt;<br>通过在map函数中指定商品项，可以打印该商品项的数量<br>basket(“eggs”) returns 12</p>
<p>17.结构体<br>是一个对象，其中含有多个字段，而这些字段可以是任何数据类型<br>例如，使用struct定义来声明客户的地址记录<br>addrss struct(houseno:string,street:string,city:string,<br>zipcode:int,state:string,country:string)     //这里用()代替&lt;&gt;,&lt;&gt;代码不显示</p>
<p>address &lt;”17”,”main st”,”seattle”,98104,”wa”,”usa”&gt;<br>可以使用点号来访问某一字段，使用address.zipcode来访问各个地址的邮政编码</p>
<p>18.联合体<br>联合体提供了一种方法，可以将不同数据类型的元素存储在同一字段的不同行中。<br>当字段的底层数据不同质的时候，这种方法很有用</p>
<p>例如，如果数据文件中存放了客户的联系信息，但是每条联系信息中包含一个或多个<br>电话号码，或者包含一个或多个电子邮件地址，那么可以声明一个contact变量来存储<br>信息<br>    contact uniontype (int,array(int),string,array(string))  //这里用()代替&lt;&gt;,&lt;&gt;代码不显示</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://wenyouwang99.io/2019/08/12/hive命令1/" data-id="cjzmf6pls000470vgburbq2ws" class="article-share-link">分享</a>
      
      
      
    </footer>
  </div>
  
</article>
 


  
    <article id="post-统计乘用车辆，商用车辆的数量销售和销售额分布" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2019/08/11/统计乘用车辆，商用车辆的数量销售和销售额分布/" class="article-date">
  <time datetime="2019-08-11T09:47:01.730Z" itemprop="datePublished">2019-08-11</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
    <div class="article-entry" itemprop="articleBody">
      
        <!-- Table of Contents -->
        
        <h2 id="统计乘用车辆，商用车辆的数量销售和销售额分布"><a href="#统计乘用车辆，商用车辆的数量销售和销售额分布" class="headerlink" title="统计乘用车辆，商用车辆的数量销售和销售额分布"></a>统计乘用车辆，商用车辆的数量销售和销售额分布</h2><p>/**<br>     * 根据汽车所属（个人，商用）来进行划分<br>     * 计算乘用车辆，商用车辆各自的数量，以及各自所占的比重<br>     */<br>    public static class CountMap extends Mapper&lt;LongWritable, Text, Text, LongWritable&gt;{</p>
<pre><code>    @Override
    public void map(LongWritable key, Text value,                
            Mapper&lt;LongWritable, Text, Text, LongWritable&gt;.Context context)
            throws IOException, InterruptedException {

        String [] owns = value.toString().trim().split(&quot; &quot;);
        if(null != owns &amp;&amp; owns.length &gt; 10 &amp;&amp; owns[10] != null) {
            if(owns[10].equals(&quot;非营运&quot;)) {
                context.write(new Text(&quot;乘用车辆&quot;), new LongWritable(1));
            }else {
                context.write(new Text(&quot;商用车辆&quot;), new LongWritable(1));
            }
        }             
    };
}




/**
 * 统计乘用车辆和商用车辆的数量以及他们的总量
 */
public static class CountReduce extends Reducer&lt;Text, LongWritable, Text, DoubleWritable&gt; {
    Map&lt;String,Long&gt; maps = new HashMap&lt;String,Long&gt;();
    //准备存放非营运的乘用车辆和营运的商用车辆的总和
    double all = 0;
    @Override
    public void reduce(Text key, Iterable&lt;LongWritable&gt; values,Context context) throws IOException, InterruptedException {
         Long sum = (long) 0;
         for (LongWritable val : values) {
            sum += val.get();
        }
         //求出车辆的总和
         all += sum;
         maps.put(key.toString(),sum);
    };

    protected  void cleanup(
            org.apache.hadoop.mapreduce.Reducer&lt;Text, LongWritable, Text, DoubleWritable&gt;.Context context) throws IOException, InterruptedException {
        Set&lt;String&gt; keySet = maps.keySet();
        //循环set集合中的乘用车辆和商用车辆的数量
        for (String str : keySet) {
            long value = maps.get(str);
            //用乘用车辆/总量  和  商用车辆/总量，求出各自的比例
            double percent = value/all;
            //输出的key为车辆类型，value的值为该车辆类型的占比
            context.write(new Text(str), new DoubleWritable(percent));
        }
    };
}</code></pre>
      
    </div>
    <footer class="article-footer">
      <a data-url="https://wenyouwang99.io/2019/08/11/统计乘用车辆，商用车辆的数量销售和销售额分布/" data-id="cjzmf6pmj001370vg5eyiuc4q" class="article-share-link">分享</a>
      
      
      
    </footer>
  </div>
  
</article>
 


  
    <article id="post-二次排序" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2019/08/10/二次排序/" class="article-date">
  <time datetime="2019-08-10T12:13:35.519Z" itemprop="datePublished">2019-08-10</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
    <div class="article-entry" itemprop="articleBody">
      
        <!-- Table of Contents -->
        
        <h2 id="二次排序（对key-value都排序）"><a href="#二次排序（对key-value都排序）" class="headerlink" title="二次排序（对key,value都排序）"></a>二次排序（对key,value都排序）</h2><pre><code>import org.apache.hadoop.conf.Configured;</code></pre><p>   import org.apache.hadoop.io.WritableComparable;<br>   import java.io.DataInput;<br>   import java.io.DataOutput;<br>   import java.io.IOException;<br>   import org.apache.hadoop.io.*;<br>   import org.apache.hadoop.mapreduce.Partitioner;<br>   import org.apache.hadoop.mapreduce.Mapper;<br>   import org.apache.hadoop.mapreduce.Reducer;<br>  import org.apache.hadoop.conf.Configuration;<br>  import org.apache.hadoop.fs.FileSystem;<br>  import org.apache.hadoop.fs.Path;<br>  import org.apache.hadoop.mapreduce.Job;<br>  import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;<br>  import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;<br>  import org.apache.hadoop.mapreduce.lib.input.TextInputFormat;<br>  import org.apache.hadoop.mapreduce.lib.output.TextOutputFormat;<br>  import org.apache.hadoop.util.Tool;<br>  import org.apache.hadoop.util.ToolRunner;<br>  import org.slf4j.Logger;<br>  import org.slf4j.LoggerFactory;</p>
<p>  public class SecondrySort extends Configured implements Tool {</p>
<pre><code>static class Fruit implements WritableComparable&lt;Fruit&gt;{
    private static final Logger logger = LoggerFactory.getLogger(Fruit.class);
    private String date;
   private String name;
  private Integer sales;
  public Fruit(){
   }
  public Fruit(String date,String name,Integer sales){
      this.date = date;
       this.name = name;
      this.sales = sales;
   }

 public String getDate(){
       return this.date;
   }

  public String getName(){
      return this.name;
   }

   public Integer getSales(){
      return this.sales;
   }

   @Override
   public void readFields(DataInput in) throws IOException{
      this.date = in.readUTF();
      this.name = in.readUTF();
       this.sales = in.readInt();
  }

  @Override
   public void write(DataOutput out) throws IOException{
       out.writeUTF(this.date);
      out.writeUTF(this.name);
      out.writeInt(sales);
   }

  @Override
   public int compareTo(Fruit other) {
      int result1 = this.date.compareTo(other.getDate());
       if(result1 == 0) {
          int result2 = this.sales - other.getSales();
          if (result2 == 0) {
              double result3 = this.name.compareTo(other.getName());
              if(result3 &gt; 0) return -1;
              else if(result3 &lt; 0) return 1;
              else return 0;
          }else if(result2 &gt;0){
               return -1;
          }else if(result2 &lt; 0){
              return 1;
           }
       }else if(result1 &gt; 0){
          return -1;
       }else{
         return 1;
       }
      return 0;
  }

   @Override
   public int hashCode(){
      return this.date.hashCode() * 157 + this.sales + this.name.hashCode();
   }

   @Override
  public boolean equals(Object object){
       if (object == null)
          return false;
       if (this == object)
          return true;
      if (object instanceof Fruit){
          Fruit r = (Fruit) object;</code></pre><p> //                if(r.getDate().toString().equals(this.getDate().toString())){<br>                 return r.getDate().equals(this.getDate()) &amp;&amp; r.getName().equals(this.getName())<br>                        &amp;&amp; this.getSales() == r.getSales();<br>             }else{<br>                return false;<br>            }<br>        }</p>
<pre><code>     public String toString() {
        return this.date + &quot; &quot; + this.name + &quot; &quot; + this.sales;
     }

}

static class FruitPartition extends Partitioner&lt;Fruit, NullWritable&gt;{
    @Override
     public int getPartition(Fruit key, NullWritable value,int numPartitions){
        return Math.abs(Integer.parseInt(key.getDate()) * 127) % numPartitions;
     }</code></pre><p>   }</p>
<pre><code> public static class GroupingComparator extends WritableComparator{
     protected GroupingComparator(){
        super(Fruit.class, true);
     }

     @Override
     public int compare(WritableComparable w1, WritableComparable w2){
        Fruit f1 = (Fruit) w1;
        Fruit f2 = (Fruit) w2;

        if(!f1.getDate().equals(f2.getDate())){
            return f1.getDate().compareTo(f2.getDate());
        }else{
            return f1.getSales().compareTo(f2.getSales());
        }
    }
 }

 public static class Map extends Mapper&lt;LongWritable, Text, Fruit, NullWritable&gt; {

    public void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {
        String line = value.toString();
        String str[] = line.split(&quot; &quot;);
        Fruit fruit = new Fruit(str[0],str[1],new Integer(str[2]));
         //Fruit fruit = new Fruit();
         //fruit.set(str[0],str[1],new Integer(str[2]));
         context.write(fruit, NullWritable.get());
    }
 }

 public static class Reduce extends Reducer&lt;Fruit, NullWritable, Text, NullWritable&gt; {

     public void reduce(Fruit key, Iterable&lt;NullWritable&gt; values, Context context) throws IOException, InterruptedException {
        String str = key.getDate() + &quot; &quot; + key.getName() + &quot; &quot; + key.getSales();
        context.write(new Text(str), NullWritable.get());
    }
}

@Override
public int run(String[] args) throws Exception {
    Configuration conf = new Configuration();
     // 判断路径是否存在，如果存在，则删除
    Path mypath = new Path(args[1]);
    FileSystem hdfs = mypath.getFileSystem(conf);
     if (hdfs.isDirectory(mypath)) {
         hdfs.delete(mypath, true);
    }

     Job job = Job.getInstance(conf, &quot;Secondry Sort app&quot;);
   // 设置主类
     job.setJarByClass(SecondrySort.class);

     // 输入路径
    FileInputFormat.setInputPaths(job, new Path(args[0]));
    // 输出路径
    FileOutputFormat.setOutputPath(job, new Path(args[1]));

    // Mapper
     job.setMapperClass(Map.class);
    // Reducer
    job.setReducerClass(Reduce.class);

    // 分区函数
     job.setPartitionerClass(FruitPartition.class);

    // 分组函数
     job.setGroupingComparatorClass(GroupingComparator.class);

    // map输出key类型
    job.setMapOutputKeyClass(Fruit.class);
     // map输出value类型
    job.setMapOutputValueClass(NullWritable.class);

    // reduce输出key类型
     job.setOutputKeyClass(Text.class);
     // reduce输出value类型
    job.setOutputValueClass(NullWritable.class);

     // 输入格式
     job.setInputFormatClass(TextInputFormat.class);
     // 输出格式
     job.setOutputFormatClass(TextOutputFormat.class);

    return job.waitForCompletion(true) ? 0 : 1;
 }

 public static void main(String[] args) throws Exception{
     int exitCode = ToolRunner.run(new SecondrySort(), args);
     System.exit(exitCode);
 }</code></pre><p> }</p>
<p>测试数据：</p>
<p>20180906 Apple 200<br>20180904 Apple 200<br>20180905 Banana 100<br>20180906 Orange 300<br>20180906 Banana 400<br>20180904 Orange 100<br>20180905 Apple 400<br>20180904 Banana 300<br>20180905 Orange 500</p>
<p>运行结果：</p>
<p>20180906 Banana 400<br>20180906 Orange 300<br>20180906 Apple 200<br>20180905 Orange 500<br>20180905 Apple 400<br>20180905 Banana 100<br>20180904 Banana 300<br>20180904 Apple 200<br>20180904 Orange 100</p>
<p>总结：</p>
<p>1、在使用实现WritableComparable接口的方式实现自定义比较器时，必须有一个无参的构造函数。否则会报Unable to initialize any output collector的错误。<br>2、readFields和write方法中处理字段的顺序必须一致，否则会报MapReduce Error: java.io.EOFException at java.io.DataInputStream.readFully(DataInputStream.java:197)的错误。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://wenyouwang99.io/2019/08/10/二次排序/" data-id="cjzmf6pmo001570vgpjya8q7z" class="article-share-link">分享</a>
      
      
      
    </footer>
  </div>
  
</article>
 


  


  <nav id="page-nav">
    <a class="extend prev" rel="prev" href="/page/2/">&laquo; 上一页</a><a class="page-number" href="/">1</a><a class="page-number" href="/page/2/">2</a><span class="page-number current">3</span><a class="page-number" href="/page/4/">4</a><a class="page-number" href="/page/5/">5</a><a class="extend next" rel="next" href="/page/4/">下一页&raquo;</a>
  </nav>
</section>
           
    <aside id="sidebar">
  
    

  
    
  
    

  
    
  
    
  <div class="widget-wrap">
    <h3 class="widget-title recent-posts">最新文章</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2019/08/22/hive随笔7/">(no title)</a>
          </li>
        
          <li>
            <a href="/2019/08/22/hive随笔6/">(no title)</a>
          </li>
        
          <li>
            <a href="/2019/08/21/hive随笔5/">(no title)</a>
          </li>
        
          <li>
            <a href="/2019/08/21/hive随笔4/">(no title)</a>
          </li>
        
          <li>
            <a href="/2019/08/19/hive随笔3/">(no title)</a>
          </li>
        
      </ul>
    </div>
  </div>

  
    

  
    
  
    <!--微信公众号二维码-->


  
</aside>

      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-left">
      &copy; 2014 - 2019 WenYouWang&nbsp;|&nbsp;
      主题 <a href="https://github.com/giscafer/hexo-theme-cafe/" target="_blank">Cafe</a>
    </div>
     <div id="footer-right">
      联系方式&nbsp;|&nbsp;youemail@outlook.com
    </div>
  </div>
</footer>
 <script src="/jquery/jquery.min.js"></script>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">首页</a>
  
    <a href="/archives" class="mobile-nav-link">归档</a>
  
    <a href="/about" class="mobile-nav-link">关于</a>
  
</nav>
    <img class="back-to-top-btn" src="/images/fly-to-top.png"/>
<script>
// Elevator script included on the page, already.
window.onload = function() {
  var elevator = new Elevator({
    selector:'.back-to-top-btn',
    element: document.querySelector('.back-to-top-btn'),
    duration: 1000 // milliseconds
  });
}
</script>
      

  
    <script>
      var cloudTieConfig = {
        url: document.location.href, 
        sourceId: "",
        productKey: "e2fb4051c49842688ce669e634bc983f",
        target: "cloud-tie-wrapper"
      };
    </script>
    <script src="https://img1.ws.126.net/f2e/tie/yun/sdk/loader.js"></script>
    

  







<!-- author:forvoid begin -->
<!-- author:forvoid begin -->

<!-- author:forvoid end -->

<!-- author:forvoid end -->


  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      })
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      })
    </script>
    <script type="text/javascript" src="https://cdn.rawgit.com/mathjax/MathJax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


 <script src="/js/is.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


<script src="/js/script.js"></script>
<script src="/js/elevator.js"></script>
  </div>
</body>
</html>